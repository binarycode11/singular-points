{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "htEwVdkET4ey"
      },
      "source": [
        "### Teste de hipotese\n",
        "\n",
        "Esse notebook tem por objetivo detectar varios pontos e gerar um descritor otimo que seja resitente a variacoes de transformacoes afins e pequenas transformacoes projetivas, para isso temos:\n",
        "\n",
        "-- BaseFeatures para extrair informacoes equivariantes (num_channels,dim_first,dim_second,dim_third).\n",
        "\n",
        "-- SingularPoints lida com escala , e extrai as features consolidadas, em dim_third caracteristicas distintas, orientacao computadas além da lista de pontos.\n",
        "\n",
        "-- Computa a funcao de perda entre os mapas de orientacao e feature e os pontos que colidiram"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pmau4e00T4e0"
      },
      "outputs": [],
      "source": [
        "# !git clone -b main https://github.com/wagner1986/singular-points.git singular_points\n",
        "# !pip install kornia e2cnn kornia_moons\n",
        "\n",
        "# !pwd\n",
        "# %cd /content/singular_points\n",
        "# !pwd\n",
        "\n",
        "# from google.colab import drive\n",
        "\n",
        "# # Monta o Google Drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HCT8rCN8T4e1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from e2cnn import gspaces\n",
        "from e2cnn import nn as enn    #the equivariant layer we need to build the model\n",
        "from torch import nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def save_checkpoint(model, optimizer, epoch, loss, path):\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'epoch': epoch,\n",
        "        'loss': loss,\n",
        "        # Adicione outras informações que você deseja salvar, como hiperparâmetros, configurações, etc.\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "    \n",
        "def load_checkpoint(model, optimizer, path):\n",
        "    checkpoint = torch.load(path)\n",
        "    model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    if optimizer is not None:\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "    epoch = checkpoint['epoch']\n",
        "    loss = checkpoint['loss']\n",
        "    # Outras informações que você salvou no dicionário de checkpoint podem ser acessadas aqui\n",
        "    return model, optimizer, epoch, loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "AzVkVP59ekwW"
      },
      "outputs": [],
      "source": [
        "from typing import List, Optional\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from typing_extensions import TypedDict\n",
        "\n",
        "from kornia.core import Module, Tensor, concatenate\n",
        "from kornia.filters import SpatialGradient\n",
        "from kornia.geometry.transform import pyrdown\n",
        "from kornia.utils.helpers import map_location_to_cpu\n",
        "\n",
        "from kornia.feature.scale_space_detector import get_default_detector_config, MultiResolutionDetector,Detector_config\n",
        "\n",
        "\n",
        "class KeyNet_conf(TypedDict):\n",
        "    num_filters: int\n",
        "    num_levels: int\n",
        "    kernel_size: int\n",
        "    Detector_conf: Detector_config\n",
        "\n",
        "\n",
        "keynet_default_config: KeyNet_conf = {\n",
        "    # Key.Net Model\n",
        "    'num_filters': 8,\n",
        "    'num_levels': 3,\n",
        "    'kernel_size': 5,\n",
        "    # Extraction Parameters\n",
        "    'Detector_conf': {'nms_size': 5, 'pyramid_levels': 2, 'up_levels': 1, 'scale_factor_levels': 1.3, 's_mult': 20.0},\n",
        "}\n",
        "\n",
        "\n",
        "class _FeatureExtractor(Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.hc_block = _HandcraftedBlock()\n",
        "        self.lb_block = _LearnableBlock()\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x_hc = self.hc_block(x)\n",
        "        x_lb = self.lb_block(x_hc)\n",
        "        return x_lb\n",
        "\n",
        "\n",
        "class _HandcraftedBlock(Module):\n",
        "    def __init__(self) -> None:\n",
        "        super().__init__()\n",
        "        self.spatial_gradient = SpatialGradient('sobel', 1)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        sobel = self.spatial_gradient(x)\n",
        "        dx, dy = sobel[:, :, 0, :, :], sobel[:, :, 1, :, :]\n",
        "\n",
        "        sobel_dx = self.spatial_gradient(dx)\n",
        "        dxx, dxy = sobel_dx[:, :, 0, :, :], sobel_dx[:, :, 1, :, :]\n",
        "\n",
        "        sobel_dy = self.spatial_gradient(dy)\n",
        "        dyy = sobel_dy[:, :, 1, :, :]\n",
        "\n",
        "        hc_feats = concatenate([dx, dy, dx**2.0, dy**2.0, dx * dy, dxy, dxy**2.0, dxx, dyy, dxx * dyy], 1)\n",
        "\n",
        "        return hc_feats\n",
        "\n",
        "\n",
        "def _KeyNetConvBlock(\n",
        "    feat_type_in,\n",
        "    feat_type_out,\n",
        "    r2_act,\n",
        "    kernel_size: int = 5,\n",
        "    stride: int = 1,\n",
        "    padding: int = 2,\n",
        "    dilation: int = 1,\n",
        ") -> nn.Sequential:\n",
        "    return enn.SequentialModule(\n",
        "            enn.R2Conv(feat_type_in, feat_type_out, kernel_size=kernel_size, padding=padding, bias=False),\n",
        "            enn.InnerBatchNorm(feat_type_out),\n",
        "            enn.ReLU(feat_type_out, inplace=True),\n",
        "        )\n",
        "\n",
        "\n",
        "class _LearnableBlock(nn.Sequential):\n",
        "    def __init__(self, in_channels: int = 10, out_channels: int = 8, group_size=8) -> None:\n",
        "        super().__init__()\n",
        "        r2_act = gspaces.Rot2dOnR2(N=group_size)\n",
        "\n",
        "        feat_type_in = enn.FieldType(r2_act, in_channels * [r2_act.trivial_repr])\n",
        "        self.in_type = feat_type_in\n",
        "        feat_type_out = enn.FieldType(r2_act, out_channels * [r2_act.regular_repr])\n",
        "        self.block0 = _KeyNetConvBlock(feat_type_in, feat_type_out, r2_act)\n",
        "\n",
        "        feat_type_out = enn.FieldType(r2_act, out_channels * [r2_act.regular_repr])\n",
        "        self.block1 = _KeyNetConvBlock(self.block0.out_type, feat_type_out, r2_act)\n",
        "\n",
        "        feat_type_out = enn.FieldType(r2_act, out_channels * [r2_act.regular_repr])\n",
        "        self.block2 = _KeyNetConvBlock(self.block1.out_type, feat_type_out, r2_act)\n",
        "        self.gpool = enn.GroupPooling(self.block2.out_type)\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        x = enn.GeometricTensor(x, self.in_type)\n",
        "        x = self.block0(x)\n",
        "        x = self.block1(x)\n",
        "        x = self.block2(x)\n",
        "        x = self.gpool(x)\n",
        "        return x.tensor\n",
        "\n",
        "class KeyNet(Module):\n",
        "    def __init__(self, pretrained: bool = False, keynet_conf: KeyNet_conf = keynet_default_config) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        num_filters = keynet_conf['num_filters']\n",
        "        self.num_levels = keynet_conf['num_levels']\n",
        "        kernel_size = keynet_conf['kernel_size']\n",
        "        padding = kernel_size // 2\n",
        "\n",
        "        self.feature_extractor = _FeatureExtractor()\n",
        "        \n",
        "        self.last_conv = nn.Sequential(\n",
        "            nn.Conv2d(\n",
        "                in_channels=num_filters * self.num_levels, out_channels=1, kernel_size=kernel_size, padding=padding\n",
        "            ),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "        if pretrained:\n",
        "            KeyNet_URL =\"./data/models/key_map_ep123.pth\"\n",
        "            load_checkpoint(self,None, KeyNet_URL)\n",
        "            print(\"KeyNet loaded\")\n",
        "\n",
        "\n",
        "    def forward(self, x: Tensor) -> Tensor:\n",
        "        shape_im = x.shape\n",
        "        feats: List[Tensor] = [self.feature_extractor(x)]\n",
        "        for i in range(1, self.num_levels):\n",
        "            x = pyrdown(x, factor=1.2)\n",
        "            feats_i = self.feature_extractor(x)\n",
        "            feats_i = F.interpolate(feats_i, size=(shape_im[2], shape_im[3]), mode='bilinear')\n",
        "            feats.append(feats_i)\n",
        "        scores = self.last_conv(concatenate(feats, 1))\n",
        "        return scores\n",
        "\n",
        "\n",
        "class KeyNetDetector(MultiResolutionDetector):\n",
        "    def __init__(\n",
        "        self,\n",
        "        pretrained: bool = False,\n",
        "        num_features: int = 2048,\n",
        "        keynet_conf: KeyNet_conf = keynet_default_config,\n",
        "        ori_module: Optional[Module] = None,\n",
        "        aff_module: Optional[Module] = None,\n",
        "    ) -> None:\n",
        "        model = KeyNet(pretrained, keynet_conf)\n",
        "        super().__init__(model, num_features, keynet_conf['Detector_conf'], ori_module, aff_module)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "FAtdAEDnekwX"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "def criar_mascara(size_batch,dimensao_janela, tamanho_borda):\n",
        "    num_channels = 1\n",
        "    mascara = torch.zeros((size_batch,num_channels, dimensao_janela, dimensao_janela), dtype=torch.uint8)\n",
        "    mascara[..., tamanho_borda:-tamanho_borda, tamanho_borda:-tamanho_borda] = 1\n",
        "    return mascara.to(torch.float32)\n",
        "\n",
        "def my_similarity(a, b):\n",
        "    # a_norm = torch.nn.functional.normalize(a.view(a.size(0), -1), dim=-1)\n",
        "    # b_norm = torch.nn.functional.normalize(b.view(b.size(0), -1), dim=-1)\n",
        "    # return torch.cdist(a_norm, b_norm, p=2)\n",
        "    return torch.cdist(a.view(a.size(0), -1), b.view(b.size(0), -1), p=2)\n",
        "\n",
        "# Crie métodos para calcular a perda\n",
        "def loss_fn(map_anch, map_pos, margin=0.9):\n",
        "    similarities = my_similarity(map_anch, map_pos)\n",
        "    # Calcular a média da diagonal principal (âncoras vs. seus respectivos positivos)\n",
        "    mean_diagonal = torch.mean(torch.diagonal(similarities))\n",
        "    # Calcular a média dos outros elementos (âncoras vs. seus negativos correspondentes)\n",
        "    mean_other = torch.mean(similarities[~torch.eye(similarities.shape[0], dtype=torch.bool)])\n",
        "    losses = torch.relu(mean_diagonal - mean_other + margin)  # pos - neg + margin\n",
        "    return losses,mean_diagonal,mean_other\n",
        "\n",
        "\n",
        "def extract_feat_in_batch(model, batch_img):\n",
        "    repo_features = torch.tensor([], dtype=torch.float).to(batch_img.device)\n",
        "    for image in batch_img:\n",
        "        feature = model(image[None])  # Adicione 'None' ou 'unsqueeze(0)' se necessário\n",
        "        repo_features = torch.cat([repo_features, feature], dim=0)  # Coloque 'feature' dentro de uma lista []\n",
        "    return repo_features\n",
        "\n",
        "import gc\n",
        "from tqdm import tqdm\n",
        "def train_one_epoch(model, train_loader, loss_map, optimizer=None, device='cpu', transformations=None,is_training=True):\n",
        "    model.train(is_training) # Set model to training mode\n",
        "    total_loss = 0.\n",
        "    desc=\"Train \" if is_training else \"Test \"\n",
        "    t = tqdm(train_loader, desc=desc)\n",
        "    batch_i = 0\n",
        "    loss_maps = 0.\n",
        "    for batch_image, labels in t:\n",
        "        batch_image = batch_image.to(device)\n",
        "        mask = criar_mascara(batch_image.shape[0],batch_image.shape[-1],30).to(device)\n",
        "        features_key_summary = extract_feat_in_batch(model,batch_image*mask)\n",
        "\n",
        "        batch_t,mask_t,features_key_summary_t = transformations(batch_image,mask,features_key_summary)# transformar orientacoes e pontos\n",
        "        features_key_summary_t2 = extract_feat_in_batch(model,batch_image*mask)# prever os pontos da imagem transformada\n",
        "\n",
        "\n",
        "        loss,mean_diagonal,mean_other = loss_map(features_key_summary_t,features_key_summary_t2,margin = 70)\n",
        "\n",
        "        if is_training:\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        t.set_description(\"{} Loss: {:.5f} Mean AP {:.5f} Mean AN {:.5f}\".format(desc,loss,mean_diagonal,mean_other))\n",
        "        del features_key_summary\n",
        "        del batch_t, mask_t, features_key_summary_t\n",
        "        del features_key_summary_t2\n",
        "        gc.collect()\n",
        "        torch.cuda.empty_cache()\n",
        "        batch_i += 1\n",
        "    return total_loss/batch_i\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y6iKv51zekwY"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "6072\n"
          ]
        }
      ],
      "source": [
        "from teste_util import *\n",
        "import teste_util as TS\n",
        "\n",
        "# Fixar a semente do Torch para operações específicas\n",
        "fixed_seed()\n",
        "\n",
        "# leitura dos dados\n",
        "trainloader,testloader =read_dataload_flower(120,'./data/datasets')\n",
        "#gerar variacao de transformacoes pespectivas e fotometrica\n",
        "iterator=iter(trainloader)\n",
        "img,labels = next(iterator)\n",
        "params_lists =AugmentationParamsGenerator(6,img.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/wagner/miniconda3/envs/singular-points/lib/python3.9/site-packages/e2cnn/nn/modules/r2_conv/basisexpansion_singleblock.py:80: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647327249/work/aten/src/ATen/native/IndexingUtils.h:27.)\n",
            "  full_mask[mask] = norms.to(torch.uint8)\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "model_keynet = KeyNet().to(device)\n",
        "optimizer = optim.Adam(model_keynet.parameters(), lr=0.001, weight_decay=0.0001)\n",
        "epoch_i=0\n",
        "# model_keynet, optimizer, epoch_i, loss =load_checkpoint(model_keynet, optimizer,'./data/models/key_map_ep123.pth')\n",
        "# print(\"epoch_i \",epoch_i,\"loss \",loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sWEdgQlekwY",
        "outputId": "bf0f6498-1f6f-404f-a9e1-034c9c47187e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train  Loss: 62.78546 Mean AP 896.03326 Mean AN 903.24780: 100%|██████████| 17/17 [00:39<00:00,  2.33s/it]\n",
            "Test  Loss: 0.00000 Mean AP 602.06274 Mean AN 706.93750: 100%|██████████| 102/102 [01:43<00:00,  1.02s/it] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "salvou no colab\n",
            "Epoch [0/300] - Running Loss: 53.1452, Test Loss: 7.6109, Initial LR: 0.001000, Current LR: 0.001000, Epochs without Improvement: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Train  Loss: 45.87219 Mean AP 1136.40100 Mean AN 1160.52881: 100%|██████████| 17/17 [00:39<00:00,  2.32s/it]\n",
            "Test  Loss: 0.03210 Mean AP 1149.11499 Mean AN 1219.08289:  82%|████████▏ | 84/102 [01:24<00:18,  1.01s/it] \n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 63\u001b[0m\n\u001b[1;32m     60\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(best_model)\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Best Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m \u001b[43mtrain_with_early_stopping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_keynet\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[0;32mIn[8], line 40\u001b[0m, in \u001b[0;36mtrain_with_early_stopping\u001b[0;34m(model, trainloader, testloader, criterion_d, optimizer, scheduler, device, transformations, epochs, patience)\u001b[0m\n\u001b[1;32m     37\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m train_one_epoch(model, trainloader, loss_map\u001b[38;5;241m=\u001b[39mcriterion_d,  optimizer\u001b[38;5;241m=\u001b[39moptimizer, device\u001b[38;5;241m=\u001b[39mdevice, transformations\u001b[38;5;241m=\u001b[39mtransformations, is_training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 40\u001b[0m     loss_test \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtestloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion_d\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransformations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransformations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Verificar se a perda melhorou\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_test \u001b[38;5;241m<\u001b[39m best_loss:\n",
            "Cell \u001b[0;32mIn[5], line 45\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, train_loader, loss_map, optimizer, device, transformations, is_training)\u001b[0m\n\u001b[1;32m     43\u001b[0m batch_image \u001b[38;5;241m=\u001b[39m batch_image\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m mask \u001b[38;5;241m=\u001b[39m criar_mascara(batch_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],batch_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\u001b[38;5;241m30\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 45\u001b[0m features_key_summary \u001b[38;5;241m=\u001b[39m \u001b[43mextract_feat_in_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_image\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m batch_t,mask_t,features_key_summary_t \u001b[38;5;241m=\u001b[39m transformations(batch_image,mask,features_key_summary)\u001b[38;5;66;03m# transformar orientacoes e pontos\u001b[39;00m\n\u001b[1;32m     48\u001b[0m features_key_summary_t2 \u001b[38;5;241m=\u001b[39m extract_feat_in_batch(model,batch_image\u001b[38;5;241m*\u001b[39mmask)\u001b[38;5;66;03m# prever os pontos da imagem transformada\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[5], line 29\u001b[0m, in \u001b[0;36mextract_feat_in_batch\u001b[0;34m(model, batch_img)\u001b[0m\n\u001b[1;32m     27\u001b[0m repo_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(batch_img\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m batch_img:\n\u001b[0;32m---> 29\u001b[0m     feature \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adicione 'None' ou 'unsqueeze(0)' se necessário\u001b[39;00m\n\u001b[1;32m     30\u001b[0m     repo_features \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([repo_features, feature], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# Coloque 'feature' dentro de uma lista []\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m repo_features\n",
            "File \u001b[0;32m~/miniconda3/envs/singular-points/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/singular-points/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[4], line 135\u001b[0m, in \u001b[0;36mKeyNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_levels):\n\u001b[1;32m    134\u001b[0m     x \u001b[38;5;241m=\u001b[39m pyrdown(x, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m)\n\u001b[0;32m--> 135\u001b[0m     feats_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m     feats_i \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39minterpolate(feats_i, size\u001b[38;5;241m=\u001b[39m(shape_im[\u001b[38;5;241m2\u001b[39m], shape_im[\u001b[38;5;241m3\u001b[39m]), mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbilinear\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    137\u001b[0m     feats\u001b[38;5;241m.\u001b[39mappend(feats_i)\n",
            "File \u001b[0;32m~/miniconda3/envs/singular-points/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/singular-points/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[4], line 42\u001b[0m, in \u001b[0;36m_FeatureExtractor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m     41\u001b[0m     x_hc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhc_block(x)\n\u001b[0;32m---> 42\u001b[0m     x_lb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlb_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_hc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_lb\n",
            "File \u001b[0;32m~/miniconda3/envs/singular-points/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/singular-points/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "Cell \u001b[0;32mIn[4], line 104\u001b[0m, in \u001b[0;36m_LearnableBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    102\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock1(x)\n\u001b[1;32m    103\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblock2(x)\n\u001b[0;32m--> 104\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgpool\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\u001b[38;5;241m.\u001b[39mtensor\n",
            "File \u001b[0;32m~/miniconda3/envs/singular-points/lib/python3.9/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/singular-points/lib/python3.9/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
            "File \u001b[0;32m~/miniconda3/envs/singular-points/lib/python3.9/site-packages/e2cnn/nn/modules/invariantmaps/gpool.py:115\u001b[0m, in \u001b[0;36mGroupPooling.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;66;03m# split the channel dimension in 2 dimensions, separating fields\u001b[39;00m\n\u001b[1;32m    113\u001b[0m fm \u001b[38;5;241m=\u001b[39m fm\u001b[38;5;241m.\u001b[39mview(b, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, s, h, w)\n\u001b[0;32m--> 115\u001b[0m max_activations, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m contiguous:\n\u001b[1;32m    118\u001b[0m     output[:, out_indices[\u001b[38;5;241m0\u001b[39m]:out_indices[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;241m=\u001b[39m max_activations\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\n",
        "from torch import optim\n",
        "from torch.optim.lr_scheduler import ExponentialLR\n",
        "\n",
        "transforms = kornia.augmentation.AugmentationSequential(\n",
        "    kornia.augmentation.RandomAffine(degrees=360, translate=(0.2, 0.2), scale=(0.95, 1.05), shear=10,p=0.8),\n",
        "    kornia.augmentation.RandomPerspective(0.2, p=0.7),\n",
        "    kornia.augmentation.RandomBoxBlur((4,4),p=0.5),\n",
        "    # kornia.augmentation.RandomEqualize(p=0.3),\n",
        "    data_keys=[\"input\",\"input\",\"input\"],\n",
        "    same_on_batch=True,\n",
        "    # random_apply=10,\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "epochs=300\n",
        "i_epoch = 0\n",
        "loss = 0\n",
        "\n",
        "scheduler = ExponentialLR(optimizer, gamma=0.75)\n",
        "\n",
        "import torch\n",
        "\n",
        "def train_with_early_stopping(model, trainloader, testloader, criterion_d, optimizer, scheduler, device, transformations, epochs=100, patience=20):\n",
        "    best_loss = float('inf')\n",
        "    best_model = None\n",
        "    epochs_without_improvement = 0\n",
        "\n",
        "    for epoch in range(epoch_i,epochs):\n",
        "        # Atualizar a taxa de aprendizado\n",
        "        if (epoch % 5 == 0) and (epoch != 0):\n",
        "            scheduler.step()\n",
        "            \n",
        "\n",
        "        running_loss = train_one_epoch(model, trainloader, loss_map=criterion_d,  optimizer=optimizer, device=device, transformations=transformations, is_training=True)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            loss_test = train_one_epoch(model, testloader, loss_map=criterion_d,  optimizer=None, device=device, transformations=transformations, is_training=False)\n",
        "\n",
        "        # Verificar se a perda melhorou\n",
        "        if loss_test < best_loss:\n",
        "            best_loss = loss_test\n",
        "            epochs_without_improvement = 0\n",
        "            best_model = model.state_dict()            \n",
        "            save_checkpoint(model=model, epoch=epoch, optimizer=optimizer, loss=loss_test, path='./data/models/key_map_ep{}.pth'.format(epoch))\n",
        "            print(\"salvou no colab\")\n",
        "        else:\n",
        "            epochs_without_improvement += 1\n",
        "\n",
        "        # Verificar a condição de parada\n",
        "        if epochs_without_improvement == patience:\n",
        "            print(f\"No improvement in loss for {epochs_without_improvement} epochs. Training stopped.\")\n",
        "            break\n",
        "\n",
        "        print(f\"Epoch [{epoch}/{epochs}] - Running Loss: {running_loss:.4f}, Test Loss: {loss_test:.4f}, Initial LR: {optimizer.param_groups[0]['initial_lr']:.6f}, Current LR: {optimizer.param_groups[0]['lr']:.6f}, Epochs without Improvement: {epochs_without_improvement}\")\n",
        "\n",
        "    # Carregar a melhor configuração do modelo\n",
        "    model.load_state_dict(best_model)\n",
        "    print(f'Epoch: {epoch}, Best Loss: {best_loss:.4f}')\n",
        "\n",
        "train_with_early_stopping(model_keynet.to(device), trainloader, testloader, loss_fn, optimizer, scheduler, device, transforms, epochs=epochs, patience=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### APOS treinamento a validacao"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "detector = KeyNetDetector(pretrained=True, num_features=60, keynet_conf=keynet_default_config)\n",
        "detector.to(device).eval()\n",
        "\n",
        "descriptor = kornia.feature.SIFTDescriptor(TS.PS, rootsift=True).to(device)\n",
        "\n",
        "image = torch.rand(1, 1, 128, 128).to(device)  # Imagem de exemplo 128x128 pixels em escala de cinza\n",
        "\n",
        "# Faz a inferência com o modelo\n",
        "with torch.no_grad():\n",
        "    lafs, resps = detector(image)\n",
        "    print(lafs.shape, resps.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QEaon7EGMwsH"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import kornia\n",
        "from kornia_moons.viz import draw_LAF_matches\n",
        "\n",
        "def detect_and_extract_features(image, detector, descriptor, PS):\n",
        "    with torch.no_grad():\n",
        "        lafs, resps = detector(image[None])\n",
        "        patches = kornia.feature.extract_patches_from_pyramid(image[None], lafs, PS)\n",
        "        B, N, CH, H, W = patches.size()\n",
        "        # print('patches  ',patches.shape,B, N, CH, H, W,resps)\n",
        "        descs = descriptor(patches.view(B * N, CH, H, W)).view(B, N, -1)\n",
        "\n",
        "        return lafs, descs\n",
        "\n",
        "def detect_extract_feat_in_batch(batch_img, detector, descriptor, PS):\n",
        "    repo_lafs_desc = []\n",
        "    with torch.no_grad():\n",
        "        for image  in batch_img:\n",
        "            try:\n",
        "                lafs, descs = detect_and_extract_features(image, detector, descriptor, PS)\n",
        "                repo_lafs_desc.append((lafs,descs))\n",
        "            except RuntimeError as e:\n",
        "                print(\"erro ao extrair features\")\n",
        "\n",
        "    return repo_lafs_desc\n",
        "\n",
        "def matching_imagens(ref_img,batch_img, repo_lafs_desc,detector, descriptor):\n",
        "    best_match_info = None\n",
        "    best_match_count = 0\n",
        "    best_match_index = None\n",
        "    with torch.no_grad():\n",
        "        # Detectar e extrair características da imagem de referência\n",
        "        lafs_ref, descs_ref = detect_and_extract_features(ref_img, detector, descriptor, TS.PS)\n",
        "\n",
        "        for i, (lafs_i, descs_i) in enumerate(repo_lafs_desc):\n",
        "            # Detectar e extrair características da imagem atual do batch\n",
        "\n",
        "            # matches = bidirectional_match(descs_ref[0], descs_i[0], threshold=0.85)\n",
        "            scores, matches = kornia.feature.match_snn(descs_ref[0], descs_i[0], 0.85) # correspondencia dos descritories a uma distância de 0.9\n",
        "            if matches.shape[0] >= 4:\n",
        "                # Cálculo da homografia\n",
        "                inliers_mask = compute_homography(lafs_ref, lafs_i, matches)\n",
        "\n",
        "                # Check if this match is better than the previous best match\n",
        "                if matches.shape[0] > best_match_count:\n",
        "                    best_match_info = (lafs_ref[0][None].cpu(), lafs_i[0][None].cpu(), matches.cpu(),\n",
        "                                       kornia.tensor_to_image(ref_img.cpu()), kornia.tensor_to_image(batch_img[i].cpu()),\n",
        "                                       inliers_mask)\n",
        "                    best_match_count = matches.shape[0]\n",
        "                    best_match_index = i\n",
        "        if best_match_info is not None and best_match_index==0:# TODO: Remove this condition best_match_index==0\n",
        "            # Plot the best match\n",
        "\n",
        "            draw_LAF_matches(\n",
        "                *best_match_info,\n",
        "                draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": (1, 1, 0.2, 0.3), \"feature_color\": None, \"vertical\": False},\n",
        "            )\n",
        "        # else:\n",
        "        #     print(\"No matches found with enough inliers.\")\n",
        "    return best_match_index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AUaOBVrNlq-"
      },
      "outputs": [],
      "source": [
        "params_lists.aug_list.data_keys =[\"input\"]\n",
        "aug_list = params_lists.aug_list\n",
        "\n",
        "acertos = 0\n",
        "total = 0\n",
        "from tqdm.notebook import tqdm\n",
        "pbar =  tqdm(testloader)\n",
        "for imgs_batch,labels_batch in pbar:# itera em todo dataset\n",
        "    imgs_batch = imgs_batch.to(device)\n",
        "\n",
        "    params_item = next(params_lists)\n",
        "    timg_gray_t = aug_list(imgs_batch,params=params_item)\n",
        "    plt.show()\n",
        "    repo_lafs_desc= detect_extract_feat_in_batch(timg_gray_t,detector,descriptor,PS)\n",
        "\n",
        "    for i,img_gray in enumerate(imgs_batch):# itera em cada batch\n",
        "\n",
        "        match_index = matching_imagens(img_gray,timg_gray_t,repo_lafs_desc,detector, descriptor)\n",
        "\n",
        "        total+=1\n",
        "        if match_index == i:\n",
        "            acertos += 1\n",
        "        pbar.set_description(f\"acertos/total: {acertos}/{total}  \")\n",
        "print(\"acertos: \",acertos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    loss_test = train_one_epoch(detector.model, testloader, loss_map=loss_fn,  optimizer=None, device=device, transformations=transforms, is_training=False)\n",
        "    print(f\"Test loss: {loss_test:.4f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "singular-points",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

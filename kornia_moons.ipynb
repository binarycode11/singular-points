{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install kornia\n",
    "# !pip install kornia-rs\n",
    "# !pip install kornia_moons\n",
    "# !pip install opencv-python --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import kornia as K\n",
    "import kornia.augmentation as KA\n",
    "import kornia.feature as KF\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from kornia_moons.feature import *\n",
    "from kornia_moons.viz import *\n",
    "import timeit\n",
    "import time\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "POINTS_LIMIT = 90\n",
    "\n",
    "def criar_mascara(size_batch,dimensao_janela, tamanho_borda):\n",
    "    \n",
    "    mascara = torch.zeros((size_batch,1, dimensao_janela, dimensao_janela), dtype=torch.uint8)\n",
    "    mascara[..., tamanho_borda:-tamanho_borda, tamanho_borda:-tamanho_borda] = 1\n",
    "\n",
    "    return mascara.to(torch.float32)\n",
    "\n",
    "def predict_all_descritor(input_data,mask):\n",
    "    features_key,features_key_summary,features_ori,features_ori_summary,max_coords_values, max_map= model(input_data*mask)    \n",
    "    max_coords_temp = model.detector.sort_tensor_by_columns(max_coords_values)\n",
    "    batch_size = features_key.shape[0]\n",
    "    batch_points = torch.zeros((batch_size,POINTS_LIMIT, 2))\n",
    "    for i in range(batch_size):\n",
    "        max_coords_temp2 = model.detector.filter_coordinates(max_coords_temp, image_index=i, channel_index=0)[:POINTS_LIMIT, :2]\n",
    "    \n",
    "        try:\n",
    "            batch_points[i,:max_coords_temp2.shape[0]] = max_coords_temp2\n",
    "        except:\n",
    "            print(max_coords_temp2.shape,batch_points.shape)\n",
    "            print('nao tem pontos suficientes')\n",
    "    return features_key,features_ori,batch_points\n",
    "\n",
    "def create_circular_mask(h, w, center=None, radius=None):\n",
    "    \"\"\"\n",
    "    Create a circular mask of shape (h, w).\n",
    "    :param h: height of the mask\n",
    "    :param w: width of the mask\n",
    "    :param center: (x, y) tuple of the center\n",
    "    :param radius: radius of the circle\n",
    "    :return: (h, w) binary mask\n",
    "    \"\"\"\n",
    "\n",
    "    if center is None:  # use the middle of the image\n",
    "        center = (int(w / 2), int(h / 2))\n",
    "    if radius is None:  # use the smallest distance between the center and image walls\n",
    "        radius = min(center[0], center[1], w - center[0], h - center[1])\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((X - center[0]) ** 2 + (Y - center[1]) ** 2)\n",
    "    mask = dist_from_center <= radius\n",
    "    return mask\n",
    "  \n",
    "def get_bounding_boxs(batch,points,size=14):\n",
    "  '''\n",
    "  Recebe um lote de imagens e um conjunto de pontos e retorna uma lista de bounding boxs para cada imagem\n",
    "  '''\n",
    "  mask = create_circular_mask(size,size)\n",
    "  half1 = size // 2\n",
    "  half2 = size - half1\n",
    "  \n",
    "  num_batch = batch.shape[0]\n",
    "  num_layer = batch.shape[1]\n",
    "  num_poits = points.shape[1]\n",
    "  bounding_boxs = torch.zeros((num_batch,num_poits,num_layer,size,size),dtype=torch.float32)\n",
    "  \n",
    "  for image_idx,image in enumerate(batch):\n",
    "    for point_idx,point in enumerate(points[image_idx]):\n",
    "      y, x = map(int, point)\n",
    "      try:\n",
    "        bounding_boxs[image_idx,point_idx]=image[:,x - half1:x + half2, y - half1:y + half2]\n",
    "      except:\n",
    "        print(\"erro\")\n",
    "        print(x,y,half1,half2)\n",
    "  return bounding_boxs*mask\n",
    "\n",
    "# Cria uma camada de pooling global average\n",
    "global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "global_max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "# Aplica o pooling global average no tensor de entrada\n",
    "# output = global_avg_pool(x)\n",
    "\n",
    "\n",
    "def bidirectional_match(feat1, feat2):\n",
    "    # Converter para tipo de ponto flutuante\n",
    "    feat1 = feat1.float()\n",
    "    feat2 = feat2.float()\n",
    "\n",
    "    # Cálculo das diferenças entre pares de elementos\n",
    "    diff = feat1.unsqueeze(1) - feat2.unsqueeze(0)\n",
    "\n",
    "    # Cálculo da distância Euclidiana\n",
    "    dist_matrix = torch.norm(diff, p=2, dim=2)\n",
    "\n",
    "    # Encontrar os índices dos elementos mais próximos em ambas as direções\n",
    "    min_indices_feat1_to_feat2 = torch.argmin(dist_matrix, dim=1)\n",
    "    min_indices_feat2_to_feat1 = torch.argmin(dist_matrix, dim=0)\n",
    "\n",
    "    # Consolidar as correspondências únicas\n",
    "    matches = []\n",
    "    for i, match_index in enumerate(min_indices_feat1_to_feat2):\n",
    "        if min_indices_feat2_to_feat1[match_index] == i:\n",
    "            matches.append((i, match_index.item()))\n",
    "\n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=16, border_size=12, box_size=21, dim_first=3, dim_second=5, dim_third=8, epochs=70, exported=False, group_size=36, img_size=120, is_loss_ssim=True, margin_loss=2.0, nms_size=5, num_channels=3, outlier_rejection=False, path_data='./data', path_model='model.pt', pyramid_levels=3, scale_pyramid=1.3, show_feature=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wagner/Documentos/python/singular-points/myenv/lib/python3.8/site-packages/e2cnn/nn/modules/r2_conv/basisexpansion_singleblock.py:80: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  full_mask[mask] = norms.to(torch.uint8)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m path_siamese \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./data/models/sp_52.pth\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     18\u001b[0m model \u001b[38;5;241m=\u001b[39m sp\u001b[38;5;241m.\u001b[39mSingularPoints(args\u001b[38;5;241m=\u001b[39msp\u001b[38;5;241m.\u001b[39margs)\u001b[38;5;241m.\u001b[39mto(sp\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath_siamese\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model, filepath)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model, filepath):\n\u001b[0;32m---> 11\u001b[0m     model\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     12\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel loaded from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documentos/python/singular-points/myenv/lib/python3.8/site-packages/torch/serialization.py:1014\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1012\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1013\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1014\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1015\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1016\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1017\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1018\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1019\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1020\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmmap can only be used with files saved with \u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1021\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`torch.save(_use_new_zipfile_serialization=True), \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1022\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease torch.save your checkpoint with this option in order to use mmap.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documentos/python/singular-points/myenv/lib/python3.8/site-packages/torch/serialization.py:1422\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1420\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1421\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1422\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1424\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1425\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1426\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1427\u001b[0m )\n",
      "File \u001b[0;32m~/Documentos/python/singular-points/myenv/lib/python3.8/site-packages/torch/serialization.py:1392\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1391\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1392\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/Documentos/python/singular-points/myenv/lib/python3.8/site-packages/torch/serialization.py:1366\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1361\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1363\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1364\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1365\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1366\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1367\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1368\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1371\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/Documentos/python/singular-points/myenv/lib/python3.8/site-packages/torch/serialization.py:381\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    380\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 381\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    382\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    383\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/Documentos/python/singular-points/myenv/lib/python3.8/site-packages/torch/serialization.py:274\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 274\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    275\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    276\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/Documentos/python/singular-points/myenv/lib/python3.8/site-packages/torch/serialization.py:258\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    255\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 258\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    259\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    260\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    261\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    262\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    263\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    264\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "import best.singular_point as sp\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model, filepath):\n",
    "    model.load_state_dict(torch.load(filepath))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    \n",
    "\n",
    "# teste = model.copy()\n",
    "path_siamese = './data/models/sp_52.pth'\n",
    "model = sp.SingularPoints(args=sp.args).to(sp.device)\n",
    "load_model(model,path_siamese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = torch.rand(1,1,256,256)\n",
    "model(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms, InterpolationMode\n",
    "\n",
    "def image_to_batch_tensor(image_path, size):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((size, size)),\n",
    "        transforms.ToTensor()\n",
    "    ])\n",
    "    return transform(Image.open(image_path)).unsqueeze(0)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((sp.args.img_size, sp.args.img_size), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.Flowers102(root='./data/datasets', split='train',\n",
    "                                        download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "first_batch = next(iter(trainloader))[0].to(sp.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RADIUS = 10\n",
    "INDEX1 = 0\n",
    "INDEX2 = 0\n",
    "\n",
    "import torch\n",
    "import kornia as K\n",
    "\n",
    "def laf_from_opencv_kpts(xy: torch.Tensor, device: torch.device=torch.device('cpu')) -> torch.Tensor:\n",
    "    B, N, _ = xy.shape\n",
    "    scales = torch.zeros(B, N, 1, 1, device=device, dtype=torch.float)\n",
    "    angles = torch.zeros(B, N, 1, device=device, dtype=torch.float)\n",
    "    laf = K.feature.laf_from_center_scale_ori(xy, scales, angles).reshape(B, -1, 2, 3)\n",
    "    return laf\n",
    "\n",
    "\n",
    "simple_transformations = KA.AugmentationSequential(\n",
    "    KA.RandomAffine(degrees=180, translate=(0.1, 0.1), scale=(0.95, 1.05), shear=10,p=0.8),\n",
    "    same_on_batch=True,\n",
    "    data_keys = ['input','input']  # Especificando as chaves de dados\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_display_correspondences(batch_image, batch_image_trans):\n",
    "    with torch.no_grad():\n",
    "        mask = criar_mascara(first_batch.shape[0], first_batch.shape[-1], sp.args.border_size).to(sp.device)\n",
    "        features_key, features_ori, batch_points = predict_all_descritor(batch_image, mask)\n",
    "        if batch_image_trans is None:\n",
    "          batch_image_trans, mask_trans = simple_transformations(first_batch, mask)  # transformar orientacoes e pontos\n",
    "\n",
    "        features_key_t2, features_ori_t2, batch_points_t2 = predict_all_descritor(batch_image_trans, mask_trans)\n",
    "        \n",
    "        sub_key_1 = get_bounding_boxs(features_key.cpu(), batch_points, size=RADIUS*2)\n",
    "        sub_key_2 = get_bounding_boxs(features_key_t2.cpu(), batch_points_t2, size=RADIUS*2)\n",
    "        \n",
    "\n",
    "    desc_1 = torch.cat([global_avg_pool(sub_key_1[INDEX1]).squeeze(), global_max_pool(sub_key_1[INDEX1]).squeeze()], dim=1)\n",
    "    desc_2 = torch.cat([global_avg_pool(sub_key_2[INDEX2]).squeeze(), global_max_pool(sub_key_2[INDEX2]).squeeze()], dim=1)\n",
    "    \n",
    "    lafs1 = laf_from_opencv_kpts(batch_points)\n",
    "    lafs2 = laf_from_opencv_kpts(batch_points_t2)\n",
    "\n",
    "    match = np.array(bidirectional_match(desc_1, desc_2))\n",
    "    src_pts = np.float32([batch_points.numpy()[0, m[0]] for m in match])\n",
    "    dst_pts = np.float32([batch_points_t2.numpy()[0, m[1]] for m in match])\n",
    "\n",
    "    F, inliers_mask = cv2.findFundamentalMat(src_pts, dst_pts, cv2.USAC_MAGSAC, 0.25, 0.999, 1000)\n",
    "    print(lafs1.shape,lafs2.shape,match.shape,first_batch[0].cpu().numpy().transpose(1, 2, 0).shape,batch_image_trans[0].cpu().numpy().transpose(1, 2, 0).shape,inliers_mask.shape)\n",
    "    # Drawing matches using kornia_moons\n",
    "    draw_LAF_matches(\n",
    "        lafs1,\n",
    "        lafs2,\n",
    "        match,\n",
    "        first_batch[0].cpu().numpy().transpose(1, 2, 0),\n",
    "        batch_image_trans[0].cpu().numpy().transpose(1, 2, 0),\n",
    "        inliers_mask,\n",
    "        draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": None, \"feature_color\": None, \"vertical\": False},\n",
    "    )\n",
    "    print(f\"{inliers_mask.sum()} inliers found\")\n",
    "    \n",
    "    \n",
    "detect_and_display_correspondences(first_batch, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import kornia.feature as KF\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms, InterpolationMode\n",
    "import kornia.augmentation as KA\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((120,120), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.Flowers102(root='./data/datasets', split='train',\n",
    "                                        download=True, transform=transform2)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "\n",
    "\n",
    "def detect_and_match(first_batch,second_batch,keynet_hardnet,matcher):\n",
    "    kp1, _, desc1 = keynet_hardnet(first_batch)\n",
    "    kp2, _, desc2 = keynet_hardnet(second_batch)\n",
    "\n",
    "    # Remove a dimensão extra dos descritores\n",
    "    desc1 = desc1[0]\n",
    "    desc2 = desc2[0]\n",
    "    # Realiza a correspondência dos descritores\n",
    "    dist,matches = matcher(desc1, desc2)\n",
    "    print(desc1.shape,desc2.shape,dist.shape,matches.shape)\n",
    "    return kp1,kp2,matches\n",
    "    \n",
    "keynet_hardnet = KF.KeyNetHardNet()\n",
    "matcher = KF.DescriptorMatcher('snn', 0.5)\n",
    "\n",
    "for data in trainloader:\n",
    "    first_batch,_ = data\n",
    "    random = KA.RandomAffine(degrees=180, translate=(0.05, 0.05), scale=(0.95, 1.05), shear=5, p=0.8)\n",
    "    second_batch = random(first_batch)\n",
    "    \n",
    "    \n",
    "    print(first_batch.shape,second_batch.shape)\n",
    "    plt.imshow(first_batch[0].permute(1,2,0))\n",
    "    plt.show()\n",
    "    plt.imshow(second_batch[0].permute(1,2,0))\n",
    "    plt.show()\n",
    "    kp1,kp2,matches =detect_and_match(first_batch,second_batch,keynet_hardnet,matcher)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(first_batch.shape,second_batch.shape)\n",
    "plt.imshow(first_batch[0].permute(1,2,0))\n",
    "plt.show()\n",
    "plt.imshow(second_batch[0].permute(1,2,0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia.augmentation as KA\n",
    "import kornia.geometry.conversions as KGC\n",
    "# Definir as transformações individuais\n",
    "random_affine = KA.RandomAffine(degrees=180, translate=(0.1, 0.1), scale=(0.95, 1.05), shear=10, p=0.8)\n",
    "color_jitter = KA.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2)\n",
    "random_horizontal_flip = KA.RandomHorizontalFlip(p=0.5)\n",
    "random_vertical_flip = KA.RandomVerticalFlip(p=0.5)\n",
    "\n",
    "# Acessar os parâmetros das transformações individuais\n",
    "affine_params = random_affine.forward_parameters((120,120))\n",
    "jitter_params = color_jitter.parameters\n",
    "print(affine_params.keys())\n",
    "# Imprimir os parâmetros\n",
    "print(\"RandomAffine Parameters:\", affine_params['center'].shape,affine_params['scale'].shape,affine_params['angle'].shape,affine_params['batch_prob'].shape)\n",
    "print(\"ColorJitter Parameters:\", jitter_params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 1, 120, 120])\n",
      "Namespace(exported=False, num_channels=1, pyramid_levels=3, scale_pyramid=1.3, dim_first=3, dim_second=5, dim_third=8, group_size=36, epochs=70, border_size=12, box_size=21, nms_size=5, img_size=120, batch_size=16, path_data='./data', path_model='model.pt', is_loss_ssim=True, margin_loss=2.0, outlier_rejection=False, show_feature=False)\n",
      "Model loaded from ./data/models/sp_map_fo_30.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import kornia\n",
    "import best.singular_point as sp\n",
    "from teste_util import *\n",
    "\n",
    "# Fixar a semente do Torch para operações específicas\n",
    "fixed_seed()\n",
    "\n",
    "# leitura dos dados\n",
    "trainloader,testloader =read_dataload_flower(sp.args.img_size,'./data/datasets',batch_size=60)\n",
    "# trainloader,testloader =read_dataload_fibers(sp.args.img_size)\n",
    "# trainloader,testloader=read_dataload_woods(sp.args.img_size)\n",
    "iterator=iter(testloader)\n",
    "img,labels = next(iterator)\n",
    "print(img.shape)\n",
    "\n",
    "# Carregar o modelo singular points\n",
    "path_siamese = './data/models/sp_map_fo_30.pth'\n",
    "# path_siamese = './data/models/sp2_75.pth'\n",
    "\n",
    "sp.args.num_channels = 1\n",
    "model = sp.SingularPoints(args=sp.args).to(sp.device)\n",
    "load_model(model,path_siamese,sp.device)\n",
    "\n",
    "#gerar variacao de transformacoes pespectivas e fotometrica\n",
    "params_lists =AugmentationParamsGenerator(6,img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from kornia.feature.scale_space_detector import get_default_detector_config, MultiResolutionDetector\n",
    "from kornia.feature.keynet import KeyNetDetector\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "keynet_default_config = {\n",
    "    'num_filters': 8,\n",
    "    'num_levels': 3,\n",
    "    'kernel_size': 5,\n",
    "    # 'Detector_conf': {'nms_size': 15, 'pyramid_levels': 1, 'up_levels': 1, 'scale_factor_levels': 1.3, 's_mult': 22.0},\n",
    "    'Detector_conf': {'nms_size': 5, 'pyramid_levels': 0, 'up_levels': 0, 'scale_factor_levels': 1.0, 's_mult': 5.0},\n",
    "   #  'Detector_conf': {'nms_size': 5, 'pyramid_levels': 3, 'up_levels': 2, 'scale_factor_levels': 1.3, 's_mult': 12.0},# flowers 420/1020  0.411\n",
    "   #  'Detector_conf': {'nms_size': 7, 'pyramid_levels': 2, 'up_levels': 1, 'scale_factor_levels': 1.3, 's_mult': 14.0}#fibers\n",
    "   #  'Detector_conf': {'nms_size': 7, 'pyramid_levels': 2, 'up_levels': 1, 'scale_factor_levels': 1.3, 's_mult': 14.0}#woods 253/496 - 0.510\n",
    "#    'Detector_conf': {'nms_size': 9, 'pyramid_levels': 2, 'up_levels': 1, 'scale_factor_levels': 1.5, 's_mult': 14.0},#woods  hard 262/496 - 0.528\n",
    "   # 'Detector_conf': {'nms_size': 7, 'pyramid_levels': 2, 'up_levels': 1, 'scale_factor_levels': 1.3, 's_mult': 14.0},#woods  sift 245/496 - 0.494\n",
    "}\n",
    "descritor = kornia.feature.SIFTDescriptor(13, rootsift=True)\n",
    "detector1 = KeyNetDetector(num_features=60, keynet_conf=keynet_default_config,ori_module=kornia.feature.LAFOrienter(32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_230127/3150767562.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load('trained_models/pretrained_nets/HardNet++.pth')\n"
     ]
    }
   ],
   "source": [
    "from external.hardnet_pytorch import HardNet        \n",
    "class Descriptor(nn.Module):\n",
    "        def __init__(self) -> None:\n",
    "           super().__init__()           \n",
    "           hardnet = HardNet()\n",
    "           checkpoint = torch.load('trained_models/pretrained_nets/HardNet++.pth')\n",
    "           hardnet.load_state_dict(checkpoint['state_dict'])\n",
    "           hardnet.eval()\n",
    "           hardnet.to(sp.device) \n",
    "           self.model = hardnet\n",
    "        def forward(self, x):\n",
    "           return self.model(x)\n",
    "descritor = Descriptor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(nn.Module):\n",
    "        def __init__(self,model) -> None:\n",
    "           super().__init__()\n",
    "           self.model = model\n",
    "        def forward(self, x):\n",
    "            features_key,features_key_summary,features_ori,features_ori_summary,max_coords_values, max_map= self.model(x)\n",
    "            return features_key_summary\n",
    "detec = Detector(model)\n",
    "detector1 = CustomNetDetector(detec,PS=32,keynet_conf=keynet_default_config,num_features=60).to(sp.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import argparse\n",
    "\n",
    "# ## for fix seed\n",
    "# import random, torch, numpy \n",
    "# def get_config(jupyter=False):\n",
    "#     parser = argparse.ArgumentParser(description='Train REKD Architecture')\n",
    "\n",
    "#     ## basic configuration\n",
    "#     parser.add_argument('--data_dir', type=str, default='../ImageNet2012/ILSVRC2012_img_val', #default='path-to-ImageNet',\n",
    "#                             help='The root path to the data from which the synthetic dataset will be created.')\n",
    "#     parser.add_argument('--synth_dir', type=str, default='', \n",
    "#                             help='The path to save the generated sythetic image pairs.')\n",
    "#     parser.add_argument('--log_dir', type=str, default='trained_models/weights',\n",
    "#                             help='The path to save the REKD weights.')\n",
    "#     parser.add_argument('--load_dir', type=str, default='',\n",
    "#                         help='Set saved model parameters if resume training is desired.')                            \n",
    "#     parser.add_argument('--exp_name', type=str, default='REKD',\n",
    "#                             help='The Rotaton-equivaraiant Keypoint Detection (REKD) experiment name')\n",
    "#     ## network architecture\n",
    "#     parser.add_argument('--factor_scaling_pyramid', type=float, default=1.2,\n",
    "#                         help='The scale factor between the multi-scale pyramid levels in the architecture.')\n",
    "#     parser.add_argument('--group_size', type=int, default=36,  \n",
    "#                         help='The number of groups for the group convolution.')\n",
    "#     parser.add_argument('--dim_first', type=int, default=2,\n",
    "#                         help='The number of channels of the first layer')\n",
    "#     parser.add_argument('--dim_second', type=int, default=2,\n",
    "#                         help='The number of channels of the second layer')\n",
    "#     parser.add_argument('--dim_third', type=int, default=2,\n",
    "#                         help='The number of channels of the thrid layer')                       \n",
    "#     ## network training\n",
    "#     parser.add_argument('--batch_size', type=int, default=16,\n",
    "#                         help='The batch size for training.')\n",
    "#     parser.add_argument('--num_epochs', type=int, default=20, \n",
    "#                         help='Number of epochs for training.')\n",
    "#     ## Loss function  \n",
    "#     parser.add_argument('--init_initial_learning_rate', type=float, default=1e-3,\n",
    "#                         help='The init initial learning rate value.')\n",
    "#     parser.add_argument('--MSIP_sizes', type=str, default=\"8,16,24,32,40\",\n",
    "#                         help='MSIP sizes.')\n",
    "#     parser.add_argument('--MSIP_factor_loss', type=str, default=\"256.0,64.0,16.0,4.0,1.0\",\n",
    "#                         help='MSIP loss balancing parameters.')\n",
    "#     parser.add_argument('--ori_loss_balance', type=float, default=100., \n",
    "#                         help='')\n",
    "#     ## Dataset generation\n",
    "#     parser.add_argument('--patch_size', type=int, default=192,\n",
    "#                         help='The patch size of the generated dataset.')\n",
    "#     parser.add_argument('--max_angle', type=int, default=180,\n",
    "#                         help='The max angle value for generating a synthetic view to train REKD.')\n",
    "#     parser.add_argument('--min_scale', type=float, default=1.0,\n",
    "#                         help='The min scale value for generating a synthetic view to train REKD.')\n",
    "#     parser.add_argument('--max_scale', type=float, default=1.0,\n",
    "#                         help='The max scale value for generating a synthetic view to train REKD.')\n",
    "#     parser.add_argument('--max_shearing', type=float, default=0.0,\n",
    "#                         help='The max shearing value for generating a synthetic view to train REKD.')\n",
    "#     parser.add_argument('--num_training_data', type=int, default=9000,\n",
    "#                         help='The number of the generated dataset.')\n",
    "#     parser.add_argument('--is_debugging', type=bool, default=False,\n",
    "#                         help='Set variable to True if you desire to train network on a smaller dataset.')\n",
    "#     ## For eval/inference\n",
    "#     parser.add_argument('--num_points', type=int, default=1500,\n",
    "#                         help='the number of points at evaluation time.')\n",
    "#     parser.add_argument('--pyramid_levels', type=int, default=5,\n",
    "#                         help='downsampling pyramid levels.')\n",
    "#     parser.add_argument('--upsampled_levels', type=int, default=2,\n",
    "#                         help='upsampling image levels.')\n",
    "#     parser.add_argument('--nms_size', type=int, default=15,\n",
    "#                         help='The NMS size for computing the validation repeatability.')\n",
    "#     parser.add_argument('--border_size', type=int, default=15,\n",
    "#                         help='The number of pixels to remove from the borders to compute the repeatability.')\n",
    "#     ## For HPatches evaluation\n",
    "#     parser.add_argument('--hpatches_path', type=str, default='./datasets/hpatches-sequences-release',\n",
    "#                         help='dataset ')\n",
    "#     parser.add_argument('--eval_split', type=str, default='debug',\n",
    "#                         help='debug, view, illum, full, debug_view, debug_illum ...')      \n",
    "#     parser.add_argument('--descriptor', type=str, default=\"hardnet\",\n",
    "#                         help='hardnet, sosnet, hynet')    \n",
    "\n",
    "#     args = parser.parse_args() if not jupyter else parser.parse_args(args=[])   \n",
    "\n",
    "#     fixed_seed()\n",
    "\n",
    "#     if args.synth_dir == \"\":\n",
    "#         args.synth_dir = 'datasets/synth_data'\n",
    "\n",
    "#     args.MSIP_sizes = [int(i) for i in args.MSIP_sizes.split(\",\")]\n",
    "#     args.MSIP_factor_loss =[float(i) for i in args.MSIP_factor_loss.split(\",\")]\n",
    "\n",
    "#     return args\n",
    "\n",
    "\n",
    "# from external.REKD import REKD, count_model_parameters\n",
    "# from external.hardnet_pytorch import HardNet\n",
    "\n",
    "# args = get_config(jupyter=True)\n",
    "# args.load_dir =  'trained_models/release_group36_f2_s2_t2.log/best_model.pt'\n",
    "\n",
    "# from torch import nn\n",
    "# class Detector(nn.Module):\n",
    "#         def __init__(self) -> None:\n",
    "#            super().__init__()\n",
    "#            detec = REKD(args, device)\n",
    "#            detec = detec.to(device) ## use GPU\n",
    "#            detec.load_state_dict(torch.load(args.load_dir))  ## Load the PyTorch learnable model parameters.\n",
    "#            detec.eval()\n",
    "#            self.model = detec\n",
    "           \n",
    "#         def forward(self, x):\n",
    "#             features_key,features_ori= self.model(x)\n",
    "#             return features_key\n",
    "\n",
    "\n",
    "# detec = Detector()\n",
    "# detector1 = CustomNetDetector(detec,PS=32,keynet_conf=keynet_default_config,num_features=60).to(sp.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kornia.feature import laf_from_center_scale_ori\n",
    "def convert_points_to_lafs(points,img1, PS=19,scale=6):\n",
    "    orient = kornia.feature.LAFOrienter(PS)#kornia.feature.LAFOrienter(PS)PassLAF()\n",
    "    scale_lafs = torch.ones(img1.shape[0],points.shape[1],1,1)*scale\n",
    "    scale_lafs = scale_lafs.to(img1.device)\n",
    "    points = points.to(img1.device)\n",
    "    lafs1 = laf_from_center_scale_ori(points,scale_lafs)\n",
    "    lafs2 = orient(lafs1, img1)\n",
    "    return lafs2\n",
    "    \n",
    "def extract_patches_simple(batch, lafs, PS=19):\n",
    "    imgs_patches = kornia.feature.extract_patches_from_pyramid(batch, lafs, PS)\n",
    "    imgs_patches =imgs_patches.reshape(-1,imgs_patches.shape[2],PS,PS)\n",
    "    return imgs_patches\n",
    "\n",
    "def plot_patches_side_by_side(imgs_patches):\n",
    "    num_imgs = imgs_patches.shape[0]  # Número de imagens\n",
    "    fig, axs = plt.subplots(1, num_imgs, figsize=(num_imgs*4, 4))\n",
    "\n",
    "    axs = axs.reshape((1, num_imgs))  # Ajustar a forma para matriz 2D com uma única linha\n",
    "\n",
    "    for i in range(num_imgs):\n",
    "        axs[0, i].imshow(kornia.tensor_to_image(imgs_patches[i]))\n",
    "        axs[0, i].axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import kornia.feature as KF\n",
    "import matplotlib.pyplot as plt\n",
    "import kornia as K\n",
    "\n",
    "def visualize_LAF(img, LAF, img_idx = 0):\n",
    "    x, y = KF.laf.get_laf_pts_to_draw(LAF, img_idx)\n",
    "    print(x[0][:5],y[0][:5])\n",
    "    plt.figure()\n",
    "    plt.imshow(K.utils.tensor_to_image(img[img_idx]))\n",
    "    plt.plot(x, y, 'r')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def plot_matches_keypoints(image1, keypoints1, image2, keypoints2, matches, **kwargs):\n",
    "    print('image1 shape: ',image1.shape,image1.dtype,image2.shape,image2.dtype)\n",
    "    # Concatenar as duas imagens lado a lado\n",
    "    combined_image = np.concatenate((image1, image2), axis=1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.imshow(combined_image)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Desenhar pontos correspondentes e linhas conectando-os\n",
    "    offset = image1.shape[1]\n",
    "\n",
    "    for i, (x, y) in enumerate(keypoints1):\n",
    "        ax.plot(x, y, 'o',markerfacecolor='none', markeredgecolor='r',\n",
    "                markersize=20, markeredgewidth=1)\n",
    "        ax.annotate(str(i), (x, y), color='r',xytext=(10, 10), textcoords='offset points', fontsize=12)\n",
    "\n",
    "    for i, (x, y) in enumerate(keypoints2):\n",
    "        ax.plot(x+offset, y, 'o',markerfacecolor='none', markeredgecolor='r',\n",
    "                markersize=20, markeredgewidth=1)\n",
    "        ax.annotate(str(i), (x+offset, y), color='r',xytext=(10, 10), textcoords='offset points', fontsize=12)\n",
    "\n",
    "    for match in matches:\n",
    "        x1, y1 = keypoints1[match[0],0], keypoints1[match[0],1]\n",
    "        x2, y2 = keypoints2[match[1],0]+offset, keypoints2[match[1],1]\n",
    "        ax.plot([x1, x2], [y1, y2], '-', color='lime', lw=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_image_with_keypoints(image_tensor, keypoints_tensor):\n",
    "    # Converter a imagem tensorial em objeto PIL.Image\n",
    "    image = kornia.utils.tensor_to_image(image_tensor)\n",
    "    # Plotar a imagem e os keypoints\n",
    "    plt.imshow(image)\n",
    "    if keypoints_tensor is not None:\n",
    "        # Extrair as coordenadas x e y dos keypoints\n",
    "        keypoints_x = keypoints_tensor[:,0].flatten().tolist()\n",
    "        keypoints_y = keypoints_tensor[:,1].flatten().tolist()\n",
    "        plt.scatter(keypoints_x, keypoints_y, c='red')\n",
    "    plt.show()\n",
    "    \n",
    "def filtrar_keypoints(lista_de_pontos, tensor_mascara):\n",
    "    # Verificar se as coordenadas estão dentro das dimensões\n",
    "    dimensao_max_x, dimensao_max_y = tensor_mascara.shape[1] - 1, tensor_mascara.shape[0] - 1\n",
    "    pontos_filtrados = [\n",
    "        ponto.tolist()  for ponto in lista_de_pontos \n",
    "        if 0 <= ponto[0] <= dimensao_max_x \n",
    "        and 0 <= ponto[1] <= dimensao_max_y \n",
    "        and tensor_mascara[int(ponto[1]), int(ponto[0])] \n",
    "    ]\n",
    "    return torch.tensor(pontos_filtrados)\n",
    "\n",
    "def filtrar_keypoints_conjuntos(lista_de_pontos_1, lista_de_pontos_2, tensor_mascara):\n",
    "    dimensao_max_x, dimensao_max_y = tensor_mascara.shape[1] - 1, tensor_mascara.shape[0] - 1\n",
    "    pontos_filtrados_1 = []\n",
    "    pontos_filtrados_2 = []\n",
    "    \n",
    "    for ponto_1, ponto_2 in zip(lista_de_pontos_1.cpu(), lista_de_pontos_2.cpu()):\n",
    "        x, y = ponto_1\n",
    "        if 0 <= x <= dimensao_max_x and 0 <= y <= dimensao_max_y and tensor_mascara[int(y), int(x)]:\n",
    "            pontos_filtrados_1.append(ponto_1.numpy())\n",
    "            pontos_filtrados_2.append(ponto_2.numpy())\n",
    "            \n",
    "    return torch.tensor(pontos_filtrados_1), torch.tensor(pontos_filtrados_2)\n",
    "\n",
    "def find_best_matching_indices_knn(points1, points2, threshold, k=3):\n",
    "    if len(points1) == 0 or len(points2) == 0:\n",
    "        return []\n",
    "    distances = cdist(points1, points2)\n",
    "    best_indices = np.argsort(distances, axis=1)[:, :k]\n",
    "    best_distances = np.take_along_axis(distances, best_indices, axis=1)\n",
    "\n",
    "    matched = []\n",
    "\n",
    "    for i in range(len(points1)):\n",
    "        min_distance = np.min(best_distances[i])\n",
    "        if min_distance < threshold:\n",
    "            best_index = np.argmin(best_distances[i])\n",
    "            matched.append((i, best_indices[i, best_index]))\n",
    "\n",
    "    return matched\n",
    "\n",
    "def calcular_associacao(matches1, matches2):\n",
    "    intersecao = set(tuple(match) for match in matches1) & set(tuple(match) for match in matches2)\n",
    "    # assoc_score = len(intersecao) / min(matches1.shape[0], matches2.shape[0])\n",
    "    return intersecao #assoc_score\n",
    "\n",
    "\n",
    "def detect_extract_feat_in_batch(batch1,aug_list, detector):\n",
    "    total = []\n",
    "    intersecao_total = []\n",
    "    with torch.no_grad():\n",
    "        for img1  in batch1:            \n",
    "            lafs1, resps1 = detector(img1[None])\n",
    "            B,C,H,W = img1[None].shape\n",
    "            mask = torch.ones(B,C,H,W).to(img1.device) \n",
    "            #lafs1 to points1\n",
    "            points1 =kornia.feature.get_laf_center(lafs1)   \n",
    "\n",
    "            if( points1.shape[1] == 0):\n",
    "                # print('aug_list shape: ',points1.shape) \n",
    "                continue                    \n",
    "            img2,mask_t,ponts_t=aug_list(img1,mask,points1)     \n",
    "            img2 = img2.to(img1.device)                   \n",
    "            lafs2, resps2 = detector(img2)            \n",
    "            points2 =kornia.feature.get_laf_center(lafs2)\n",
    "            # visualize_LAF(img2, lafs2, 0)\n",
    "                        \n",
    "            # pontos filtrados com base da mascara\n",
    "            filtered_points1,filtered_points0 = filtrar_keypoints_conjuntos(ponts_t[0],points1[0],mask_t[0,0].bool())\n",
    "            filtered_points2 = filtrar_keypoints(points2[0],mask_t[0,0].bool())                        \n",
    "            # print('filtered shape: ',filtered_points1.shape,filtered_points2.shape)\n",
    "            if( filtered_points1.shape[0] == 0 or filtered_points2.shape[0] == 0):\n",
    "                # print('filtered shape: ',filtered_points1.shape,filtered_points2.shape)\n",
    "                continue                        \n",
    "            \n",
    "            #0.5 1.0 1.5 3.5\n",
    "            matches = find_best_matching_indices_knn(filtered_points1.cpu(), filtered_points2.cpu(), threshold=1.5, k=1)\n",
    "            if(len(matches) == 0):\n",
    "                # print('matches shape: ',len(matches))\n",
    "                continue\n",
    "            \n",
    "            # print('filtered_points1 shape: ',filtered_points0.shape,filtered_points1.shape,filtered_points2.shape)\n",
    "            lafs1 = convert_points_to_lafs(filtered_points0[None],img1[None], PS=19,scale=5)\n",
    "            lafs2 = convert_points_to_lafs(filtered_points2[None],img2, PS=19,scale=5)\n",
    "            # print('lafs1 shape: ',lafs1.shape,lafs2.shape,img1.shape,img2.shape)\n",
    "            patch1 = extract_patches_simple(img1[None], lafs1, PS=32)# TODO 13 para sift e 32 hardnet\n",
    "            patch2 = extract_patches_simple(img2, lafs2, PS=32)# TODO 13 para sift e 32 hardnet\n",
    " \n",
    "            B, N, CH, H, W = patch1[None].size()       \n",
    "            # print(B, N, CH, H, W) \n",
    "            desc1 =descritor(patch1.view(B * N, CH, H, W))\n",
    "            B, N, CH, H, W = patch2[None].size()\n",
    "            desc2 =descritor(patch2.view(B * N, CH, H, W))                        \n",
    "            #TODO: verificar a correspondencia entre os descritores\n",
    "\n",
    "            dist,match_desc = kornia.feature.match_smnn(desc1, desc2, th=0.8) \n",
    "            \n",
    "            # print('calcular_associacao ',match_desc.shape,len(matches))            \n",
    "            intersecao = calcular_associacao(match_desc.cpu().numpy(), np.array(matches)) \n",
    "            total.append(len(matches))\n",
    "            intersecao_total.append(len(intersecao))            \n",
    "            # plot_matches_keypoints(img2[0,0].cpu().numpy(), filtered_points1.cpu().numpy(), img2[0,0].cpu().numpy(), filtered_points2.cpu().numpy(), matches)\n",
    "            # plot_matches_keypoints(img2[0,0].cpu().numpy(), filtered_points1.cpu().numpy(), img2[0,0].cpu().numpy(), filtered_points2.cpu().numpy(), match_desc)\n",
    "    print('total: ',np.sum(total),' intersecao: ',np.sum(intersecao_total))\n",
    "    return (np.sum(intersecao_total)/np.sum(total))*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c075a6d9df7b4451aee1c30f4a485545",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total:  1870  intersecao:  974\n",
      "52.0855614973262\n",
      "total:  1844  intersecao:  892\n",
      "48.37310195227766\n",
      "total:  1695  intersecao:  655\n",
      "38.64306784660767\n",
      "total:  1904  intersecao:  875\n",
      "45.955882352941174\n",
      "total:  1961  intersecao:  1111\n",
      "56.65476797552269\n",
      "total:  1799  intersecao:  955\n",
      "53.085047248471376\n",
      "total:  1934  intersecao:  988\n",
      "51.08583247156153\n",
      "total:  1886  intersecao:  960\n",
      "50.90137857900318\n",
      "total:  1756  intersecao:  827\n",
      "47.09567198177677\n",
      "total:  1866  intersecao:  1038\n",
      "55.62700964630225\n",
      "total:  1737  intersecao:  793\n",
      "45.65342544617156\n",
      "total:  1734  intersecao:  796\n",
      "45.905420991926185\n",
      "total:  1932  intersecao:  965\n",
      "49.94824016563147\n",
      "total:  1813  intersecao:  884\n",
      "48.75896304467733\n",
      "total:  1933  intersecao:  1026\n",
      "53.078116916709774\n",
      "total:  1800  intersecao:  830\n",
      "46.111111111111114\n",
      "total:  1811  intersecao:  917\n",
      "50.63500828271673\n",
      "match of dataset  49.38809455945498\n"
     ]
    }
   ],
   "source": [
    "fixed_seed()\n",
    "params_lists.aug_list.data_keys =[\"input\"]\n",
    "\n",
    "transforms = kornia.augmentation.AugmentationSequential(\n",
    "    kornia.augmentation.RandomAffine(degrees=360, translate=(0.2, 0.2), scale=(0.95, 1.05), shear=10,p=0.8),\n",
    "    kornia.augmentation.RandomPerspective(0.2, p=0.7),\n",
    "    kornia.augmentation.RandomBoxBlur((4,4),p=0.5),\n",
    "    # kornia.augmentation.RandomEqualize(p=0.3),\n",
    "    data_keys=[\"input\", \"mask\",\"keypoints\"],\n",
    "    same_on_batch=True,\n",
    "    # random_apply=10,\n",
    ")\n",
    "\n",
    "acertos = 0\n",
    "total = 0\n",
    "from tqdm.notebook import tqdm\n",
    "pbar =  tqdm(testloader)\n",
    "list_acc = []\n",
    "for imgs_batch,labels_batch in pbar:# itera em todo dataset\n",
    "    imgs_batch = imgs_batch.to(device)\n",
    "    _acc = detect_extract_feat_in_batch(imgs_batch,transforms,detector1)\n",
    "    print(_acc)\n",
    "    list_acc.append(_acc)\n",
    "print('match of dataset ',np.mean(list_acc))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "singular-points",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

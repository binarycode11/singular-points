{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/datasets/fibers/doc_000.jpg', './data/datasets/fibers/doc_001.jpg', './data/datasets/fibers/doc_002.jpg', './data/datasets/fibers/doc_003.jpg', './data/datasets/fibers/doc_004.jpg', './data/datasets/fibers/doc_005.jpg', './data/datasets/fibers/doc_006.jpg', './data/datasets/fibers/doc_007.jpg', './data/datasets/fibers/doc_008.jpg', './data/datasets/fibers/doc_010.jpg', './data/datasets/fibers/doc_011.jpg', './data/datasets/fibers/doc_012.jpg', './data/datasets/fibers/doc_0128.jpg', './data/datasets/fibers/doc_0129.jpg', './data/datasets/fibers/doc_013.jpg', './data/datasets/fibers/doc_0130.jpg', './data/datasets/fibers/doc_0131.jpg', './data/datasets/fibers/doc_0132.jpg', './data/datasets/fibers/doc_0133.jpg', './data/datasets/fibers/doc_0134.jpg', './data/datasets/fibers/doc_0135.jpg', './data/datasets/fibers/doc_0136.jpg', './data/datasets/fibers/doc_0137.jpg', './data/datasets/fibers/doc_0138.jpg', './data/datasets/fibers/doc_0139.jpg', './data/datasets/fibers/doc_014.jpg', './data/datasets/fibers/doc_0140.jpg', './data/datasets/fibers/doc_0141.jpg', './data/datasets/fibers/doc_0142.jpg', './data/datasets/fibers/doc_0143.jpg', './data/datasets/fibers/doc_0144.jpg', './data/datasets/fibers/doc_0145.jpg', './data/datasets/fibers/doc_0146.jpg', './data/datasets/fibers/doc_0147.jpg', './data/datasets/fibers/doc_0148.jpg', './data/datasets/fibers/doc_0149.jpg', './data/datasets/fibers/doc_015.jpg', './data/datasets/fibers/doc_0150.jpg', './data/datasets/fibers/doc_0151.jpg', './data/datasets/fibers/doc_0152.jpg', './data/datasets/fibers/doc_0153.jpg', './data/datasets/fibers/doc_0154.jpg', './data/datasets/fibers/doc_0155.jpg', './data/datasets/fibers/doc_0156.jpg', './data/datasets/fibers/doc_0157.jpg', './data/datasets/fibers/doc_0158.jpg', './data/datasets/fibers/doc_0159.jpg', './data/datasets/fibers/doc_016.jpg', './data/datasets/fibers/doc_0160.jpg', './data/datasets/fibers/doc_0161.jpg', './data/datasets/fibers/doc_0162.jpg', './data/datasets/fibers/doc_0163.jpg', './data/datasets/fibers/doc_0164.jpg', './data/datasets/fibers/doc_0165.jpg', './data/datasets/fibers/doc_0166.jpg', './data/datasets/fibers/doc_0167.jpg', './data/datasets/fibers/doc_0168.jpg', './data/datasets/fibers/doc_0169.jpg', './data/datasets/fibers/doc_017.jpg', './data/datasets/fibers/doc_0170.jpg', './data/datasets/fibers/doc_0171.jpg', './data/datasets/fibers/doc_0172.jpg', './data/datasets/fibers/doc_0173.jpg', './data/datasets/fibers/doc_0174.jpg', './data/datasets/fibers/doc_0175.jpg', './data/datasets/fibers/doc_0176.jpg', './data/datasets/fibers/doc_0177.jpg', './data/datasets/fibers/doc_0178.jpg', './data/datasets/fibers/doc_0179.jpg', './data/datasets/fibers/doc_018.jpg', './data/datasets/fibers/doc_0180.jpg', './data/datasets/fibers/doc_0181.jpg', './data/datasets/fibers/doc_0182.jpg', './data/datasets/fibers/doc_0183.jpg', './data/datasets/fibers/doc_0184.jpg', './data/datasets/fibers/doc_0185.jpg', './data/datasets/fibers/doc_0186.jpg', './data/datasets/fibers/doc_0187.jpg', './data/datasets/fibers/doc_0188.jpg', './data/datasets/fibers/doc_0189.jpg', './data/datasets/fibers/doc_019.jpg', './data/datasets/fibers/doc_0190.jpg', './data/datasets/fibers/doc_0191.jpg', './data/datasets/fibers/doc_0192.jpg', './data/datasets/fibers/doc_0193.jpg', './data/datasets/fibers/doc_0194.jpg', './data/datasets/fibers/doc_0195.jpg', './data/datasets/fibers/doc_0196.jpg', './data/datasets/fibers/doc_0198.jpg', './data/datasets/fibers/doc_0199.jpg', './data/datasets/fibers/doc_020.jpg', './data/datasets/fibers/doc_0200.jpg', './data/datasets/fibers/doc_0201.jpg', './data/datasets/fibers/doc_0202.jpg', './data/datasets/fibers/doc_0203.jpg', './data/datasets/fibers/doc_0204.jpg', './data/datasets/fibers/doc_0205.jpg', './data/datasets/fibers/doc_0206.jpg', './data/datasets/fibers/doc_0207.jpg', './data/datasets/fibers/doc_0208.jpg', './data/datasets/fibers/doc_0209.jpg', './data/datasets/fibers/doc_021.jpg', './data/datasets/fibers/doc_0210.jpg', './data/datasets/fibers/doc_0211.jpg', './data/datasets/fibers/doc_0212.jpg', './data/datasets/fibers/doc_0213.jpg', './data/datasets/fibers/doc_0214.jpg', './data/datasets/fibers/doc_0215.jpg', './data/datasets/fibers/doc_0216.jpg', './data/datasets/fibers/doc_0217.jpg', './data/datasets/fibers/doc_0218.jpg', './data/datasets/fibers/doc_0219.jpg', './data/datasets/fibers/doc_022.jpg', './data/datasets/fibers/doc_0220.jpg', './data/datasets/fibers/doc_0221.jpg', './data/datasets/fibers/doc_0222.jpg', './data/datasets/fibers/doc_0223.jpg', './data/datasets/fibers/doc_0224.jpg', './data/datasets/fibers/doc_0225.jpg', './data/datasets/fibers/doc_0226.jpg', './data/datasets/fibers/doc_0227.jpg', './data/datasets/fibers/doc_0228.jpg', './data/datasets/fibers/doc_0229.jpg', './data/datasets/fibers/doc_023.jpg', './data/datasets/fibers/doc_0230.jpg', './data/datasets/fibers/doc_0231.jpg', './data/datasets/fibers/doc_0232.jpg', './data/datasets/fibers/doc_0233.jpg', './data/datasets/fibers/doc_0234.jpg', './data/datasets/fibers/doc_0235.jpg', './data/datasets/fibers/doc_0236.jpg', './data/datasets/fibers/doc_0237.jpg', './data/datasets/fibers/doc_0239.jpg', './data/datasets/fibers/doc_024.jpg', './data/datasets/fibers/doc_0240.jpg', './data/datasets/fibers/doc_0241.jpg', './data/datasets/fibers/doc_0243.jpg', './data/datasets/fibers/doc_0244.jpg', './data/datasets/fibers/doc_0245.jpg', './data/datasets/fibers/doc_0246.jpg', './data/datasets/fibers/doc_0247.jpg', './data/datasets/fibers/doc_0248.jpg', './data/datasets/fibers/doc_0249.jpg', './data/datasets/fibers/doc_025.jpg', './data/datasets/fibers/doc_0250.jpg', './data/datasets/fibers/doc_0251.jpg', './data/datasets/fibers/doc_0252.jpg', './data/datasets/fibers/doc_0253.jpg', './data/datasets/fibers/doc_0254.jpg', './data/datasets/fibers/doc_0255.jpg', './data/datasets/fibers/doc_0256.jpg', './data/datasets/fibers/doc_0257.jpg', './data/datasets/fibers/doc_0258.jpg', './data/datasets/fibers/doc_0259.jpg', './data/datasets/fibers/doc_026.jpg', './data/datasets/fibers/doc_0260.jpg', './data/datasets/fibers/doc_0261.jpg', './data/datasets/fibers/doc_0262.jpg', './data/datasets/fibers/doc_0263.jpg', './data/datasets/fibers/doc_0264.jpg', './data/datasets/fibers/doc_0265.jpg', './data/datasets/fibers/doc_0266.jpg', './data/datasets/fibers/doc_0267.jpg', './data/datasets/fibers/doc_0268.jpg', './data/datasets/fibers/doc_0269.jpg', './data/datasets/fibers/doc_027.jpg', './data/datasets/fibers/doc_0270.jpg', './data/datasets/fibers/doc_0271.jpg', './data/datasets/fibers/doc_0272.jpg', './data/datasets/fibers/doc_0273.jpg', './data/datasets/fibers/doc_0274.jpg', './data/datasets/fibers/doc_0275.jpg', './data/datasets/fibers/doc_0276.jpg', './data/datasets/fibers/doc_0278.jpg', './data/datasets/fibers/doc_0279.jpg', './data/datasets/fibers/doc_028.jpg', './data/datasets/fibers/doc_0280.jpg', './data/datasets/fibers/doc_0281.jpg', './data/datasets/fibers/doc_0282.jpg', './data/datasets/fibers/doc_0283.jpg', './data/datasets/fibers/doc_0284.jpg', './data/datasets/fibers/doc_0285.jpg', './data/datasets/fibers/doc_0286.jpg', './data/datasets/fibers/doc_0287.jpg', './data/datasets/fibers/doc_0288.jpg', './data/datasets/fibers/doc_0289.jpg', './data/datasets/fibers/doc_029.jpg', './data/datasets/fibers/doc_0291.jpg', './data/datasets/fibers/doc_0292.jpg', './data/datasets/fibers/doc_0293.jpg', './data/datasets/fibers/doc_0294.jpg', './data/datasets/fibers/doc_0296.jpg', './data/datasets/fibers/doc_0297.jpg', './data/datasets/fibers/doc_0298.jpg', './data/datasets/fibers/doc_0299.jpg', './data/datasets/fibers/doc_030.jpg', './data/datasets/fibers/doc_0300.jpg', './data/datasets/fibers/doc_0301.jpg', './data/datasets/fibers/doc_0302.jpg', './data/datasets/fibers/doc_0303.jpg', './data/datasets/fibers/doc_0304.jpg', './data/datasets/fibers/doc_0305.jpg', './data/datasets/fibers/doc_0306.jpg', './data/datasets/fibers/doc_0307.jpg', './data/datasets/fibers/doc_0308.jpg', './data/datasets/fibers/doc_0309.jpg', './data/datasets/fibers/doc_031.jpg', './data/datasets/fibers/doc_0310.jpg', './data/datasets/fibers/doc_0311.jpg', './data/datasets/fibers/doc_0312.jpg', './data/datasets/fibers/doc_0313.jpg', './data/datasets/fibers/doc_0314.jpg', './data/datasets/fibers/doc_0315.jpg', './data/datasets/fibers/doc_0316.jpg', './data/datasets/fibers/doc_0317.jpg', './data/datasets/fibers/doc_0318.jpg', './data/datasets/fibers/doc_0319.jpg', './data/datasets/fibers/doc_032.jpg', './data/datasets/fibers/doc_0320.jpg']\n",
      "['./data/datasets/fibers/doc_0321.jpg', './data/datasets/fibers/doc_0322.jpg', './data/datasets/fibers/doc_0323.jpg', './data/datasets/fibers/doc_0324.jpg', './data/datasets/fibers/doc_0325.jpg', './data/datasets/fibers/doc_0326.jpg', './data/datasets/fibers/doc_0327.jpg', './data/datasets/fibers/doc_0328.jpg', './data/datasets/fibers/doc_0329.jpg', './data/datasets/fibers/doc_033.jpg', './data/datasets/fibers/doc_0330.jpg', './data/datasets/fibers/doc_0331.jpg', './data/datasets/fibers/doc_0332.jpg', './data/datasets/fibers/doc_0333.jpg', './data/datasets/fibers/doc_0334.jpg', './data/datasets/fibers/doc_0335.jpg', './data/datasets/fibers/doc_0336.jpg', './data/datasets/fibers/doc_0337.jpg', './data/datasets/fibers/doc_0338.jpg', './data/datasets/fibers/doc_0339.jpg', './data/datasets/fibers/doc_034.jpg', './data/datasets/fibers/doc_0340.jpg', './data/datasets/fibers/doc_0341.jpg', './data/datasets/fibers/doc_0342.jpg', './data/datasets/fibers/doc_0343.jpg', './data/datasets/fibers/doc_0344.jpg', './data/datasets/fibers/doc_0345.jpg', './data/datasets/fibers/doc_0346.jpg', './data/datasets/fibers/doc_0347.jpg', './data/datasets/fibers/doc_0348.jpg', './data/datasets/fibers/doc_0349.jpg', './data/datasets/fibers/doc_035.jpg', './data/datasets/fibers/doc_0350.jpg', './data/datasets/fibers/doc_0351.jpg', './data/datasets/fibers/doc_0352.jpg', './data/datasets/fibers/doc_0353.jpg', './data/datasets/fibers/doc_0355.jpg', './data/datasets/fibers/doc_0357.jpg', './data/datasets/fibers/doc_0358.jpg', './data/datasets/fibers/doc_0359.jpg', './data/datasets/fibers/doc_036.jpg', './data/datasets/fibers/doc_0360.jpg', './data/datasets/fibers/doc_0361.jpg', './data/datasets/fibers/doc_0362.jpg', './data/datasets/fibers/doc_0363.jpg', './data/datasets/fibers/doc_0364.jpg', './data/datasets/fibers/doc_0365.jpg', './data/datasets/fibers/doc_0366.jpg', './data/datasets/fibers/doc_0367.jpg', './data/datasets/fibers/doc_0368.jpg', './data/datasets/fibers/doc_0369.jpg', './data/datasets/fibers/doc_037.jpg', './data/datasets/fibers/doc_0370.jpg', './data/datasets/fibers/doc_0371.jpg', './data/datasets/fibers/doc_0372.jpg', './data/datasets/fibers/doc_0374.jpg', './data/datasets/fibers/doc_0375.jpg', './data/datasets/fibers/doc_0376.jpg', './data/datasets/fibers/doc_0377.jpg', './data/datasets/fibers/doc_0378.jpg', './data/datasets/fibers/doc_0379.jpg', './data/datasets/fibers/doc_038.jpg', './data/datasets/fibers/doc_0380.jpg', './data/datasets/fibers/doc_0381.jpg', './data/datasets/fibers/doc_0382.jpg', './data/datasets/fibers/doc_0383.jpg', './data/datasets/fibers/doc_0384.jpg', './data/datasets/fibers/doc_0385.jpg', './data/datasets/fibers/doc_0386.jpg', './data/datasets/fibers/doc_0387.jpg', './data/datasets/fibers/doc_0388.jpg', './data/datasets/fibers/doc_0389.jpg', './data/datasets/fibers/doc_039.jpg', './data/datasets/fibers/doc_0391.jpg', './data/datasets/fibers/doc_0392.jpg', './data/datasets/fibers/doc_0393.jpg', './data/datasets/fibers/doc_0394.jpg', './data/datasets/fibers/doc_0396.jpg', './data/datasets/fibers/doc_0397.jpg', './data/datasets/fibers/doc_0398.jpg', './data/datasets/fibers/doc_0399.jpg', './data/datasets/fibers/doc_040.jpg', './data/datasets/fibers/doc_0400.jpg', './data/datasets/fibers/doc_0401.jpg', './data/datasets/fibers/doc_0402.jpg', './data/datasets/fibers/doc_0403.jpg', './data/datasets/fibers/doc_0404.jpg', './data/datasets/fibers/doc_0405.jpg', './data/datasets/fibers/doc_0406.jpg', './data/datasets/fibers/doc_0407.jpg', './data/datasets/fibers/doc_0408.jpg', './data/datasets/fibers/doc_0409.jpg', './data/datasets/fibers/doc_041.jpg', './data/datasets/fibers/doc_0410.jpg', './data/datasets/fibers/doc_0411.jpg', './data/datasets/fibers/doc_0412.jpg', './data/datasets/fibers/doc_0413.jpg', './data/datasets/fibers/doc_0414.jpg', './data/datasets/fibers/doc_0415.jpg', './data/datasets/fibers/doc_0416.jpg', './data/datasets/fibers/doc_0417.jpg', './data/datasets/fibers/doc_0418.jpg', './data/datasets/fibers/doc_0419.jpg', './data/datasets/fibers/doc_042.jpg', './data/datasets/fibers/doc_0420.jpg', './data/datasets/fibers/doc_0421.jpg', './data/datasets/fibers/doc_0422.jpg', './data/datasets/fibers/doc_0423.jpg', './data/datasets/fibers/doc_0424.jpg', './data/datasets/fibers/doc_0425.jpg', './data/datasets/fibers/doc_0426.jpg', './data/datasets/fibers/doc_0427.jpg', './data/datasets/fibers/doc_0428.jpg', './data/datasets/fibers/doc_0429.jpg', './data/datasets/fibers/doc_043.jpg', './data/datasets/fibers/doc_0430.jpg', './data/datasets/fibers/doc_0431.jpg', './data/datasets/fibers/doc_0432.jpg', './data/datasets/fibers/doc_0433.jpg', './data/datasets/fibers/doc_0434.jpg', './data/datasets/fibers/doc_0435.jpg', './data/datasets/fibers/doc_0436.jpg', './data/datasets/fibers/doc_0437.jpg', './data/datasets/fibers/doc_0438.jpg', './data/datasets/fibers/doc_0439.jpg', './data/datasets/fibers/doc_044.jpg', './data/datasets/fibers/doc_0440.jpg', './data/datasets/fibers/doc_0441.jpg', './data/datasets/fibers/doc_0442.jpg', './data/datasets/fibers/doc_0443.jpg', './data/datasets/fibers/doc_0444.jpg', './data/datasets/fibers/doc_0445.jpg', './data/datasets/fibers/doc_0446.jpg', './data/datasets/fibers/doc_0447.jpg', './data/datasets/fibers/doc_0448.jpg', './data/datasets/fibers/doc_0449.jpg', './data/datasets/fibers/doc_045.jpg', './data/datasets/fibers/doc_0450.jpg', './data/datasets/fibers/doc_046.jpg', './data/datasets/fibers/doc_047.jpg', './data/datasets/fibers/doc_048.jpg', './data/datasets/fibers/doc_049.jpg', './data/datasets/fibers/doc_050.jpg', './data/datasets/fibers/doc_051.jpg', './data/datasets/fibers/doc_052.jpg', './data/datasets/fibers/doc_053.jpg', './data/datasets/fibers/doc_054.jpg', './data/datasets/fibers/doc_055.jpg', './data/datasets/fibers/doc_056.jpg', './data/datasets/fibers/doc_057.jpg', './data/datasets/fibers/doc_058.jpg', './data/datasets/fibers/doc_059.jpg', './data/datasets/fibers/doc_060.jpg', './data/datasets/fibers/doc_061.jpg', './data/datasets/fibers/doc_062.jpg', './data/datasets/fibers/doc_063.jpg', './data/datasets/fibers/doc_064.jpg', './data/datasets/fibers/doc_065.jpg', './data/datasets/fibers/doc_066.jpg', './data/datasets/fibers/doc_067.jpg', './data/datasets/fibers/doc_068.jpg', './data/datasets/fibers/doc_069.jpg', './data/datasets/fibers/doc_070.jpg', './data/datasets/fibers/doc_071.jpg', './data/datasets/fibers/doc_072.jpg', './data/datasets/fibers/doc_073.jpg', './data/datasets/fibers/doc_074.jpg', './data/datasets/fibers/doc_075.jpg', './data/datasets/fibers/doc_076.jpg', './data/datasets/fibers/doc_077.jpg', './data/datasets/fibers/doc_078.jpg', './data/datasets/fibers/doc_079.jpg', './data/datasets/fibers/doc_080.jpg', './data/datasets/fibers/doc_081.jpg', './data/datasets/fibers/doc_082.jpg', './data/datasets/fibers/doc_083.jpg', './data/datasets/fibers/doc_084.jpg', './data/datasets/fibers/doc_085.jpg', './data/datasets/fibers/doc_086.jpg', './data/datasets/fibers/doc_087.jpg', './data/datasets/fibers/doc_088.jpg', './data/datasets/fibers/doc_089.jpg', './data/datasets/fibers/doc_090.jpg', './data/datasets/fibers/doc_091.jpg', './data/datasets/fibers/doc_092.jpg', './data/datasets/fibers/doc_093.jpg', './data/datasets/fibers/doc_094.jpg', './data/datasets/fibers/doc_095.jpg', './data/datasets/fibers/doc_096.jpg', './data/datasets/fibers/doc_097.jpg', './data/datasets/fibers/doc_098.jpg', './data/datasets/fibers/doc_099.jpg', './data/datasets/fibers/doc_100.jpg', './data/datasets/fibers/doc_101.jpg', './data/datasets/fibers/doc_102.jpg', './data/datasets/fibers/doc_103.jpg', './data/datasets/fibers/doc_104.jpg', './data/datasets/fibers/doc_105.jpg', './data/datasets/fibers/doc_106.jpg', './data/datasets/fibers/doc_107.jpg', './data/datasets/fibers/doc_108.jpg', './data/datasets/fibers/doc_109.jpg', './data/datasets/fibers/doc_110.jpg', './data/datasets/fibers/doc_111.jpg', './data/datasets/fibers/doc_112.jpg', './data/datasets/fibers/doc_113.jpg', './data/datasets/fibers/doc_114.jpg', './data/datasets/fibers/doc_115.jpg', './data/datasets/fibers/doc_116.jpg', './data/datasets/fibers/doc_117.jpg', './data/datasets/fibers/doc_118.jpg', './data/datasets/fibers/doc_119.jpg', './data/datasets/fibers/doc_120.jpg', './data/datasets/fibers/doc_121.jpg', './data/datasets/fibers/doc_122.jpg', './data/datasets/fibers/doc_123.jpg', './data/datasets/fibers/doc_124.jpg', './data/datasets/fibers/doc_125.jpg', './data/datasets/fibers/doc_126.jpg', './data/datasets/fibers/doc_127.jpg']\n",
      "220\n",
      "torch.Size([44, 1, 120, 120])\n",
      "Namespace(exported=False, num_channels=1, pyramid_levels=3, scale_pyramid=1.3, dim_first=3, dim_second=5, dim_third=8, group_size=36, epochs=70, border_size=12, box_size=21, nms_size=5, img_size=120, batch_size=16, path_data='./data', path_model='model.pt', is_loss_ssim=True, margin_loss=2.0, outlier_rejection=False, show_feature=False)\n",
      "Model loaded from ./data/models/sp_map_fo_30.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import kornia\n",
    "import best.singular_point as sp\n",
    "from teste_util import *\n",
    "\n",
    "# Fixar a semente do Torch para operações específicas\n",
    "fixed_seed()\n",
    "\n",
    "# leitura dos dados\n",
    "# trainloader,testloader =read_dataload_flower(sp.args.img_size,'./data/datasets',batch_size=60)\n",
    "trainloader,testloader =read_dataload_fibers(sp.args.img_size)\n",
    "# trainloader,testloader=read_dataload_woods(sp.args.img_size)\n",
    "iterator=iter(testloader)\n",
    "img,labels = next(iterator)\n",
    "print(img.shape)\n",
    "\n",
    "# Carregar o modelo singular points\n",
    "# path_siamese = './data/models/sp2_mp_25.pth'\n",
    "# path_siamese = './data/models/sp2_85.pth'\n",
    "path_siamese = './data/models/sp_map_fo_30.pth'\n",
    "\n",
    "sp.args.num_channels = 1\n",
    "model = sp.SingularPoints(args=sp.args).to(sp.device)\n",
    "load_model(model,path_siamese,sp.device)\n",
    "\n",
    "#gerar variacao de transformacoes pespectivas e fotometrica\n",
    "params_lists =AugmentationParamsGenerator(6,img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from kornia.feature.scale_space_detector import get_default_detector_config, MultiResolutionDetector\n",
    "from kornia.feature.keynet import KeyNetDetector\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "keynet_default_config = {\n",
    "    'num_filters': 8,\n",
    "    'num_levels': 3,\n",
    "    'kernel_size': 5,\n",
    "    # 'Detector_conf': {'nms_size': 15, 'pyramid_levels': 1, 'up_levels': 1, 'scale_factor_levels': 1.3, 's_mult': 22.0},\n",
    "    'Detector_conf': {'nms_size': 5, 'pyramid_levels': 0, 'up_levels': 0, 'scale_factor_levels': 1.0, 's_mult': 5.0},\n",
    "   #  'Detector_conf': {'nms_size': 5, 'pyramid_levels': 3, 'up_levels': 2, 'scale_factor_levels': 1.3, 's_mult': 12.0},# flowers 420/1020  0.411\n",
    "   #  'Detector_conf': {'nms_size': 7, 'pyramid_levels': 2, 'up_levels': 1, 'scale_factor_levels': 1.3, 's_mult': 14.0}#fibers\n",
    "   #  'Detector_conf': {'nms_size': 7, 'pyramid_levels': 2, 'up_levels': 1, 'scale_factor_levels': 1.3, 's_mult': 14.0}#woods 253/496 - 0.510\n",
    "   # 'Detector_conf': {'nms_size': 9, 'pyramid_levels': 2, 'up_levels': 1, 'scale_factor_levels': 1.5, 's_mult': 14.0},#woods  hard 262/496 - 0.528\n",
    "   # 'Detector_conf': {'nms_size': 7, 'pyramid_levels': 2, 'up_levels': 1, 'scale_factor_levels': 1.3, 's_mult': 14.0},#woods  sift 245/496 - 0.494\n",
    "}\n",
    "kornia.feature.SIFTDescriptor(32, rootsift=True)\n",
    "detector1 = KeyNetDetector(num_features=60, keynet_conf=keynet_default_config,ori_module=kornia.feature.LAFOrienter(32)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detector(nn.Module):\n",
    "        def __init__(self,model) -> None:\n",
    "           super().__init__()\n",
    "           self.model = model\n",
    "        def forward(self, x):\n",
    "            features_key,features_key_summary,features_ori,features_ori_summary,max_coords_values, max_map= self.model(x)\n",
    "            return features_key_summary\n",
    "detec = Detector(model)\n",
    "detector1 = CustomNetDetector(detec,PS=32,keynet_conf=keynet_default_config,num_features=60).to(sp.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_222587/2274395680.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  detec.load_state_dict(torch.load(args.load_dir,map_location=device))  ## Load the PyTorch learnable model parameters.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "\n",
    "## for fix seed\n",
    "import random, torch, numpy \n",
    "def get_config(jupyter=False):\n",
    "    parser = argparse.ArgumentParser(description='Train REKD Architecture')\n",
    "\n",
    "    ## basic configuration\n",
    "    parser.add_argument('--data_dir', type=str, default='../ImageNet2012/ILSVRC2012_img_val', #default='path-to-ImageNet',\n",
    "                            help='The root path to the data from which the synthetic dataset will be created.')\n",
    "    parser.add_argument('--synth_dir', type=str, default='', \n",
    "                            help='The path to save the generated sythetic image pairs.')\n",
    "    parser.add_argument('--log_dir', type=str, default='trained_models/weights',\n",
    "                            help='The path to save the REKD weights.')\n",
    "    parser.add_argument('--load_dir', type=str, default='',\n",
    "                        help='Set saved model parameters if resume training is desired.')                            \n",
    "    parser.add_argument('--exp_name', type=str, default='REKD',\n",
    "                            help='The Rotaton-equivaraiant Keypoint Detection (REKD) experiment name')\n",
    "    ## network architecture\n",
    "    parser.add_argument('--factor_scaling_pyramid', type=float, default=1.2,\n",
    "                        help='The scale factor between the multi-scale pyramid levels in the architecture.')\n",
    "    parser.add_argument('--group_size', type=int, default=36,  \n",
    "                        help='The number of groups for the group convolution.')\n",
    "    parser.add_argument('--dim_first', type=int, default=2,\n",
    "                        help='The number of channels of the first layer')\n",
    "    parser.add_argument('--dim_second', type=int, default=2,\n",
    "                        help='The number of channels of the second layer')\n",
    "    parser.add_argument('--dim_third', type=int, default=2,\n",
    "                        help='The number of channels of the thrid layer')                       \n",
    "    ## network training\n",
    "    parser.add_argument('--batch_size', type=int, default=16,\n",
    "                        help='The batch size for training.')\n",
    "    parser.add_argument('--num_epochs', type=int, default=20, \n",
    "                        help='Number of epochs for training.')\n",
    "    ## Loss function  \n",
    "    parser.add_argument('--init_initial_learning_rate', type=float, default=1e-3,\n",
    "                        help='The init initial learning rate value.')\n",
    "    parser.add_argument('--MSIP_sizes', type=str, default=\"8,16,24,32,40\",\n",
    "                        help='MSIP sizes.')\n",
    "    parser.add_argument('--MSIP_factor_loss', type=str, default=\"256.0,64.0,16.0,4.0,1.0\",\n",
    "                        help='MSIP loss balancing parameters.')\n",
    "    parser.add_argument('--ori_loss_balance', type=float, default=100., \n",
    "                        help='')\n",
    "    ## Dataset generation\n",
    "    parser.add_argument('--patch_size', type=int, default=192,\n",
    "                        help='The patch size of the generated dataset.')\n",
    "    parser.add_argument('--max_angle', type=int, default=180,\n",
    "                        help='The max angle value for generating a synthetic view to train REKD.')\n",
    "    parser.add_argument('--min_scale', type=float, default=1.0,\n",
    "                        help='The min scale value for generating a synthetic view to train REKD.')\n",
    "    parser.add_argument('--max_scale', type=float, default=1.0,\n",
    "                        help='The max scale value for generating a synthetic view to train REKD.')\n",
    "    parser.add_argument('--max_shearing', type=float, default=0.0,\n",
    "                        help='The max shearing value for generating a synthetic view to train REKD.')\n",
    "    parser.add_argument('--num_training_data', type=int, default=9000,\n",
    "                        help='The number of the generated dataset.')\n",
    "    parser.add_argument('--is_debugging', type=bool, default=False,\n",
    "                        help='Set variable to True if you desire to train network on a smaller dataset.')\n",
    "    ## For eval/inference\n",
    "    parser.add_argument('--num_points', type=int, default=1500,\n",
    "                        help='the number of points at evaluation time.')\n",
    "    parser.add_argument('--pyramid_levels', type=int, default=5,\n",
    "                        help='downsampling pyramid levels.')\n",
    "    parser.add_argument('--upsampled_levels', type=int, default=2,\n",
    "                        help='upsampling image levels.')\n",
    "    parser.add_argument('--nms_size', type=int, default=15,\n",
    "                        help='The NMS size for computing the validation repeatability.')\n",
    "    parser.add_argument('--border_size', type=int, default=15,\n",
    "                        help='The number of pixels to remove from the borders to compute the repeatability.')\n",
    "    ## For HPatches evaluation\n",
    "    parser.add_argument('--hpatches_path', type=str, default='./datasets/hpatches-sequences-release',\n",
    "                        help='dataset ')\n",
    "    parser.add_argument('--eval_split', type=str, default='debug',\n",
    "                        help='debug, view, illum, full, debug_view, debug_illum ...')      \n",
    "    parser.add_argument('--descriptor', type=str, default=\"hardnet\",\n",
    "                        help='hardnet, sosnet, hynet')    \n",
    "\n",
    "    args = parser.parse_args() if not jupyter else parser.parse_args(args=[])   \n",
    "\n",
    "    fixed_seed()\n",
    "\n",
    "    if args.synth_dir == \"\":\n",
    "        args.synth_dir = 'datasets/synth_data'\n",
    "\n",
    "    args.MSIP_sizes = [int(i) for i in args.MSIP_sizes.split(\",\")]\n",
    "    args.MSIP_factor_loss =[float(i) for i in args.MSIP_factor_loss.split(\",\")]\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "from external.REKD import REKD, count_model_parameters\n",
    "from external.hardnet_pytorch import HardNet\n",
    "\n",
    "args = get_config(jupyter=True)\n",
    "args.load_dir =  'trained_models/release_group36_f2_s2_t2.log/best_model.pt'\n",
    "\n",
    "from torch import nn\n",
    "class Detector(nn.Module):\n",
    "        def __init__(self) -> None:\n",
    "           super().__init__()\n",
    "           detec = REKD(args, device)\n",
    "           detec = detec.to(device) ## use GPU\n",
    "           detec.load_state_dict(torch.load(args.load_dir,map_location=device))  ## Load the PyTorch learnable model parameters.\n",
    "           detec.eval()\n",
    "           self.model = detec\n",
    "           \n",
    "        def forward(self, x):\n",
    "            features_key,features_ori= self.model(x)\n",
    "            return features_key\n",
    "\n",
    "\n",
    "detec = Detector()\n",
    "detector1 = CustomNetDetector(detec,PS=32,keynet_conf=keynet_default_config,num_features=60).to(sp.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import kornia.feature as KF\n",
    "import matplotlib.pyplot as plt\n",
    "import kornia as K\n",
    "\n",
    "def visualize_LAF(img, LAF, img_idx = 0):\n",
    "    x, y = KF.laf.get_laf_pts_to_draw(LAF, img_idx)\n",
    "    print(x[0][:5],y[0][:5])\n",
    "    plt.figure()\n",
    "    plt.imshow(K.utils.tensor_to_image(img[img_idx]))\n",
    "    plt.plot(x, y, 'r')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_matching_images(img1, LAF1, img2, LAF2, matches):\n",
    "    print(LAF1.shape,LAF2.shape)\n",
    "    x1, y1 = KF.laf.get_laf_pts_to_draw(LAF1, 0)  # Pontos da primeira imagem\n",
    "    x2, y2 = KF.laf.get_laf_pts_to_draw(LAF2, 0)  # Pontos da segunda imagem\n",
    "\n",
    "    # Converte as listas x2 e y2 em arrays NumPy\n",
    "    x2 = np.array(x2)\n",
    "    y2 = np.array(y2)\n",
    "\n",
    "    # Crie uma imagem combinada concatenando as duas imagens lado a lado\n",
    "    combined_image = np.concatenate((K.utils.tensor_to_image(img1), K.utils.tensor_to_image(img2)), axis=1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))  # Cria uma figura com uma subplot\n",
    "\n",
    "    # Plota a imagem combinada\n",
    "    ax.imshow(combined_image)\n",
    "    ax.axis('off')\n",
    "    # Plota os pontos correspondentes nas duas imagens\n",
    "    ax.plot(x1, y1, 'c')  # 'ro' representa pontos vermelhos na primeira imagem\n",
    "    ax.plot(x2 + img1.shape[1], y2, 'y')  # Desloca os pontos azuis na segunda imagem para a direita\n",
    "\n",
    "    points1 =kornia.feature.get_laf_center(LAF1)[0].cpu()\n",
    "    points2 =kornia.feature.get_laf_center(LAF2)[0].cpu()\n",
    "    print(points1.shape,points2.shape)\n",
    "    for match in matches:\n",
    "        x1_match, y1_match = points1[match[0],0], points1[match[0],1]\n",
    "        x2_match, y2_match = points2[match[1],0] + img1.shape[1], points2[match[1],1]\n",
    "       \n",
    "        ax.plot([x1_match, x2_match], [y1_match, y2_match], '-', color='red', lw=1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_matches_keypoints(image1, keypoints1, image2, keypoints2, matches, **kwargs):\n",
    "    print('image1 shape: ',image1.shape,image1.dtype,image2.shape,image2.dtype)\n",
    "    # Concatenar as duas imagens lado a lado\n",
    "    combined_image = np.concatenate((image1, image2), axis=1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.imshow(combined_image)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Desenhar pontos correspondentes e linhas conectando-os\n",
    "    offset = image1.shape[1]\n",
    "\n",
    "    for i, (x, y) in enumerate(keypoints1):\n",
    "        ax.plot(x, y, 'o',markerfacecolor='none', markeredgecolor='r',\n",
    "                markersize=20, markeredgewidth=1)\n",
    "        ax.annotate(str(i), (x, y), color='r',xytext=(10, 10), textcoords='offset points', fontsize=12)\n",
    "\n",
    "    for i, (x, y) in enumerate(keypoints2):\n",
    "        ax.plot(x+offset, y, 'o',markerfacecolor='none', markeredgecolor='r',\n",
    "                markersize=20, markeredgewidth=1)\n",
    "        ax.annotate(str(i), (x+offset, y), color='r',xytext=(10, 10), textcoords='offset points', fontsize=12)\n",
    "\n",
    "    for match in matches:\n",
    "        x1, y1 = keypoints1[match[0],0], keypoints1[match[0],1]\n",
    "        x2, y2 = keypoints2[match[1],0]+offset, keypoints2[match[1],1]\n",
    "        ax.plot([x1, x2], [y1, y2], '-', color='lime', lw=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_image_with_keypoints(image_tensor, keypoints_tensor):\n",
    "    # Converter a imagem tensorial em objeto PIL.Image\n",
    "    image = kornia.utils.tensor_to_image(image_tensor)\n",
    "    # Plotar a imagem e os keypoints\n",
    "    plt.imshow(image)\n",
    "    if keypoints_tensor is not None:\n",
    "        # Extrair as coordenadas x e y dos keypoints\n",
    "        keypoints_x = keypoints_tensor[:,0].flatten().tolist()\n",
    "        keypoints_y = keypoints_tensor[:,1].flatten().tolist()\n",
    "        plt.scatter(keypoints_x, keypoints_y, c='red')\n",
    "    plt.show()\n",
    "    \n",
    "def filtrar_keypoints(lista_de_pontos, tensor_mascara):\n",
    "    # Verificar se as coordenadas estão dentro das dimensões\n",
    "    dimensao_max_x, dimensao_max_y = tensor_mascara.shape[1] - 1, tensor_mascara.shape[0] - 1\n",
    "    pontos_filtrados = [\n",
    "        ponto.tolist()  for ponto in lista_de_pontos \n",
    "        if 0 <= ponto[0] <= dimensao_max_x \n",
    "        and 0 <= ponto[1] <= dimensao_max_y \n",
    "        and tensor_mascara[int(ponto[1]), int(ponto[0])] \n",
    "    ]\n",
    "    return torch.tensor(pontos_filtrados)\n",
    "\n",
    "def find_best_matching_indices_knn(points1, points2, threshold, k=3):\n",
    "    if len(points1) == 0 or len(points2) == 0:\n",
    "        return []\n",
    "    distances = cdist(points1, points2)\n",
    "    best_indices = np.argsort(distances, axis=1)[:, :k]\n",
    "    best_distances = np.take_along_axis(distances, best_indices, axis=1)\n",
    "    matched = []\n",
    "\n",
    "    for i in range(len(points1)):\n",
    "        min_distance = np.min(best_distances[i])        \n",
    "        if min_distance < threshold:\n",
    "            best_index = np.argmin(best_distances[i])\n",
    "            matched.append((i, best_indices[i, best_index]))\n",
    "\n",
    "    return matched\n",
    "\n",
    "\n",
    "def detect_extract_feat_in_batch(batch1,aug_list, detector):\n",
    "    origem_total = []\n",
    "    matches_total = []\n",
    "    with torch.no_grad():\n",
    "        for img1  in batch1:            \n",
    "            lafs1, resps1 = detector(img1[None])\n",
    "\n",
    "            B,C,H,W = img1[None].shape\n",
    "            mask = torch.ones(B,C,H,W).to(img1.device) \n",
    "            \n",
    "            #lafs1 to points1\n",
    "            points1 =kornia.feature.get_laf_center(lafs1)\n",
    "\n",
    "            if( points1.shape[1] == 0):\n",
    "                print('aug_list shape: ',points1.shape) \n",
    "                continue         \n",
    "            img2,mask_t,ponts_t=aug_list(img1,mask,points1)           \n",
    "             \n",
    "            img2 = img2.to(img1.device)\n",
    "            lafs2, resps2 = detector(img2)\n",
    "            points2 =kornia.feature.get_laf_center(lafs2)     \n",
    "            filtered_points1 = filtrar_keypoints(ponts_t[0],mask_t[0,0].bool())            \n",
    "            filtered_points2 = filtrar_keypoints(points2[0],mask_t[0,0].bool())\n",
    "            # print(\"shape p & f\",points1.shape,filtered_points1.shape,filtered_points2.shape)\n",
    "            matches = find_best_matching_indices_knn(filtered_points1.cpu(), filtered_points2.cpu(), threshold=1.0, k=1)  \n",
    "            matches2 = find_best_matching_indices_knn(ponts_t[0].cpu(), points2[0].cpu(), threshold=0.6, k=1)  \n",
    "            \n",
    "            # visualize_matching_images(img1[0], lafs1, img2[0,0], lafs2,matches2)\n",
    "            # print(img1.shape,img1.device,img2[0].shape,img2.device)\n",
    "            # plot_matches_keypoints(img2[0,0].cpu(), filtered_points1, img2[0,0].cpu(), filtered_points2, matches)\n",
    "            if( filtered_points1.shape[0] == 0 or filtered_points2.shape[0] == 0):\n",
    "                print('filtered_points1 shape: ',filtered_points1.shape,'filtered_points2 shape: ',filtered_points2.shape)\n",
    "                continue            \n",
    "            origem_total.append(len(filtered_points1))\n",
    "            matches_total.append(len(matches))            \n",
    "    return (np.mean(matches_total)/np.mean(origem_total))*100\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc4cdeaa5a094858a9dfbc90f5caa49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "match of batch  5.245901639344262\n",
      "match of batch  9.539473684210526\n",
      "match of batch  2.0771513353115725\n",
      "aug_list shape:  torch.Size([1, 0, 2])\n",
      "aug_list shape:  torch.Size([1, 0, 2])\n",
      "aug_list shape:  torch.Size([1, 0, 2])\n",
      "aug_list shape:  torch.Size([1, 0, 2])\n",
      "aug_list shape:  torch.Size([1, 0, 2])\n",
      "aug_list shape:  torch.Size([1, 0, 2])\n",
      "aug_list shape:  torch.Size([1, 0, 2])\n",
      "aug_list shape:  torch.Size([1, 0, 2])\n",
      "match of batch  2.083333333333333\n",
      "aug_list shape:  torch.Size([1, 0, 2])\n",
      "aug_list shape:  torch.Size([1, 0, 2])\n",
      "aug_list shape:  torch.Size([1, 0, 2])\n",
      "aug_list shape:  torch.Size([1, 0, 2])\n",
      "aug_list shape:  torch.Size([1, 0, 2])\n",
      "match of batch  4.545454545454546\n",
      "match of dataset  4.698262907530848\n"
     ]
    }
   ],
   "source": [
    "fixed_seed()\n",
    "params_lists.aug_list.data_keys =[\"input\"]\n",
    "\n",
    "transforms = kornia.augmentation.AugmentationSequential(\n",
    "    kornia.augmentation.RandomAffine(degrees=360, translate=(0.2, 0.2), scale=(0.95, 1.05), shear=10,p=0.8),\n",
    "    kornia.augmentation.RandomPerspective(0.2, p=0.7),\n",
    "    kornia.augmentation.RandomBoxBlur((4,4),p=0.5),\n",
    "    # kornia.augmentation.RandomEqualize(p=0.3),\n",
    "    data_keys=[\"input\", \"mask\",\"keypoints\"],\n",
    "    same_on_batch=True,\n",
    "    # random_apply=10,\n",
    ")\n",
    "\n",
    "acertos = 0\n",
    "total = 0\n",
    "from tqdm.notebook import tqdm\n",
    "pbar =  tqdm(testloader)\n",
    "matches_total = []\n",
    "for imgs_batch,labels_batch in pbar:# itera em todo dataset\n",
    "    imgs_batch = imgs_batch.to(device)\n",
    "    mean = detect_extract_feat_in_batch(imgs_batch,transforms,detector1)\n",
    "    matches_total.append(mean)\n",
    "    print('match of batch ',mean)\n",
    "print('match of dataset ',np.mean(matches_total))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "singular-points",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

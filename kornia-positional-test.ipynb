{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment Documentation: Assessing Positional Congruence in Keypoint Detection\n",
    "\n",
    "This Colab notebook evaluates the **repeatability** of keypoint detection algorithms by measuring **positional congruence** between detected keypoints in original images (`I`) and their transformed counterparts (`τ(I)`). The experiment compares our method's performance against state-of-the-art detectors, including **KeyNet** and **REKD**.\n",
    "\n",
    "### Methodology:\n",
    "The evaluation is restricted to overlapping subregions between the original and transformed datasets to ensure meaningful comparisons. The criterion for positional congruence is defined as:\n",
    "\n",
    "**|Kᵢ(τ(Iᵢ)) - τ(Kᵢ(Iᵢ))| ≤ α**\n",
    "\n",
    "Where:\n",
    "- `Kᵢ(Iᵢ)`: Keypoints detected in the original image.\n",
    "- `Kᵢ(τ(Iᵢ))`: Keypoints detected in the transformed image.\n",
    "- `τ`: Transformation applied to the image.\n",
    "- `α`: Acceptable positional deviation threshold.\n",
    "\n",
    "### Objective:\n",
    "This experiment aims to validate the hypothesis that improving keypoint repeatability enhances feature matching and image identification accuracy. It forms the foundation for comparing detector performance and evaluating their robustness to image transformations.\n",
    "\n",
    "By adopting a rigorous and reproducible framework, this notebook provides an impartial assessment of keypoint detectors under diverse real-world conditions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train True   598\n",
      "train False   7946\n",
      "7920\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import kornia\n",
    "from torch import nn\n",
    "from teste_util import *\n",
    "# from kornia.feature.keynet import KeyNetDetector\n",
    "# from custom_local_feature import REKDSosNet, SingularPointSosNet\n",
    "from LocalFeatureCombinations import KeyNetFeatureSIFT, REKDSosNet, SingularPointSosNet\n",
    "import itertools\n",
    "\n",
    "# Fixar a semente do Torch para operações específicas\n",
    "set_seed(42)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "keynet_default_config = {\n",
    "    'num_filters': 8,\n",
    "    'num_levels': 3,\n",
    "    'kernel_size': 5,\n",
    "    'Detector_conf': {'nms_size': 5, 'pyramid_levels': 0, 'up_levels': 0, 'scale_factor_levels': 1.0, 's_mult': 5.0},\n",
    "}\n",
    "\n",
    "\n",
    "# Inicializar os detectores e adicionar seus nomes de classe\n",
    "detectors = {\n",
    "    # \"KeyNetDetector\": KeyNetFeatureSIFT(config=keynet_default_config,device=device).initialize_detector(num_features=60).to(device),\n",
    "    \"REKDSosNet\": REKDSosNet(config=keynet_default_config,device=device).initialize_detector(num_features=60).to(device),\n",
    "    # \"SingularPointSosNet\": SingularPointSosNet(config=keynet_default_config,device=device).initialize_detector(num_features=60).to(device),\n",
    "}\n",
    "\n",
    "# Leitura dos dados\n",
    "dataloaders = {\n",
    "    # \"flower\": read_dataload_flower(120)[1],\n",
    "    # \"fibers\": read_dataload_fibers(120)[1],\n",
    "    \"woods\": read_dataload_woods(120)[1],\n",
    "}\n",
    "\n",
    "# Gerar combinações entre detectores e dataloaders\n",
    "def generate_combinations(detectors, dataloaders):\n",
    "    # Produto cartesiano entre detectores e dataloaders\n",
    "    return list(itertools.product(dataloaders.items(), detectors.items()))\n",
    "\n",
    "# Gerar combinações\n",
    "combinations = generate_combinations(detectors, dataloaders)\n",
    "\n",
    "# Listar as combinações\n",
    "# for i, ((dataset_name, dataloader), (class_name, detector)) in enumerate(combinations, 1):\n",
    "#     print(f\"Combinação {i}: Detector - {class_name} ({detector.__class__.__name__}), Dataset - {dataset_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cdist\n",
    "import kornia.feature as KF\n",
    "import matplotlib.pyplot as plt\n",
    "import kornia as K\n",
    "\n",
    "def visualize_LAF(img, LAF, img_idx = 0):\n",
    "    x, y = KF.laf.get_laf_pts_to_draw(LAF, img_idx)\n",
    "    print(x[0][:5],y[0][:5])\n",
    "    plt.figure()\n",
    "    plt.imshow(K.utils.tensor_to_image(img[img_idx]))\n",
    "    plt.plot(x, y, 'r')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def visualize_matching_images(img1, LAF1, img2, LAF2, matches):\n",
    "    print(LAF1.shape,LAF2.shape)\n",
    "    x1, y1 = KF.laf.get_laf_pts_to_draw(LAF1, 0)  # Pontos da primeira imagem\n",
    "    x2, y2 = KF.laf.get_laf_pts_to_draw(LAF2, 0)  # Pontos da segunda imagem\n",
    "\n",
    "    # Converte as listas x2 e y2 em arrays NumPy\n",
    "    x2 = np.array(x2)\n",
    "    y2 = np.array(y2)\n",
    "\n",
    "    # Crie uma imagem combinada concatenando as duas imagens lado a lado\n",
    "    combined_image = np.concatenate((K.utils.tensor_to_image(img1), K.utils.tensor_to_image(img2)), axis=1)\n",
    "\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 6))  # Cria uma figura com uma subplot\n",
    "\n",
    "    # Plota a imagem combinada\n",
    "    ax.imshow(combined_image)\n",
    "    ax.axis('off')\n",
    "    # Plota os pontos correspondentes nas duas imagens\n",
    "    ax.plot(x1, y1, 'c')  # 'ro' representa pontos vermelhos na primeira imagem\n",
    "    ax.plot(x2 + img1.shape[1], y2, 'y')  # Desloca os pontos azuis na segunda imagem para a direita\n",
    "\n",
    "    points1 =kornia.feature.get_laf_center(LAF1)[0].cpu()\n",
    "    points2 =kornia.feature.get_laf_center(LAF2)[0].cpu()\n",
    "    print(points1.shape,points2.shape)\n",
    "    for match in matches:\n",
    "        x1_match, y1_match = points1[match[0],0], points1[match[0],1]\n",
    "        x2_match, y2_match = points2[match[1],0] + img1.shape[1], points2[match[1],1]\n",
    "       \n",
    "        ax.plot([x1_match, x2_match], [y1_match, y2_match], '-', color='red', lw=1.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def plot_matches_keypoints(image1, keypoints1, image2, keypoints2, matches, **kwargs):\n",
    "    print('image1 shape: ',image1.shape,image1.dtype,image2.shape,image2.dtype)\n",
    "    # Concatenar as duas imagens lado a lado\n",
    "    combined_image = np.concatenate((image1, image2), axis=1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.imshow(combined_image)\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Desenhar pontos correspondentes e linhas conectando-os\n",
    "    offset = image1.shape[1]\n",
    "\n",
    "    for i, (x, y) in enumerate(keypoints1):\n",
    "        ax.plot(x, y, 'o',markerfacecolor='none', markeredgecolor='r',\n",
    "                markersize=20, markeredgewidth=1)\n",
    "        ax.annotate(str(i), (x, y), color='r',xytext=(10, 10), textcoords='offset points', fontsize=12)\n",
    "\n",
    "    for i, (x, y) in enumerate(keypoints2):\n",
    "        ax.plot(x+offset, y, 'o',markerfacecolor='none', markeredgecolor='r',\n",
    "                markersize=20, markeredgewidth=1)\n",
    "        ax.annotate(str(i), (x+offset, y), color='r',xytext=(10, 10), textcoords='offset points', fontsize=12)\n",
    "\n",
    "    for match in matches:\n",
    "        x1, y1 = keypoints1[match[0],0], keypoints1[match[0],1]\n",
    "        x2, y2 = keypoints2[match[1],0]+offset, keypoints2[match[1],1]\n",
    "        ax.plot([x1, x2], [y1, y2], '-', color='lime', lw=0.5)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def plot_image_with_keypoints(image_tensor, keypoints_tensor):\n",
    "    # Converter a imagem tensorial em objeto PIL.Image\n",
    "    image = kornia.utils.tensor_to_image(image_tensor)\n",
    "    # Plotar a imagem e os keypoints\n",
    "    plt.imshow(image)\n",
    "    if keypoints_tensor is not None:\n",
    "        # Extrair as coordenadas x e y dos keypoints\n",
    "        keypoints_x = keypoints_tensor[:,0].flatten().tolist()\n",
    "        keypoints_y = keypoints_tensor[:,1].flatten().tolist()\n",
    "        plt.scatter(keypoints_x, keypoints_y, c='red')\n",
    "    plt.show()\n",
    "    \n",
    "def filtrar_keypoints(lista_de_pontos, tensor_mascara):\n",
    "    # Verificar se as coordenadas estão dentro das dimensões\n",
    "    dimensao_max_x, dimensao_max_y = tensor_mascara.shape[1] - 1, tensor_mascara.shape[0] - 1\n",
    "    pontos_filtrados = [\n",
    "        ponto.tolist()  for ponto in lista_de_pontos \n",
    "        if 0 <= ponto[0] <= dimensao_max_x \n",
    "        and 0 <= ponto[1] <= dimensao_max_y \n",
    "        and tensor_mascara[int(ponto[1]), int(ponto[0])] \n",
    "    ]\n",
    "    return torch.tensor(pontos_filtrados)\n",
    "\n",
    "def find_best_matching_indices_knn(points1, points2, threshold, k=3):\n",
    "    if len(points1) == 0 or len(points2) == 0:\n",
    "        return []\n",
    "    distances = cdist(points1, points2)\n",
    "    best_indices = np.argsort(distances, axis=1)[:, :k]\n",
    "    best_distances = np.take_along_axis(distances, best_indices, axis=1)\n",
    "    matched = []\n",
    "\n",
    "    for i in range(len(points1)):\n",
    "        min_distance = np.min(best_distances[i])        \n",
    "        if min_distance < threshold:\n",
    "            best_index = np.argmin(best_distances[i])\n",
    "            matched.append((i, best_indices[i, best_index]))\n",
    "\n",
    "    return matched\n",
    "\n",
    "\n",
    "def detect_extract_feat_in_batch(batch1,aug_list, detector):\n",
    "    origem_total = []\n",
    "    matches_total = []\n",
    "    with torch.no_grad():\n",
    "        for img1  in batch1:            \n",
    "            lafs1, resps1 = detector(img1[None])\n",
    "\n",
    "            B,C,H,W = img1[None].shape\n",
    "            mask = torch.ones(B,C,H,W).to(img1.device) \n",
    "            \n",
    "            #lafs1 to points1\n",
    "            points1 =kornia.feature.get_laf_center(lafs1)\n",
    "\n",
    "            if( points1.shape[1] == 0):\n",
    "                print('aug_list shape: ',points1.shape) \n",
    "                continue     \n",
    "\n",
    "            params = next(aug_list)    \n",
    "            img2,mask_t,ponts_t=aug_list.augmentation_sequence(img1,mask,points1,params=params)           \n",
    "             \n",
    "            img2 = img2.to(img1.device)\n",
    "            lafs2, resps2 = detector(img2)\n",
    "            points2 =kornia.feature.get_laf_center(lafs2)     \n",
    "            filtered_points1 = filtrar_keypoints(ponts_t[0],mask_t[0,0].bool())            \n",
    "            filtered_points2 = filtrar_keypoints(points2[0],mask_t[0,0].bool())\n",
    "            # print(\"shape p & f\",points1.shape,filtered_points1.shape,filtered_points2.shape)\n",
    "            matches = find_best_matching_indices_knn(filtered_points1.cpu(), filtered_points2.cpu(), threshold=1.0, k=1)  \n",
    "\n",
    "            if( filtered_points1.shape[0] == 0 or filtered_points2.shape[0] == 0):\n",
    "                print('filtered_points1 shape: ',filtered_points1.shape,'filtered_points2 shape: ',filtered_points2.shape)\n",
    "                continue       \n",
    "                 \n",
    "            origem_total.append(len(filtered_points1))\n",
    "            matches_total.append(len(matches))            \n",
    "    return (np.mean(matches_total)/np.mean(origem_total))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AugmentationGenerator:\n",
    "    def __init__(self, n_variations):\n",
    "        # Definir as augmentações\n",
    "        aug_gen = kornia.augmentation.AugmentationSequential(\n",
    "            kornia.augmentation.RandomAffine(degrees=360, translate=(0.2, 0.2), scale=(0.95, 1.05), shear=10,p=0.8),\n",
    "            kornia.augmentation.RandomPerspective(0.2, p=0.7),\n",
    "            kornia.augmentation.RandomBoxBlur((4,4),p=0.5),\n",
    "            data_keys=[kornia.constants.DataKey.INPUT,  # Especifica as chaves corretamente\n",
    "                       kornia.constants.DataKey.MASK,\n",
    "                       kornia.constants.DataKey.KEYPOINTS],\n",
    "            same_on_batch=True,\n",
    "        )\n",
    "\n",
    "        self.augmentation_sequence = aug_gen\n",
    "        self.n_variations = n_variations\n",
    "        self.param_list = []\n",
    "        self.current_index = 0\n",
    "\n",
    "    def generate_variations(self, image, mask, keypoints):\n",
    "        \"\"\"\n",
    "        Gera múltiplas variações de augmentações e coleta seus parâmetros.\n",
    "        \"\"\"\n",
    "        for _ in range(self.n_variations):\n",
    "            # Apenas executa a sequência de augmentação e salva os parâmetros gerados\n",
    "            self.augmentation_sequence(image, mask, keypoints)\n",
    "            self.param_list.append(self.augmentation_sequence._params)\n",
    "\n",
    "    def __iter__(self):\n",
    "        self.current_index = 0  # Resetar o índice a cada nova iteração\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        \"\"\"\n",
    "        Retorna a próxima variação de parâmetros de augmentação.\n",
    "        A iteração será circular.\n",
    "        \"\"\"\n",
    "        result = self.param_list[self.current_index]\n",
    "        self.current_index = (self.current_index + 1) % len(self.param_list)\n",
    "        return result\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Método para resetar o estado do gerador de augmentação.\"\"\"\n",
    "        self.current_index = 0  # Reseta o índice de iteração"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wagner/miniconda3/envs/singular-points/lib/python3.9/site-packages/torch/nn/functional.py:4969: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/wagner/miniconda3/envs/singular-points/lib/python3.9/site-packages/torch/nn/functional.py:4902: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04b9d7c9c0c44efeafb87b1ff043c6af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation REKDSosNet-woods:   0%|          | 0/256 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "filtered_points1 shape:  torch.Size([33, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([48, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([13, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([31, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([14, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([15, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([37, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([32, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([27, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([46, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([18, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([17, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([29, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([18, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([23, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([33, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([23, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([23, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([13, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([16, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([35, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([29, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([18, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([8, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([21, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([33, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([14, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([27, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([39, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([29, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([26, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([5, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([34, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([33, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([17, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([42, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([14, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([20, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([29, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([46, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([41, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([50, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([15, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([17, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([26, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([11, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([17, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([28, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([42, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([14, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([12, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([27, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([18, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([32, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([36, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([16, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([31, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([33, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([30, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([41, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([14, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([21, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([18, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([42, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([15, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([18, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([44, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([34, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([48, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([30, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([21, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([28, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([24, 2]) filtered_points2 shape:  torch.Size([0])\n",
      "filtered_points1 shape:  torch.Size([30, 2]) filtered_points2 shape:  torch.Size([0])\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "aug_gen = AugmentationGenerator(15)\n",
    "image = torch.rand(1, 1, 120, 120)  # Imagem com valores aleatórios\n",
    "mask = torch.ones(1, 1, 120, 120)  # Máscara binária\n",
    "keypoints = torch.tensor([[[30, 30], [90, 90]]], dtype=torch.float32)  # Pontos chave de exemplo\n",
    "\n",
    "# Gerar as variações\n",
    "aug_gen.generate_variations(image, mask, keypoints)\n",
    "\n",
    "for i, ((dataset_name, dataloader), (class_name, detector)) in enumerate(combinations, 1):\n",
    "    matches_total = []\n",
    "    pbar = tqdm(dataloader, desc=f\"Evaluation {class_name}-{dataset_name}\")  # Usando f-string para formatar o nome da classe do detector\n",
    "    for imgs_batch, labels_batch in pbar:  # Itera em todo o dataset\n",
    "        imgs_batch = imgs_batch.to(device)    \n",
    "        mean = detect_extract_feat_in_batch(imgs_batch, aug_gen, detector)\n",
    "        matches_total.append(mean)\n",
    "        pbar.set_postfix({\"Dataset Match Mean\": f\"{np.mean(matches_total):.4f}\"})\n",
    "        aug_gen.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation KeyNetDetector-flower: 100% 102/102 [05:33<00:00,  2.52s/it, Dataset Match Mean=11.0770]\n",
    "Evaluation REKDSosNet-flower: 100% 102/102 [05:05<00:00,  2.32s/it, Dataset Match Mean=30.2933]\n",
    "Evaluation SingularPointSosNet-flower: 100% 102/102 [07:20<00:00,  3.29s/it, Dataset Match Mean=40.1591]\n",
    "\n",
    "Evaluation KeyNetDetector-fibers: 100% 9/9 [00:18<00:00,  2.03s/it, Dataset Match Mean=10.4233]\n",
    "Evaluation REKDSosNet-fibers: 100% 9/9 [00:14<00:00,  1.57s/it, Dataset Match Mean=3.6245]\n",
    "Evaluation SingularPointSosNet-fibers: 100% 9/9 [00:29<00:00,  3.26s/it, Dataset Match Mean=42.1785]\n",
    "\n",
    "Evaluation KeyNetDetector-woods: 100% 256/256 [07:16<00:00,  1.44s/it, Dataset Match Mean=13.1803]\n",
    "Evaluation REKDSosNet-woods: 100% 256/256 [06:11<00:00,  1.17s/it, Dataset Match Mean=21.7026]\n",
    "Evaluation SingularPointSosNet-woods: 100% 256/256 [09:06<00:00,  1.92s/it, Dataset Match Mean=37.6250]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "singular-points",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

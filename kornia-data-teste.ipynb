{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Fixar a semente do Torch para operações específicas\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importa e plota tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First load libraries and images\n",
    "%matplotlib inline\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import kornia\n",
    "import kornia as K\n",
    "import cv2\n",
    "from kornia.feature import *\n",
    "from time import time\n",
    "import torch.optim as optim\n",
    "from torch.nn import Parameter\n",
    "from kornia.color import rgb_to_grayscale\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms, InterpolationMode\n",
    "\n",
    "from skimage import data\n",
    "\n",
    "img_size =120\n",
    "batch_size = 60\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((img_size, img_size), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.Flowers102(root='./data/datasets', split='train',\n",
    "                                        download=True, transform=transform2)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.Flowers102(root='./data/datasets', split='test',\n",
    "                                        download=True, transform=transform2)\n",
    "\n",
    "num_datapoints_to_keep = math.ceil(len(testset) / 2)\n",
    "num_datapoints_to_keep = 1020\n",
    "indices_to_keep = torch.randperm(num_datapoints_to_keep)[:num_datapoints_to_keep]\n",
    "reduced_testset = torch.utils.data.Subset(testset, indices_to_keep)\n",
    "testloader = torch.utils.data.DataLoader(reduced_testset, batch_size=batch_size,\n",
    "                                        shuffle=False, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aumento de dados caminho de ida e volta e reprodutibilidade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia\n",
    "\n",
    "class AugmentationParamsGenerator:\n",
    "    def __init__(self, n, shape):\n",
    "        torch.manual_seed(0)\n",
    "        torch.cuda.manual_seed(0)\n",
    "        \n",
    "        aug_list = kornia.augmentation.AugmentationSequential(\n",
    "            kornia.augmentation.RandomAffine(degrees=360, translate=(0.2, 0.2), scale=(0.95, 1.05), shear=10,p=0.8),\n",
    "            kornia.augmentation.RandomPerspective(0.1, p=0.7),\n",
    "            kornia.augmentation.RandomBoxBlur((5,5),p=0.7),\n",
    "            # kornia.augmentation.RandomEqualize(p=0.3),\n",
    "            data_keys=[\"input\"],\n",
    "            same_on_batch=True,\n",
    "            # random_apply=10,\n",
    "        )\n",
    "\n",
    "        self.index = 0\n",
    "        self.data = []\n",
    "        for i in range(n):\n",
    "            out = aug_list(torch.rand(shape))\n",
    "            self.data.append(aug_list._params)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index >= len(self.data):\n",
    "            self.index = 0  # Reset index to start over for circular iteration\n",
    "            \n",
    "        result = self.data[self.index]\n",
    "        self.index += 1\n",
    "        return result\n",
    "\n",
    "\n",
    "def generate_random_points(image_size, num_points, min_distance=10):\n",
    "    # Extrair as dimensões da imagem\n",
    "    H, W = image_size\n",
    "\n",
    "    # Inicializar a lista de pontos válidos\n",
    "    valid_points = []\n",
    "\n",
    "    while len(valid_points) < num_points:\n",
    "        # Gerar um tensor com coordenadas aleatórias\n",
    "        random_coords = torch.rand(1, 2)\n",
    "        random_coords[:, 0] *= W\n",
    "        random_coords[:, 1] *= H\n",
    "\n",
    "        # Se não houver pontos válidos, adicionar o primeiro ponto gerado\n",
    "        if not valid_points:\n",
    "            valid_points.append(random_coords)\n",
    "        else:\n",
    "            # Verificar a distância entre o ponto gerado e os pontos válidos existentes\n",
    "            distances = F.pairwise_distance(random_coords, torch.cat(valid_points, dim=0))\n",
    "            min_distance_check = distances >= min_distance\n",
    "\n",
    "            # Se o ponto estiver afastado o suficiente dos pontos válidos existentes, adicioná-lo\n",
    "            if min_distance_check.all():\n",
    "                valid_points.append(random_coords)\n",
    "\n",
    "    # Concatenar todos os pontos válidos e retorná-los como um tensor\n",
    "    return torch.cat(valid_points, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_patches_side_by_side(imgs_patches):\n",
    "    num_imgs = imgs_patches.shape[0]  # Número de imagens\n",
    "    fig, axs = plt.subplots(1, num_imgs, figsize=(num_imgs*4, 4))\n",
    "\n",
    "    axs = axs.reshape((1, num_imgs))  # Ajustar a forma para matriz 2D com uma única linha\n",
    "\n",
    "    for i in range(num_imgs):\n",
    "        axs[0, i].imshow(kornia.tensor_to_image(imgs_patches[i]))\n",
    "        axs[0, i].axis('off')\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "def plot_images_with_points_side_by_side(image1, image2, points1=None, points2=None):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # Cria uma figura com duas subplots\n",
    "\n",
    "    # Converte tensores para imagens\n",
    "    image1 = kornia.utils.tensor_to_image(image1)\n",
    "    image2 = kornia.utils.tensor_to_image(image2)\n",
    "\n",
    "    # Plot da primeira imagem na subplot da esquerda\n",
    "    axs[0].imshow(image1)\n",
    "    if points1 is not None:\n",
    "        points1 = points1.cpu().numpy()  # Converte para numpy\n",
    "        keypoints_x = points1[:,0].flatten().tolist()\n",
    "        keypoints_y = points1[:,1].flatten().tolist()\n",
    "        axs[0].scatter(keypoints_x, keypoints_y, c='red', marker='x')  # Plota os pontos em vermelho com marcador 'x'\n",
    "    axs[0].axis('off')  # Remove os eixos\n",
    "\n",
    "    # Plot da segunda imagem na subplot da direita\n",
    "    axs[1].imshow(image2)\n",
    "    if points2 is not None:\n",
    "        points2 = points2.cpu().numpy()  # Converte para numpy\n",
    "        keypoints_x = points2[:,0].flatten().tolist()\n",
    "        keypoints_y = points2[:,1].flatten().tolist()\n",
    "        axs[1].scatter(keypoints_x, keypoints_y, c='red', marker='x')\n",
    "    axs[1].axis('off')  # Remove os eixos\n",
    "\n",
    "    plt.show()  # Mostra o plot com as duas imagens lado a lado\n",
    "\n",
    "def plot_images_with_points_side_by_side(image1, image2, points1=None, points2=None):\n",
    "    fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # Cria uma figura com duas subplots\n",
    "\n",
    "    # Converte tensores para imagens\n",
    "    image1 = kornia.utils.tensor_to_image(image1)\n",
    "    image2 = kornia.utils.tensor_to_image(image2)\n",
    "\n",
    "    # Plot da primeira imagem na subplot da esquerda\n",
    "    axs[0].imshow(image1)\n",
    "    if points1 is not None:\n",
    "        points1 = points1.cpu().numpy()  # Converte para numpy\n",
    "        keypoints_x = points1[:,0].flatten()\n",
    "        keypoints_y = points1[:,1].flatten()\n",
    "        axs[0].scatter(keypoints_x, keypoints_y, c='red', marker='x')  # Plota os pontos em vermelho com marcador 'x'\n",
    "        \n",
    "        # Adiciona os números dos labels aos pontos na subplot da esquerda\n",
    "        for i, (x, y) in enumerate(zip(keypoints_x, keypoints_y)):\n",
    "            axs[0].text(x, y, str(i), color='red')\n",
    "\n",
    "    axs[0].axis('off')  # Remove os eixos\n",
    "\n",
    "    # Plot da segunda imagem na subplot da direita\n",
    "    axs[1].imshow(image2)\n",
    "    if points2 is not None:\n",
    "        points2 = points2.cpu().numpy()  # Converte para numpy\n",
    "        keypoints_x = points2[:,0].flatten()\n",
    "        keypoints_y = points2[:,1].flatten()\n",
    "        axs[1].scatter(keypoints_x, keypoints_y, c='red', marker='x')\n",
    "        \n",
    "        # Adiciona os números dos labels aos pontos na subplot da direita\n",
    "        for i, (x, y) in enumerate(zip(keypoints_x, keypoints_y)):\n",
    "            axs[1].text(x, y, str(i), color='red')\n",
    "\n",
    "    axs[1].axis('off')  # Remove os eixos\n",
    "\n",
    "    plt.show()  # Mostra o plot com as duas imagens lado a lado\n",
    "\n",
    "\n",
    "def filtrar_keypoints(lista_de_pontos, tensor_mascara):\n",
    "    # Verificar se as coordenadas estão dentro das dimensões\n",
    "    dimensao_max_x, dimensao_max_y = tensor_mascara.shape[1] - 1, tensor_mascara.shape[0] - 1\n",
    "    pontos_filtrados = [\n",
    "        1  if 0 <= p1[0] <= dimensao_max_x \n",
    "        and 0 <= p1[1] <= dimensao_max_y \n",
    "        and tensor_mascara[int(p1[1]), int(p1[0])] else 0 for p1 in lista_de_pontos\n",
    "    ]\n",
    "    pontos_filtrados = torch.tensor(pontos_filtrados, dtype=torch.bool)\n",
    "    lista_de_pontos = torch.tensor(lista_de_pontos)\n",
    "    return lista_de_pontos[pontos_filtrados],pontos_filtrados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia\n",
    "\n",
    "\n",
    "iterator=iter(testloader)\n",
    "input,labels = next(iterator)\n",
    "\n",
    "B,C,H,W = input.shape\n",
    "mask = torch.ones(B,C,H,W)\n",
    "border_size =20\n",
    "mask[:, :, :border_size, :] = 0\n",
    "mask[:, :, -border_size:, :] = 0\n",
    "mask[:, :, :, :border_size] = 0\n",
    "mask[:, :, :, -border_size:] = 0\n",
    "\n",
    "bbox = torch.tensor([[\n",
    "    [1., 1.],\n",
    "    [2., 1.],\n",
    "    [2., 2.],\n",
    "    [1., 2.],\n",
    "]]).expand(B, 1, -1, -1)\n",
    "\n",
    "points = generate_random_points((H,W),50).expand(B, -1, -1)\n",
    "shape = input.shape\n",
    "params_lists =AugmentationParamsGenerator(6,shape)\n",
    "next_item = next(params_lists)\n",
    "points = points.to(device)\n",
    "mask = mask.to(device)\n",
    "bbox = bbox.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kornia.feature import laf_from_center_scale_ori\n",
    "from kornia_moons.feature import visualize_LAF\n",
    "\n",
    "def convert_points_to_lafs(points,img1, PS=19):\n",
    "    orient = kornia.feature.LAFOrienter(PS)#\n",
    "    scale_lafs = torch.ones(img1.shape[0],points.shape[1],1,1)*PS\n",
    "    scale_lafs = scale_lafs.to(points.device)\n",
    "    lafs1 = laf_from_center_scale_ori(points,scale_lafs)\n",
    "    lafs2 = orient(lafs1, img1)\n",
    "    return lafs2\n",
    "    \n",
    "def extract_patches_simple(batch, lafs, PS=19):\n",
    "    # visualize_LAF(img1,lafs2,img_idx=0,figsize=(8,6))\n",
    "    imgs_patches = kornia.feature.extract_patches_from_pyramid(batch, lafs, PS)\n",
    "    # plot_patches_side_by_side(imgs_patches[0][0])#plota todas as features do patch 0 imagem 0\n",
    "    return imgs_patches\n",
    "    \n",
    "def extract_patches_from_keypoints(batch,filtered_points, PS=13):\n",
    "    '''\n",
    "    Extrai patches das imagens com base nos pontos de interesse filtrados.\n",
    "\n",
    "    Parâmetros:\n",
    "        batch (torch.Tensor): Tensor contendo as imagens com o formato B, C, H, W. <=out[0]\n",
    "        pontos (torch.Tensor): Tensor contendo os pontos de interesse com o formato B, N, 2. <=out[3][0]\n",
    "        mask (torch.Tensor): Tensor binário indicando os pontos a serem mantidos, com o formato B, N. <=out[1][0, 0]\n",
    "        PS (int): Tamanho do patch a ser extraído.\n",
    "\n",
    "    Retorna:\n",
    "        patchs_mini_img (torch.Tensor): Tensor contendo os patches extraídos com o formato B, N_filtered, C, PS, PS.\n",
    "    '''\n",
    "    CH = 8  # TODO temporário enquanto não aplico a convolução\n",
    "    B, _, H, W = batch.shape\n",
    "    filtered_points = filtered_points.repeat(B, 1, 1)  # B, N(filtered), 2\n",
    "    assert torch.allclose(filtered_points[0], filtered_points[1]), 'pontos devem ser iguais em todas as imagens'\n",
    "    lafs = convert_points_to_lafs(filtered_points, batch)  # B, N(filtered), 2, 3\n",
    "\n",
    "    maps_activations = batch.repeat(1, CH, 1, 1)  # B, CH, H, W o repeat 8 é temporário enquanto não aplico a convolução\n",
    "    # maps_activations = extract_features(model_sp,batch)\n",
    "    print('maps_activations ',maps_activations.shape,maps_activations.device)\n",
    "    # maps_activations[:, 2] = maps_activations[2, 2]\n",
    "    patchs_mini_img = extract_patches_simple(maps_activations, lafs, PS)\n",
    "    return patchs_mini_img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wagner/.local/lib/python3.11/site-packages/e2cnn/nn/modules/r2_conv/basisexpansion_singleblock.py:80: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  full_mask[mask] = norms.to(torch.uint8)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.7678, 4.4151, 3.9970, 3.5622, 4.5677, 4.1504, 3.8267, 3.7374, 4.1247,\n",
       "        3.4617, 4.1508, 3.5717, 3.7994, 4.1814, 4.1044, 3.8746, 3.6095, 4.4004,\n",
       "        3.7782, 4.3038, 3.8337, 4.1391, 4.2422, 3.8081, 3.6655, 3.7796, 4.1400,\n",
       "        4.1180, 3.9309, 3.9568, 4.3090, 4.6009, 3.6586, 4.3789, 3.5193, 4.4851,\n",
       "        3.9907, 4.0311, 4.2564, 3.8601, 3.9827, 3.7911, 3.6861, 3.8840, 4.2896,\n",
       "        3.5813, 3.6717, 4.5272, 3.7939, 4.0212], device='cuda:0',\n",
       "       grad_fn=<LinalgVectorNormBackward0>)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from e2cnn import gspaces\n",
    "from e2cnn import nn as enn    #the equivariant layer we need to build the model\n",
    "from torch import nn\n",
    "class Feature(nn.Module):\n",
    "    def __init__(self,n_channel=2) -> None:\n",
    "        super().__init__()\n",
    "        r2_act = gspaces.Rot2dOnR2(N=18)      \n",
    "\n",
    "        feat_type_in  = enn.FieldType(r2_act,  n_channel*[r2_act.trivial_repr])\n",
    "        feat_type_out = enn.FieldType(r2_act, 2*n_channel*[r2_act.regular_repr])     \n",
    "        self.input_type = feat_type_in\n",
    "\n",
    "        self.block1 = enn.SequentialModule(                \n",
    "                enn.R2Conv(feat_type_in, feat_type_out, kernel_size=3, padding=0, bias=False),\n",
    "                enn.InnerBatchNorm(feat_type_out),\n",
    "                enn.ReLU(feat_type_out, inplace=True)\n",
    "                )\n",
    "        \n",
    "        self.pool1 = enn.PointwiseAvgPoolAntialiased(feat_type_out, sigma=0.66, stride=1, padding=0)\n",
    "\n",
    "        feat_type_in  = self.block1.out_type\n",
    "        feat_type_out = enn.FieldType(r2_act,  4*n_channel*[r2_act.regular_repr])\n",
    "        self.block2 = enn.SequentialModule(                \n",
    "                enn.R2Conv(feat_type_in, feat_type_out, kernel_size=3, padding=0, bias=False),\n",
    "                enn.InnerBatchNorm(feat_type_out),\n",
    "                enn.ReLU(feat_type_out, inplace=True),\n",
    "                )\n",
    "        # self.pool2 = enn.PointwiseAvgPool(feat_type_out, 21)\n",
    "        \n",
    "        feat_type_in  = feat_type_out\n",
    "        feat_type_out = enn.FieldType(r2_act,  8*n_channel*[r2_act.regular_repr])\n",
    "        self.block3 = enn.SequentialModule(                \n",
    "                enn.R2Conv(feat_type_in, feat_type_out, kernel_size=3, padding=0, bias=False),\n",
    "                enn.InnerBatchNorm(feat_type_out),\n",
    "                enn.ReLU(feat_type_out, inplace=True),\n",
    "                )\n",
    "        \n",
    "        feat_type_in  = feat_type_out\n",
    "        feat_type_out = enn.FieldType(r2_act,  16*n_channel*[r2_act.regular_repr])\n",
    "        self.block4 = enn.SequentialModule(                \n",
    "                enn.R2Conv(feat_type_in, feat_type_out, kernel_size=3, padding=0, bias=False),\n",
    "                enn.InnerBatchNorm(feat_type_out),\n",
    "                enn.ReLU(feat_type_out, inplace=True),\n",
    "                enn.GroupPooling(feat_type_out)\n",
    "                )\n",
    "        self.pool = enn.PointwiseAdaptiveAvgPool(self.block4.out_type,1)\n",
    "                \n",
    "    def forward(self,X1)->torch.Tensor:\n",
    "        x = enn.GeometricTensor(X1, self.input_type)\n",
    "        n_dim = X1.shape[-1]\n",
    "        mask = enn.MaskModule(self.input_type, n_dim, margin=2).to(X1.device)\n",
    "        x = mask(x)     \n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.pool(x)\n",
    "        return x.tensor\n",
    "    \n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, n_classes=10) -> None:\n",
    "        super().__init__()\n",
    "        #criar camadas densa a partir de x que é uma cnn\n",
    "        self.dense1 = nn.Linear(2*1*128, 256)\n",
    "        self.dense2 = nn.Linear(256, 128)\n",
    "        self.droupout = nn.Dropout(0.2)\n",
    "        self.activation = nn.LeakyReLU()\n",
    "        #self.activation = nn.ELU()\n",
    "        #função de ativação ideal para retornar um valor entre 0 e 1\n",
    "        self.activation2 = nn.Tanh()\n",
    "\n",
    "    \n",
    "    def forward(self,X1,X2)->torch.Tensor:\n",
    "        flatten_x1 = X1.view(X1.size(0), -1)\n",
    "        flatten_x2 = X2.view(X2.size(0), -1)\n",
    "        x = torch.cat((flatten_x1,flatten_x2),dim=1)\n",
    "        x = self.droupout(self.dense1(x))\n",
    "        x = self.activation(x)\n",
    "        x = self.droupout(self.dense2(x))\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        # Calculando a distância euclidiana\n",
    "        x = torch.norm(x, dim=1)\n",
    "        # x = self.activation(distance) #retorna um valor entre 0 e 1\n",
    "        return x\n",
    "    \n",
    "class Siamesa(nn.Module):\n",
    "    def __init__(self,n_channel=2) -> None:\n",
    "            super().__init__()\n",
    "            self.feature = Feature(n_channel=n_channel)\n",
    "            self.discriminator = Discriminator()\n",
    "    \n",
    "    def forward(self,X1,X2)->torch.Tensor:\n",
    "        x1 = self.feature(X1)\n",
    "        x2 = self.feature(X2)\n",
    "        x = self.discriminator(x1,x2)\n",
    "        return x\n",
    "\n",
    "\n",
    "n_channel =8\n",
    "PS =21\n",
    "model =Siamesa(n_channel=n_channel).to(device)\n",
    "X1=torch.rand(50,n_channel,PS,PS).to(device)\n",
    "X2=torch.rand(50,n_channel,PS,PS).to(device)\n",
    "dist = model(X1,X1)\n",
    "dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import pairwise_distance\n",
    "from tqdm import tqdm\n",
    "#Create methods to calculate loss\n",
    "def loss_fn(output_pos,output_neg):  \n",
    "      \n",
    "    margim = 0.8\n",
    "    zero = torch.tensor(0.,requires_grad=True).to(output_pos.device)\n",
    "    loss = output_pos - output_neg + margim\n",
    "    loss = torch.max(torch.tensor(0),loss)\n",
    "    loss = torch.sum(loss)  # Reduzir para um escalar\n",
    "    return loss\n",
    "\n",
    "#Create methods to train the model\n",
    "def train_one_epoch(model, data_loader, optimizer, loss_fn, device='cpu', is_training=True):\n",
    "    model.train(is_training)\n",
    "    total_loss = 0.\n",
    "    # Definir os intervalos de colunas\n",
    "    \n",
    "    progress_bar = tqdm(data_loader)\n",
    "    for data in progress_bar:\n",
    "        #extrair as features e orientações\n",
    "        batch_in,batch_out = data[0].to(device),data[1].to(device)\n",
    "        #predição no cenário positivo                \n",
    "        output_pos = model(batch_in, batch_out)\n",
    "        #predição no cenário negativo\n",
    "        batch_out_neg = torch.roll(batch_out, 1, 0)\n",
    "        output_neg = model(batch_in, batch_out_neg)\n",
    "        #calcular a loss\n",
    "        loss = loss_fn(output_pos, output_neg)\n",
    "        \n",
    "        if is_training:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_description(f'Loss: {loss.item()} - Total Loss: {total_loss}')\n",
    "\n",
    "    return total_loss / len(data_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class MeuDataset(Dataset):\n",
    "    def __init__(self, summary_pool_list, labels_list):\n",
    "        self.summary = summary_pool_list\n",
    "        self.labels = labels_list\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.summary )\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        summary = self.summary[idx]\n",
    "        labels = self.labels[idx]\n",
    "        # Implemente aqui a lógica para retornar uma amostra do seu conjunto de dados\n",
    "        return summary, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dt aug torch.Size([60, 1, 120, 120]) torch.Size([60, 1, 120, 120]) torch.Size([60, 1, 4, 2]) torch.Size([60, 50, 2])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_31989/732539104.py:90: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  lista_de_pontos = torch.tensor(lista_de_pontos)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 1, 120, 120]) cuda:0 cuda:0 cuda:0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'extract_features' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 22\u001b[0m\n\u001b[1;32m     20\u001b[0m filtered_points,indices \u001b[39m=\u001b[39m filtrar_keypoints(out[\u001b[39m3\u001b[39m][\u001b[39m0\u001b[39m],out[\u001b[39m1\u001b[39m][\u001b[39m0\u001b[39m,\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mbool())  \n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mshape,\u001b[39minput\u001b[39m\u001b[39m.\u001b[39mdevice,points\u001b[39m.\u001b[39mdevice,filtered_points\u001b[39m.\u001b[39mdevice)      \n\u001b[0;32m---> 22\u001b[0m patchs_mini_img1 \u001b[39m=\u001b[39mextract_patches_from_keypoints(\u001b[39minput\u001b[39;49m, points[\u001b[39m0\u001b[39;49m][indices],PS\u001b[39m=\u001b[39;49mPS)\n\u001b[1;32m     23\u001b[0m patchs_mini_img1 \u001b[39m=\u001b[39mpatchs_mini_img1\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,patchs_mini_img1\u001b[39m.\u001b[39mshape[\u001b[39m2\u001b[39m],PS,PS)\n\u001b[1;32m     24\u001b[0m patchs_mini_src \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat((patchs_mini_src,patchs_mini_img1),dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "Cell \u001b[0;32mIn[6], line 38\u001b[0m, in \u001b[0;36mextract_patches_from_keypoints\u001b[0;34m(batch, filtered_points, PS)\u001b[0m\n\u001b[1;32m     35\u001b[0m lafs \u001b[39m=\u001b[39m convert_points_to_lafs(filtered_points, batch)  \u001b[39m# B, N(filtered), 2, 3\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[39m# maps_activations = batch.repeat(1, CH, 1, 1)  # B, CH, H, W o repeat 8 é temporário enquanto não aplico a convolução\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m maps_activations \u001b[39m=\u001b[39m extract_features(model_sp,batch)\n\u001b[1;32m     39\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mmaps_activations \u001b[39m\u001b[39m'\u001b[39m,maps_activations\u001b[39m.\u001b[39mshape,maps_activations\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m     40\u001b[0m \u001b[39m# maps_activations[:, 2] = maps_activations[2, 2]\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'extract_features' is not defined"
     ]
    }
   ],
   "source": [
    "aug_list = kornia.augmentation.AugmentationSequential(\n",
    "    kornia.augmentation.RandomAffine(degrees=360, translate=(0.2, 0.2), scale=(0.95, 1.05), shear=10,p=0.8),\n",
    "    kornia.augmentation.RandomPerspective(0.1, p=0.7),\n",
    "    kornia.augmentation.RandomBoxBlur((5,5),p=0.7),\n",
    "    data_keys=[\"input\", \"mask\", \"bbox\", \"keypoints\"],\n",
    "    same_on_batch=True,\n",
    ")\n",
    "\n",
    "PS =23\n",
    "print(\"dt aug\",input.shape, mask.shape, bbox.shape, points.shape)\n",
    "orient = PassLAF()#kornia.feature.LAFOrienter(PS)PassLAF()\n",
    "patchs_mini_src = torch.tensor([])\n",
    "patchs_mini_dst = torch.tensor([])\n",
    "for input,labels in trainloader:\n",
    "    input = input.to(device)\n",
    "    with torch.no_grad():\n",
    "        params_item = next(params_lists)\n",
    "        out = aug_list(input, mask, bbox, points,params=params_item)\n",
    "        #extrair patch da imagem original\n",
    "        filtered_points,indices = filtrar_keypoints(out[3][0],out[1][0,0].bool())  \n",
    "        print(input.shape,input.device,points.device,filtered_points.device)      \n",
    "        patchs_mini_img1 =extract_patches_from_keypoints(input, points[0][indices],PS=PS)\n",
    "        patchs_mini_img1 =patchs_mini_img1.reshape(-1,patchs_mini_img1.shape[2],PS,PS)\n",
    "        patchs_mini_src = torch.cat((patchs_mini_src,patchs_mini_img1),dim=0)\n",
    "        print(patchs_mini_img1.shape,patchs_mini_src.shape)\n",
    "        plot_patches_side_by_side(patchs_mini_img1[:50,0])#plota todas os patch na feature map 0    \n",
    "        #extrair patch da imagem transformada\n",
    "        patchs_mini_img2 =extract_patches_from_keypoints(out[0], filtered_points,PS=PS)\n",
    "        patchs_mini_img2 =patchs_mini_img2.reshape(-1,patchs_mini_img2.shape[2],PS,PS)\n",
    "        patchs_mini_dst = torch.cat((patchs_mini_dst,patchs_mini_img2),dim=0)\n",
    "        print(patchs_mini_img2.shape,patchs_mini_dst.shape)\n",
    "        plot_patches_side_by_side(patchs_mini_img2[:50,0])#plota todas os patch na feature map 0    \n",
    "\n",
    "        plot_images_with_points_side_by_side(input[0],out[0][0],points[0][indices],filtered_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "print(patchs_mini_src.shape, patchs_mini_dst.shape)\n",
    "optimizer_siamese = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.0001)\n",
    "# Crie uma instância do seu conjunto de dados personalizado\n",
    "\n",
    "meu_dataset = MeuDataset(patchs_mini_src, patchs_mini_dst)\n",
    "train_dataset, val_dataset, test_dataset = random_split(meu_dataset, [0.3,0.5,0.2])\n",
    "\n",
    "# Crie uma instância do DataLoader usando seu conjunto de dados personalizado\n",
    "dataloader_train = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
    "dataloader_val = DataLoader(val_dataset, batch_size=50, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([21240, 8, 23, 23]), 1260)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patchs_mini_src.shape,len(dataloader.dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def find_best_matching_indices_knn(points1, points2, threshold, k=3):\n",
    "    distances = cdist(points1, points2)\n",
    "    best_indices = np.argsort(distances, axis=1)[:, :k]\n",
    "    best_distances = np.take_along_axis(distances, best_indices, axis=1)\n",
    "\n",
    "    matched = []\n",
    "\n",
    "    for i in range(len(points1)):\n",
    "        min_distance = np.min(best_distances[i])\n",
    "        if min_distance < threshold:\n",
    "            best_index = np.argmin(best_distances[i])\n",
    "            matched.append((i, best_indices[i, best_index]))\n",
    "\n",
    "    return matched\n",
    "\n",
    "\n",
    "def find_matching_in_batch(batch_points1, batch_points2, threshold):\n",
    "    # Lista para armazenar as correspondências\n",
    "    lista_correspondencias = []\n",
    "\n",
    "    # Iterar sobre o lote de pontos\n",
    "    for i in range(batch_points1.shape[0]):\n",
    "        # Obter os pontos correspondentes entre duas imagens\n",
    "        correspondencias = find_best_matching_indices_knn(batch_points1[i], batch_points2[i], threshold)\n",
    "        lista_correspondencias.append(correspondencias)\n",
    "\n",
    "    return lista_correspondencias\n",
    "\n",
    "def evaluates_repeatability(_kp1, _kp2,batch_size=1):\n",
    "    \n",
    "    lista_correspondencias = find_matching_in_batch(_kp1, _kp2, 2.5)\n",
    "    num_matchs = 0\n",
    "    num_matchs_with_outliers = 0\n",
    "    for i in range(batch_size):\n",
    "        \n",
    "        print(_kp1.shape, _kp2.shape)\n",
    "        pts1 = np.array([_kp1[i, match[0]] for match in lista_correspondencias[i]])\n",
    "        pts2 = np.array([_kp2[i, match[1]] for match in lista_correspondencias[i]])\n",
    "        num_matchs += len(lista_correspondencias[i])\n",
    "        \n",
    "        print(\"Quant. de correspondecias Posicionais: {}\".format(num_matchs))\n",
    "        \n",
    "        # Calcular a matriz fundamental usando o método RANSAC\n",
    "        fundamental_matrix, mask = cv2.findFundamentalMat(pts1, pts2, method=cv2.FM_RANSAC, ransacReprojThreshold=1.0)\n",
    "        print(\"Fundamental Matrix:\")\n",
    "        print(fundamental_matrix)\n",
    "        # Filtrar os pontos correspondentes com base na máscara\n",
    "        pts1_filtered = pts1[mask.ravel().astype(bool)]\n",
    "        pts2_filtered = pts2[mask.ravel().astype(bool)]\n",
    "        num_matchs_with_outliers += mask.ravel().astype(bool).sum()\n",
    "        print(\"Correspondencias sem outliers {}\".format(num_matchs_with_outliers))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_matches_keypoints(image1, keypoints1, image2, keypoints2, matches, **kwargs):\n",
    "    # Concatenar as duas imagens lado a lado\n",
    "    combined_image = np.concatenate((image1.cpu(), image2.cpu()), axis=1)\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.imshow(combined_image, cmap='gray')\n",
    "    ax.axis('off')\n",
    "\n",
    "    # Desenhar pontos correspondentes e linhas conectando-os\n",
    "    offset = image1.shape[1]\n",
    "\n",
    "    for i, (x, y) in enumerate(keypoints1.cpu()):\n",
    "        ax.plot(x, y, 'o',markerfacecolor='none', markeredgecolor='g',\n",
    "                markersize=20, markeredgewidth=1)\n",
    "        ax.annotate(str(i), (x, y), color='g',xytext=(10, 10), textcoords='offset points', fontsize=12)\n",
    "\n",
    "    for i, (x, y) in enumerate(keypoints2.cpu()):\n",
    "        ax.plot(x+offset, y, 'o',markerfacecolor='none', markeredgecolor='g',\n",
    "                markersize=20, markeredgewidth=1)\n",
    "        ax.annotate(str(i), (x+offset, y), color='g',xytext=(10, 10), textcoords='offset points', fontsize=12)\n",
    "    try:\n",
    "        for match in matches:\n",
    "            x1, y1 = keypoints1[match[0],0], keypoints1[match[0],1]\n",
    "            x2, y2 = keypoints2[match[1],0]+offset, keypoints2[match[1],1]\n",
    "            ax.plot([x1, x2], [y1, y2], '-', color='lime', lw=0.5)\n",
    "    except:\n",
    "        print(\"Erro ao plotar correspondencias\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import kornia.augmentation as KA\n",
    "from torchvision.transforms import transforms, InterpolationMode\n",
    "\n",
    "PONTOS = 10\n",
    "BATCH = 2\n",
    "\n",
    "# Definir a semente\n",
    "def seed_everything(seed=123):\n",
    "    torch.manual_seed(seed) # Definir a semente para PyTorch\n",
    "    random.seed(seed)# Definir a semente para o módulo random do Python\n",
    "    np.random.seed(seed)# Definir a semente para o módulo numpy\n",
    "    \n",
    "data_augmentation = KA.AugmentationSequential(\n",
    "    KA.RandomAffine(degrees=180, translate=(0.1, 0.1), scale=(0.95, 1.05), shear=10, p=0.8),\n",
    "    KA.RandomHorizontalFlip(p=0.5),\n",
    "    KA.RandomVerticalFlip(p=0.5),\n",
    "    same_on_batch=False,\n",
    "    data_keys=[\"input\", \"keypoints\"]\n",
    ")\n",
    "\n",
    "def laf_to_xy(lafs):\n",
    "    xy = torch.zeros((lafs.shape[0],lafs.shape[1], 2))\n",
    "    xy[:,:, 0] = lafs[:,:,0, 2]\n",
    "    xy[:,:, 1] = lafs[:,:,1, 2]\n",
    "    return xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 93, 2]) torch.Size([1, 93, 2]) <class 'torch.Tensor'>\n",
      "(1, 93, 2) (1, 93, 2)\n",
      "Quant. de correspondecias Posicionais: 9\n",
      "Fundamental Matrix:\n",
      "[[ 1.39896929e-05  1.23313573e-04  5.36051438e-02]\n",
      " [ 2.49851319e-05  1.72062378e-05  4.43945985e-03]\n",
      " [-6.80311831e-02 -1.77450376e-02  1.00000000e+00]]\n",
      "Correspondencias sem outliers 7\n",
      "tensor([[ 76.6864,  80.2367],\n",
      "        [ 10.6509,  48.2840],\n",
      "        [ 78.8166,  88.7574],\n",
      "        [ 78.8166,  63.9053],\n",
      "        [101.5385,  56.0947]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 110, 2]) torch.Size([1, 110, 2]) <class 'torch.Tensor'>\n",
      "(1, 110, 2) (1, 110, 2)\n",
      "Quant. de correspondecias Posicionais: 16\n",
      "Fundamental Matrix:\n",
      "[[ 1.76361466e-05  1.91282091e-03 -1.20345103e-01]\n",
      " [-1.79954336e-03  5.98665334e-05  1.14440731e-01]\n",
      " [ 1.07479115e-01 -1.29319742e-01  1.00000000e+00]]\n",
      "Correspondencias sem outliers 13\n",
      "tensor([[ 68.1657,  22.0118],\n",
      "        [101.5385,  46.8639],\n",
      "        [ 90.8876,  95.1479],\n",
      "        [ 90.1775,  77.3964],\n",
      "        [ 96.5680,  62.4852]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 102, 2]) torch.Size([1, 102, 2]) <class 'torch.Tensor'>\n",
      "(1, 102, 2) (1, 102, 2)\n",
      "Quant. de correspondecias Posicionais: 102\n",
      "Fundamental Matrix:\n",
      "[[ 1.50716835e-19 -1.46260981e-03  1.49000906e-01]\n",
      " [ 1.46260981e-03  3.54628986e-20 -2.45837292e-02]\n",
      " [-1.49000906e-01  2.45837292e-02  0.00000000e+00]]\n",
      "Correspondencias sem outliers 102\n",
      "tensor([[ 48.9941,  64.6154],\n",
      "        [102.9586,  81.6568],\n",
      "        [ 89.4675, 101.5385],\n",
      "        [ 58.2249,  61.7751],\n",
      "        [100.1183,  89.4675]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 82, 2]) torch.Size([1, 82, 2]) <class 'torch.Tensor'>\n",
      "(1, 82, 2) (1, 82, 2)\n",
      "Quant. de correspondecias Posicionais: 7\n",
      "Fundamental Matrix:\n",
      "[[ 2.38842503e-04  4.77803760e-03 -1.72554902e-01]\n",
      " [-4.77803760e-03  1.17107008e-04  2.80022715e-01]\n",
      " [ 1.44132644e-01 -2.88563759e-01  1.00000000e+00]\n",
      " [ 6.68533863e-05  8.21928992e-03 -1.16600866e-01]\n",
      " [-8.21928992e-03  8.55846775e-04  4.55347816e-01]\n",
      " [ 1.08645313e-01 -5.22747684e-01  1.00000000e+00]\n",
      " [ 3.48471295e-04  2.58452478e-03 -2.08220957e-01]\n",
      " [-2.58452478e-03 -3.53778296e-04  1.68267510e-01]\n",
      " [ 1.66752873e-01 -1.39290938e-01  1.00000000e+00]]\n",
      "Correspondencias sem outliers 7\n",
      "tensor([[ 90.8876,  45.4438],\n",
      "        [ 24.1420,  97.9882],\n",
      "        [105.0888, 105.0888],\n",
      "        [ 86.6272,  68.8757],\n",
      "        [ 81.6568,  25.5621]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 83, 2]) torch.Size([1, 83, 2]) <class 'torch.Tensor'>\n",
      "(1, 83, 2) (1, 83, 2)\n",
      "Quant. de correspondecias Posicionais: 12\n",
      "Fundamental Matrix:\n",
      "[[ 1.36182818e-02  2.11122220e-02 -3.59722105e+00]\n",
      " [-2.17421816e-02 -2.66899314e-03  1.77480781e+00]\n",
      " [ 2.42623831e+00 -1.25902909e+00  1.00000000e+00]]\n",
      "Correspondencias sem outliers 7\n",
      "tensor([[ 94.4379,  33.3728],\n",
      "        [ 86.6272, 101.5385],\n",
      "        [ 76.6864,  24.8521],\n",
      "        [ 68.8757,  17.7515],\n",
      "        [ 86.6272,  41.1834]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 100, 2]) torch.Size([1, 100, 2]) <class 'torch.Tensor'>\n",
      "(1, 100, 2) (1, 100, 2)\n",
      "Quant. de correspondecias Posicionais: 10\n",
      "Fundamental Matrix:\n",
      "[[ 8.26097401e-05  1.69047519e-03 -9.33711876e-02]\n",
      " [-1.62251622e-03  8.27519045e-05  5.02952211e-02]\n",
      " [ 7.92802289e-02 -6.85676876e-02  1.00000000e+00]]\n",
      "Correspondencias sem outliers 7\n",
      "tensor([[ 95.1479,  27.6923],\n",
      "        [ 60.3550,  15.6213],\n",
      "        [ 76.6864,  17.0414],\n",
      "        [ 95.1479,  18.4615],\n",
      "        [ 76.6864, 105.0888]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 96, 2]) torch.Size([1, 96, 2]) <class 'torch.Tensor'>\n",
      "(1, 96, 2) (1, 96, 2)\n",
      "Quant. de correspondecias Posicionais: 7\n",
      "Fundamental Matrix:\n",
      "[[ 5.09462924e-05 -2.55892742e-03  1.31798415e-01]\n",
      " [ 2.58798889e-03  2.40997468e-04 -2.91916348e-01]\n",
      " [-1.39609052e-01  2.61873528e-01  1.00000000e+00]]\n",
      "Correspondencias sem outliers 7\n",
      "tensor([[107.9290,  66.0355],\n",
      "        [ 66.0355,  68.8757],\n",
      "        [ 53.2544,  95.8580],\n",
      "        [ 97.2781,  71.0059],\n",
      "        [ 91.5976,  27.6923]], grad_fn=<SliceBackward0>)\n",
      "torch.Size([1, 106, 2]) torch.Size([1, 106, 2]) <class 'torch.Tensor'>\n",
      "(1, 106, 2) (1, 106, 2)\n",
      "Quant. de correspondecias Posicionais: 5\n",
      "Fundamental Matrix:\n",
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'ravel'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 55\u001b[0m\n\u001b[1;32m     52\u001b[0m kp2 \u001b[39m=\u001b[39mlaf_to_xy(laf2)\n\u001b[1;32m     54\u001b[0m \u001b[39mprint\u001b[39m(keypoints_trans\u001b[39m.\u001b[39mshape,kp2\u001b[39m.\u001b[39mshape,\u001b[39mtype\u001b[39m(keypoints_trans))\n\u001b[0;32m---> 55\u001b[0m evaluates_repeatability(keypoints_trans\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mnumpy(),kp2\u001b[39m.\u001b[39;49mdetach()\u001b[39m.\u001b[39;49mnumpy())\n\u001b[1;32m     56\u001b[0m \u001b[39mprint\u001b[39m(kp1[\u001b[39m0\u001b[39m,:\u001b[39m5\u001b[39m]\u001b[39m.\u001b[39mcpu())\n",
      "Cell \u001b[0;32mIn[1], line 52\u001b[0m, in \u001b[0;36mevaluates_repeatability\u001b[0;34m(_kp1, _kp2, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mprint\u001b[39m(fundamental_matrix)\n\u001b[1;32m     51\u001b[0m \u001b[39m# Filtrar os pontos correspondentes com base na máscara\u001b[39;00m\n\u001b[0;32m---> 52\u001b[0m pts1_filtered \u001b[39m=\u001b[39m pts1[mask\u001b[39m.\u001b[39;49mravel()\u001b[39m.\u001b[39mastype(\u001b[39mbool\u001b[39m)]\n\u001b[1;32m     53\u001b[0m pts2_filtered \u001b[39m=\u001b[39m pts2[mask\u001b[39m.\u001b[39mravel()\u001b[39m.\u001b[39mastype(\u001b[39mbool\u001b[39m)]\n\u001b[1;32m     54\u001b[0m num_matchs_with_outliers \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m mask\u001b[39m.\u001b[39mravel()\u001b[39m.\u001b[39mastype(\u001b[39mbool\u001b[39m)\u001b[39m.\u001b[39msum()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'ravel'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import kornia.feature as KF\n",
    "import torchvision\n",
    "from torchvision.transforms import transforms, InterpolationMode\n",
    "import kornia_moons.feature as KMF\n",
    "import kornia.augmentation as KA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def detect_and_match(first_batch,second_batch,keynet_hardnet,matcher):\n",
    "    laf1, _, desc1 = keynet_hardnet(first_batch)\n",
    "    laf2, _, desc2 = keynet_hardnet(second_batch)\n",
    "    \n",
    "    \n",
    "    kp1 =laf_to_xy(laf1.detach().numpy())\n",
    "    kp2 =laf_to_xy(laf2.detach().numpy())\n",
    "    \n",
    "    # print(laf1.shape,kp1.shape,desc1.shape)\n",
    "    \n",
    "    # Remove a dimensão extra dos descritores\n",
    "    desc1 = desc1[0]\n",
    "    desc2 = desc2[0]\n",
    "    \n",
    "    # Realiza a correspondência dos descritores\n",
    "    dist,matches = matcher(desc1, desc2)\n",
    "    # print(dist.shape,matches.shape)\n",
    "    \n",
    "    return kp1,kp2,matches\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Grayscale(),\n",
    "    transforms.Resize((120,120), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.Flowers102(root='./data/datasets', split='train',\n",
    "                                        download=True, transform=transform2)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=1,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "keynet_hardnet = KF.KeyNetHardNet()\n",
    "matcher = KF.DescriptorMatcher('snn', 0.5)\n",
    "seed_everything()\n",
    "\n",
    "for data in trainloader:\n",
    "    first_batch,_ = data\n",
    "    laf1, _, desc1 = keynet_hardnet(first_batch)\n",
    "    kp1 =laf_to_xy(laf1)\n",
    "\n",
    "    second_batch, keypoints_trans = data_augmentation(first_batch, kp1)  # Realizar transformações de aumento de dados\n",
    "    laf2, _, desc2 = keynet_hardnet(first_batch)\n",
    "    kp2 =laf_to_xy(laf2)\n",
    "    \n",
    "    print(keypoints_trans.shape,kp2.shape,type(keypoints_trans))\n",
    "    evaluates_repeatability(keypoints_trans.detach().numpy(),kp2.detach().numpy())\n",
    "    print(kp1[0,:5].cpu())\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keypoint 1: 0, Keypoint 2: 0, Distance: 2.8284270763397217\n",
      "Keypoint 1: 1, Keypoint 2: 1, Distance: 2.8284270763397217\n",
      "Keypoint 1: 2, Keypoint 2: 2, Distance: 7.071067810058594\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def find_matching_points(keypoints1, keypoints2, threshold):\n",
    "    matches = []\n",
    "    \n",
    "    for i, kp1 in enumerate(keypoints1):\n",
    "        best_match = None\n",
    "        best_distance = float('inf')\n",
    "        \n",
    "        for j, kp2 in enumerate(keypoints2):\n",
    "            distance = np.linalg.norm(np.array(kp1.pt) - np.array(kp2.pt))\n",
    "            \n",
    "            if distance < best_distance:\n",
    "                best_distance = distance\n",
    "                best_match = j\n",
    "        \n",
    "        if best_distance < threshold:\n",
    "            matches.append(cv2.DMatch(i, best_match, best_distance))\n",
    "    \n",
    "    return matches\n",
    "\n",
    "\n",
    "# Exemplo de uso\n",
    "# keypoints1 e keypoints2 são listas de keypoints usando a estrutura cv2.KeyPoint do OpenCV\n",
    "keypoints1 = [cv2.KeyPoint(10, 20, 1.0), cv2.KeyPoint(30, 40, 1.0), cv2.KeyPoint(50, 60, 1.0)]\n",
    "keypoints2 = [cv2.KeyPoint(12, 22, 1.0), cv2.KeyPoint(28, 38, 1.0), cv2.KeyPoint(55, 65, 1.0)]\n",
    "threshold = 10.0\n",
    "\n",
    "matches = find_matching_points(keypoints1, keypoints2, threshold)\n",
    "\n",
    "# Imprimir correspondências encontradas\n",
    "for match in matches:\n",
    "    print(f\"Keypoint 1: {match.queryIdx}, Keypoint 2: {match.trainIdx}, Distance: {match.distance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 2.8284270763397217\n",
      "1 1 2.8284270763397217\n",
      "2 2 7.071067810058594\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def find_matching_points(keypoints1, keypoints2, threshold):\n",
    "    # Converter os keypoints em matrizes NumPy com tipo CV_32F\n",
    "    keypoints1_np = np.array([kp.pt for kp in keypoints1], dtype=np.float32)\n",
    "    keypoints2_np = np.array([kp.pt for kp in keypoints2], dtype=np.float32)\n",
    "\n",
    "    # Inicializar o matcher de força bruta\n",
    "    matcher = cv2.BFMatcher(cv2.NORM_L2)  # Usando a métrica de distância L2\n",
    "\n",
    "    # Encontrar correspondências entre os keypoints\n",
    "    matches = matcher.match(keypoints1_np, keypoints2_np)\n",
    "\n",
    "    # Filtrar correspondências com base no threshold\n",
    "    good_matches = [match for match in matches if match.distance < threshold]\n",
    "\n",
    "    return good_matches\n",
    "\n",
    "# Exemplo de uso\n",
    "keypoints1 = [cv2.KeyPoint(10, 20, 1.0), cv2.KeyPoint(30, 40, 1.0), cv2.KeyPoint(50, 60, 1.0)]\n",
    "keypoints2 = [cv2.KeyPoint(12, 22, 1.0), cv2.KeyPoint(28, 38, 1.0), cv2.KeyPoint(55, 65, 1.0)]\n",
    "threshold = 10.0\n",
    "\n",
    "matches = find_matching_points(keypoints1, keypoints2, threshold)\n",
    "\n",
    "# Imprimir correspondências encontradas\n",
    "for match in matches:\n",
    "    print(match.queryIdx, match.trainIdx, match.distance)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "# Fixar a semente do Torch para operações específicas\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importa e plota tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First load libraries and images\n",
    "import math\n",
    "import torchvision\n",
    "from torch import optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR\n",
    "from torchvision.transforms import transforms, InterpolationMode\n",
    "import best.singular_point as sp\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((sp.args.img_size, sp.args.img_size), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "transform2 = transforms.Compose([\n",
    "    transforms.Resize((sp.args.img_size, sp.args.img_size), interpolation=InterpolationMode.BICUBIC),\n",
    "    transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5), (0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.Flowers102(root='./data/datasets', split='train',\n",
    "                                        download=True, transform=transform2)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=60,\n",
    "                                        shuffle=False, num_workers=2)\n",
    "\n",
    "device = sp.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60, 1, 120, 120]) torch.Size([60])\n"
     ]
    }
   ],
   "source": [
    "iterator=iter(trainloader)\n",
    "img,labels = next(iterator)\n",
    "print(img.shape,labels.shape)\n",
    "# img = img.to(sp.device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustonNetDetector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(exported=False, num_channels=1, pyramid_levels=3, scale_pyramid=1.3, dim_first=3, dim_second=5, dim_third=8, group_size=36, epochs=70, border_size=12, box_size=21, nms_size=5, img_size=120, batch_size=16, path_data='./data', path_model='model.pt', is_loss_ssim=True, margin_loss=2.0, outlier_rejection=False, show_feature=False)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wagner/.local/lib/python3.11/site-packages/e2cnn/nn/modules/r2_conv/basisexpansion_singleblock.py:80: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at ../aten/src/ATen/native/IndexingUtils.h:27.)\n",
      "  full_mask[mask] = norms.to(torch.uint8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from ./data/models/sp_map_fo_30.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def save_model(model, filepath):\n",
    "    torch.save(model.state_dict(), filepath)\n",
    "    print(f\"Model saved to {filepath}\")\n",
    "\n",
    "def load_model(model, filepath):\n",
    "    model.load_state_dict(torch.load(filepath,map_location=sp.device))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {filepath}\")\n",
    "    \n",
    "\n",
    "# teste = model.copy()\n",
    "path_siamese = './data/models/sp_map_fo_30.pth'\n",
    "sp.args.num_channels = 1\n",
    "model = sp.SingularPoints(args=sp.args).to(sp.device)\n",
    "load_model(model,path_siamese)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "PS = 19\n",
    "\n",
    "def create_circular_mask(h, w, center=None, radius=None):\n",
    "    if center is None:  # use the middle of the image\n",
    "        center = (int(w / 2), int(h / 2))\n",
    "    if radius is None:  # use the smallest distance between the center and image walls\n",
    "        radius = min(center[0], center[1], w - center[0], h - center[1])\n",
    "    Y, X = np.ogrid[:h, :w]\n",
    "    dist_from_center = np.sqrt((X - center[0]) ** 2 + (Y - center[1]) ** 2)\n",
    "    mask = dist_from_center <= radius\n",
    "    return mask\n",
    " \n",
    "from torch import nn\n",
    "class Detector(nn.Module):\n",
    "        def __init__(self,model) -> None:\n",
    "           super().__init__()\n",
    "           self.model = model\n",
    "        def forward(self, x):\n",
    "            features_key,features_key_summary,features_ori,features_ori_summary,max_coords_values, max_map= self.model(x)\n",
    "            return features_key_summary\n",
    "        \n",
    "class Detector2(nn.Module):\n",
    "        def __init__(self,model) -> None:\n",
    "           super().__init__()\n",
    "           self.model = model\n",
    "        def forward(self, x):\n",
    "            features_key,features_key_summary,features_ori,features_ori_summary,max_coords_values, max_map= self.model(x)\n",
    "            return features_key\n",
    "        \n",
    "from external.hardnet_pytorch import HardNet        \n",
    "class Descriptor(nn.Module):\n",
    "        def __init__(self) -> None:\n",
    "           super().__init__()           \n",
    "           hardnet = HardNet()\n",
    "           checkpoint = torch.load('trained_models/pretrained_nets/HardNet++.pth')\n",
    "           hardnet.load_state_dict(checkpoint['state_dict'])\n",
    "           hardnet.eval()\n",
    "           hardnet.to(device) \n",
    "           self.model = hardnet\n",
    "        def forward(self, x):\n",
    "           return self.model(x)\n",
    "\n",
    "\n",
    "\n",
    "detec = Detector(model)\n",
    "detec2 = Detector2(model)\n",
    "desc = Descriptor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import kornia\n",
    "from kornia.feature.scale_space_detector import get_default_detector_config, MultiResolutionDetector\n",
    "\n",
    "device = sp.device\n",
    "\n",
    "keynet_default_config = {\n",
    "    'num_filters': 8,\n",
    "    'num_levels': 3,\n",
    "    'kernel_size': 5,\n",
    "    'Detector_conf': {'nms_size': 15, 'pyramid_levels': 1, 'up_levels': 1, 'scale_factor_levels': 1.3, 's_mult': 22.0},\n",
    "}\n",
    "\n",
    "class CustomNetDetector(MultiResolutionDetector):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        pretrained: bool = False,\n",
    "        num_features: int = 60,\n",
    "        keynet_conf=keynet_default_config,\n",
    "        ori_module=kornia.feature.LAFOrienter(PS),\n",
    "        aff_module=None,#kornia.feature.LAFAffineShapeEstimator(PS),\n",
    "    ):\n",
    "        super().__init__(model, num_features, keynet_conf['Detector_conf'], ori_module, aff_module)\n",
    "\n",
    "\n",
    "# timg_gray = img.to(device)#timg_gray.to(device)\n",
    "sift = kornia.feature.SIFTDescriptor(PS, rootsift=True).to(device)\n",
    "descriptor = desc#sift\n",
    "detector = CustomNetDetector(detec).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kornia\n",
    "\n",
    "class AugmentationParamsGenerator:\n",
    "    def __init__(self, n, shape):\n",
    "        torch.manual_seed(0)\n",
    "        torch.cuda.manual_seed(0)\n",
    "        \n",
    "        aug_list = kornia.augmentation.AugmentationSequential(\n",
    "            kornia.augmentation.RandomAffine(degrees=360, translate=(0.2, 0.2), scale=(0.95, 1.05), shear=10,p=0.8),\n",
    "            kornia.augmentation.RandomPerspective(0.1, p=0.7),\n",
    "            kornia.augmentation.RandomBoxBlur((5,5),p=0.7),\n",
    "            data_keys=[\"input\"],\n",
    "            same_on_batch=True,\n",
    "            # random_apply=10,\n",
    "        )\n",
    "\n",
    "        self.index = 0\n",
    "        self.data = []\n",
    "        for i in range(n):\n",
    "            out = aug_list(torch.rand(shape))\n",
    "            self.data.append(aug_list._params)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return self\n",
    "\n",
    "    def __next__(self):\n",
    "        if self.index >= len(self.data):\n",
    "            self.index = 0  # Reset index to start over for circular iteration\n",
    "            \n",
    "        result = self.data[self.index]\n",
    "        self.index += 1\n",
    "        return result\n",
    "\n",
    "params_lists =AugmentationParamsGenerator(3,img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def bidirectional_match(feat1, feat2, threshold=0.5):\n",
    "#     feat1 = feat1.float()\n",
    "#     feat2 = feat2.float()\n",
    "    \n",
    "#     diff = feat1.unsqueeze(1) - feat2.unsqueeze(0)\n",
    "#     dist_matrix = torch.norm(diff, p=2, dim=2)\n",
    "    \n",
    "#     min_indices_feat1_to_feat2 = torch.argmin(dist_matrix, dim=1)\n",
    "#     min_indices_feat2_to_feat1 = torch.argmin(dist_matrix, dim=0)\n",
    "#     print(min_indices_feat1_to_feat2.shape,min_indices_feat2_to_feat1.shape)\n",
    "#     matches = []\n",
    "#     for i, match_index in enumerate(min_indices_feat1_to_feat2):\n",
    "#         if min_indices_feat2_to_feat1[match_index] == i:\n",
    "#             dist = dist_matrix[i, match_index]\n",
    "#             if dist <= threshold:\n",
    "#                 matches.append((i, match_index.item()))\n",
    "\n",
    "#     return torch.tensor(matches)\n",
    "\n",
    "def bidirectional_match(feat1, feat2, threshold=0.5):\n",
    "    feat1 = feat1.float()\n",
    "    feat2 = feat2.float()\n",
    "\n",
    "    s1, matches1 = kornia.feature.match_snn(feat1, feat2, threshold)\n",
    "    s2, matches2 = kornia.feature.match_snn(feat2, feat1, threshold)\n",
    "    \n",
    "    bidirectional_matches = []\n",
    "    for i, match in enumerate(matches1):\n",
    "        indices = torch.where(matches2[:, 0] == match[1].item())[0]\n",
    "        if indices.numel() > 0:\n",
    "            for index in indices:\n",
    "                if matches2[index][1].item() == match[0].item():\n",
    "                     bidirectional_matches.append((match[0].item(), match[1].item()))\n",
    "    return torch.tensor(bidirectional_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_extract_features(image, detector, descriptor, PS):\n",
    "    with torch.no_grad():\n",
    "        lafs, resps = detector(image[None])\n",
    "        print('lafs ',lafs.shape,image[None].shape)\n",
    "        map = detec2(image[None])\n",
    "        # print('map ',map.shape,image[None].shape)\n",
    "        patches = kornia.feature.extract_patches_from_pyramid(image[None], lafs, 32)\n",
    "        B, N, CH, H, W = patches.size()\n",
    "        # print('patches  ',patches.shape)\n",
    "        descs = descriptor(patches.view(B * N, CH, H, W)).view(B, N, -1)\n",
    "        # print('descs  ',descs.shape)\n",
    "        return lafs, descs\n",
    "\n",
    "def detect_extract_feat_in_batch(batch_img, detector, descriptor, PS):\n",
    "    repo_lafs_desc = []\n",
    "    with torch.no_grad():\n",
    "        for image  in batch_img:\n",
    "            \n",
    "            lafs, resps = detector(image[None])\n",
    "            patches = kornia.feature.extract_patches_from_pyramid(image[None], lafs, 32)\n",
    "\n",
    "            B, N, CH, H, W = patches.size()\n",
    "            descs = descriptor(patches.view(B * N, CH, H, W)).view(B, N, -1)\n",
    "            repo_lafs_desc.append((lafs,descs))\n",
    "    return repo_lafs_desc\n",
    "\n",
    "def compute_homography(lafs1, lafs2, matches):\n",
    "    src_pts = lafs1[0, matches[:, 0], :, 2].data.cpu().numpy()\n",
    "    dst_pts = lafs2[0, matches[:, 1], :, 2].data.cpu().numpy()\n",
    "    F, inliers_mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 1.0, 0.999, 1000)\n",
    "    inliers_point = matches[torch.from_numpy(inliers_mask).bool().squeeze(), :]\n",
    "    return inliers_mask\n",
    "\n",
    "def matching_imagens(ref_img,batch_img, repo_lafs_desc):\n",
    "    best_match_info = None\n",
    "    best_match_count = 0\n",
    "    best_match_index = None\n",
    "    with torch.no_grad():\n",
    "        # Detectar e extrair características da imagem de referência\n",
    "        lafs_ref, descs_ref = detect_and_extract_features(ref_img, detector, descriptor, PS)\n",
    "        \n",
    "        for i, (lafs_i, descs_i) in enumerate(repo_lafs_desc):\n",
    "            # Detectar e extrair características da imagem atual do batch\n",
    "            # lafs_i, descs_i = detect_and_extract_features(img, detector, descriptor, PS)\n",
    "            # Comparar as características da imagem de referência com a imagem atual do batch\n",
    "            scores, matches = kornia.feature.match_snn(descs_ref[0], descs_i[0], 0.85) # correspondencia dos descritories a uma distância de 0.9\n",
    "\n",
    "            if matches.shape[0] >= 4:\n",
    "                # Cálculo da homografia\n",
    "                inliers_mask = compute_homography(lafs_ref, lafs_i, matches)\n",
    "                # print(lafs_ref[0][None].shape, lafs_ref[0].shape, matches.shape, inliers_mask.shape)\n",
    "\n",
    "                # Check if this match is better than the previous best match\n",
    "                if matches.shape[0] > best_match_count:\n",
    "                    best_match_info = (lafs_ref[0][None].cpu(), lafs_i[0][None].cpu(), matches.cpu(),\n",
    "                                       kornia.tensor_to_image(ref_img.cpu()), kornia.tensor_to_image(batch_img[i].cpu()),\n",
    "                                       inliers_mask)\n",
    "                    best_match_count = matches.shape[0]\n",
    "                    best_match_index = i\n",
    "\n",
    "        if best_match_info is not None and best_match_index==0:# TODO: Remove this condition best_match_index==0\n",
    "            # Plot the best match\n",
    "            from kornia_moons.viz import draw_LAF_matches\n",
    "\n",
    "            draw_LAF_matches(\n",
    "                *best_match_info,\n",
    "                draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": (1, 1, 0.2, 0.3), \"feature_color\": None, \"vertical\": False},\n",
    "            )\n",
    "        # else:\n",
    "        #     print(\"No matches found with enough inliers.\")\n",
    "    return best_match_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "306dcf3e6e454df6a415bb644e1dcf14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/17 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "cannot reshape tensor of 0 elements into shape [0, -1] because the unspecified dimension size -1 can be any value and is ambiguous",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m params_item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(params_lists)\n\u001b[1;32m     19\u001b[0m timg_gray_t \u001b[39m=\u001b[39m aug_list(imgs_batch,params\u001b[39m=\u001b[39mparams_item)\n\u001b[0;32m---> 20\u001b[0m repo_lafs_desc\u001b[39m=\u001b[39m detect_extract_feat_in_batch(timg_gray_t,detector,descriptor,PS)\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m i,img_gray \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(imgs_batch):\u001b[39m# itera em cada batch\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     match_index \u001b[39m=\u001b[39m matching_imagens(img_gray,timg_gray_t,repo_lafs_desc)\n",
      "Cell \u001b[0;32mIn[9], line 22\u001b[0m, in \u001b[0;36mdetect_extract_feat_in_batch\u001b[0;34m(batch_img, detector, descriptor, PS)\u001b[0m\n\u001b[1;32m     19\u001b[0m         patches \u001b[39m=\u001b[39m kornia\u001b[39m.\u001b[39mfeature\u001b[39m.\u001b[39mextract_patches_from_pyramid(image[\u001b[39mNone\u001b[39;00m], lafs, \u001b[39m32\u001b[39m)\n\u001b[1;32m     21\u001b[0m         B, N, CH, H, W \u001b[39m=\u001b[39m patches\u001b[39m.\u001b[39msize()\n\u001b[0;32m---> 22\u001b[0m         descs \u001b[39m=\u001b[39m descriptor(patches\u001b[39m.\u001b[39;49mview(B \u001b[39m*\u001b[39;49m N, CH, H, W))\u001b[39m.\u001b[39mview(B, N, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m         repo_lafs_desc\u001b[39m.\u001b[39mappend((lafs,descs))\n\u001b[1;32m     24\u001b[0m \u001b[39mreturn\u001b[39;00m repo_lafs_desc\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m, in \u001b[0;36mDescriptor.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 41\u001b[0m    \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documentos/python/singular-points/external/hardnet_pytorch.py:70\u001b[0m, in \u001b[0;36mHardNet.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 70\u001b[0m     x_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minput_norm(\u001b[39minput\u001b[39;49m))\n\u001b[1;32m     71\u001b[0m     x \u001b[39m=\u001b[39m x_features\u001b[39m.\u001b[39mview(x_features\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     72\u001b[0m     \u001b[39mreturn\u001b[39;00m L2Norm()(x)\n",
      "File \u001b[0;32m~/Documentos/python/singular-points/external/hardnet_pytorch.py:63\u001b[0m, in \u001b[0;36mHardNet.input_norm\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39minput_norm\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 63\u001b[0m     flat \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39;49mview(x\u001b[39m.\u001b[39;49msize(\u001b[39m0\u001b[39;49m), \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m     64\u001b[0m     mp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmean(flat, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m     65\u001b[0m     sp \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstd(flat, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m+\u001b[39m \u001b[39m1e-7\u001b[39m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cannot reshape tensor of 0 elements into shape [0, -1] because the unspecified dimension size -1 can be any value and is ambiguous"
     ]
    }
   ],
   "source": [
    "aug_list = kornia.augmentation.AugmentationSequential(\n",
    "            kornia.augmentation.RandomAffine(degrees=360, translate=(0.2, 0.2), scale=(0.95, 1.05), shear=10,p=0.8),\n",
    "            kornia.augmentation.RandomPerspective(0.1, p=0.7),\n",
    "            kornia.augmentation.RandomBoxBlur((5,5),p=0.7),\n",
    "    # kornia.augmentation.RandomEqualize(p=0.3),\n",
    "    data_keys=[\"input\"],#data_keys=[\"input\", \"mask\", \"bbox\", \"keypoints\"],\n",
    "    same_on_batch=True,\n",
    "    # random_apply=15,\n",
    ")\n",
    "\n",
    "acertos = 0\n",
    "total = 0\n",
    "from tqdm.notebook import tqdm\n",
    "pbar =  tqdm(trainloader)\n",
    "for imgs_batch,labels_batch in pbar:# itera em todo dataset\n",
    "    imgs_batch = imgs_batch.to(device)\n",
    "    \n",
    "    params_item = next(params_lists)\n",
    "    timg_gray_t = aug_list(imgs_batch,params=params_item)\n",
    "    repo_lafs_desc= detect_extract_feat_in_batch(timg_gray_t,detector,descriptor,PS)\n",
    "        \n",
    "    for i,img_gray in enumerate(imgs_batch):# itera em cada batch\n",
    "\n",
    "        match_index = matching_imagens(img_gray,timg_gray_t,repo_lafs_desc)\n",
    "        # print(\"match_index: \",match_index,\" i: \",i)\n",
    "        total+=1\n",
    "        if match_index == i:\n",
    "            acertos += 1\n",
    "        pbar.set_description(f\"acertos/total: {acertos}/{total}  \")\n",
    "print(\"acertos: \",acertos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

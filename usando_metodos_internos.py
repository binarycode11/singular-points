# -*- coding: utf-8 -*-
"""Cópia de keypoint_net_eq_gr.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d5jDVbRKdXHoP2NBsZFByrwF04gP185j

## Instalação
"""
"""# Importar bibliotecas"""

import torch
import torchvision
import torchvision.transforms as transforms

from e2cnn import gspaces
from e2cnn import nn

import kornia as K
import random

import matplotlib.pyplot as plt

# !rm -rf singularpoints images
from config import device, args
from utils import imread, imshow
from training import random_augmentation
from training import KeyPointsSelection

# from singularpoints.keypoint_loss import kp_loss
print(device, args)

"""# Definir Model
- criar classe
- testar com dado sintetico
"""

import torch.nn.functional as F


class KeyEqGroup(torch.nn.Module):
    def __init__(self, args) -> None:
        super(KeyEqGroup, self).__init__()

        r2_act = gspaces.Rot2dOnR2(N=args.group_size)

        self.pyramid_levels = args.pyramid_levels
        self.feat_type_in = nn.FieldType(r2_act, args.num_channels * [
            r2_act.trivial_repr])  ## input 1 channels (gray scale image)

        feat_type_out1 = nn.FieldType(r2_act, args.dim_first * [r2_act.regular_repr])
        feat_type_out2 = nn.FieldType(r2_act, args.dim_second * [r2_act.regular_repr])
        feat_type_out3 = nn.FieldType(r2_act, args.dim_third * [r2_act.regular_repr])

        feat_type_ori_est = nn.FieldType(r2_act, [r2_act.regular_repr])

        self.block1 = nn.SequentialModule(
            nn.R2Conv(self.feat_type_in, feat_type_out1, kernel_size=5, padding=2, bias=False),
            nn.InnerBatchNorm(feat_type_out1),
            nn.ReLU(feat_type_out1, inplace=True)
        )
        self.block2 = nn.SequentialModule(
            nn.R2Conv(feat_type_out1, feat_type_out2, kernel_size=5, padding=2, bias=False),
            nn.InnerBatchNorm(feat_type_out2),
            nn.ReLU(feat_type_out2, inplace=True)
        )
        self.block3 = nn.SequentialModule(
            nn.R2Conv(feat_type_out2, feat_type_out3, kernel_size=5, padding=2, bias=False),
            nn.InnerBatchNorm(feat_type_out3),
            nn.ReLU(feat_type_out3, inplace=True)
        )

        self.ori_learner = nn.SequentialModule(
            nn.R2Conv(feat_type_out3, feat_type_ori_est, kernel_size=1, padding=0, bias=False)
            ## Channel pooling by 8*G -> 1*G conv.
        )

        self.gpool = nn.GroupPooling(feat_type_out3)

        self.softmax = torch.nn.Softmax(dim=1)
        self.last_layer_learner = torch.nn.Sequential(
            torch.nn.BatchNorm2d(num_features=2 * self.pyramid_levels),
            torch.nn.Conv2d(in_channels=2 * self.pyramid_levels, out_channels=1, kernel_size=1, bias=True),
            torch.nn.ReLU(inplace=True)  ## clamp to make the scores positive values.
        )

        self.exported = False

    def forward(self, input_data):
        return self.compute_features(input_data)

    def compute_features(self, input_data):
        B, C, H, W = input_data.shape
        # print("Shape ",B,C,H,W)
        for idx_level in range(self.pyramid_levels):
            features_t, features_o = self._forward_network(input_data)

            features_t = F.interpolate(features_t, size=(H, W), align_corners=True, mode='bilinear')
            features_o = F.interpolate(features_o, size=(H, W), align_corners=True, mode='bilinear')
            # print("Shape 1# ",idx_level,features_t.shape, features_o.shape)
            if idx_level == 0:
                features_key = features_t
                features_ori = features_o
            else:
                features_key = torch.cat([features_key, features_t], axis=1)  # concatena no eixo X
                features_ori = torch.add(features_ori, features_o)  # somatorio dos kernels
            # print("Shape 2# ",idx_level,features_key.shape, features_ori.shape)

        features_key = self.last_layer_learner(features_key)
        features_ori = self.softmax(features_ori)
        # print("Shape 3# ",idx_level,features_key.shape, features_ori.shape)
        return features_key, features_ori

    def _forward_network(self, input_data_resized):
        features_t = nn.GeometricTensor(input_data_resized,
                                        self.feat_type_in) if not self.exported else input_data_resized
        features_t = self.block1(features_t)
        features_t = self.block2(features_t)
        features_t = self.block3(features_t)

        # orientação
        features_o = self.ori_learner(features_t)  ## self.cpool
        features_o = features_o.tensor if not self.exported else features_o

        # keypoint
        features_t = self.gpool(features_t)
        features_t = features_t.tensor if not self.exported else features_t

        return features_t, features_o

    def load(self,map):
        self.gpool.load_state_dict(map['gpool'])



model = KeyEqGroup(args).to(device)


def remove_borders(images, borders):
    ## input [B,C,H,W]
    shape = images.shape

    if len(shape) == 4:
        for batch_id in range(shape[0]):
            images[batch_id, :, 0:borders, :] = 0
            images[batch_id, :, :, 0:borders] = 0
            images[batch_id, :, shape[2] - borders:shape[2], :] = 0
            images[batch_id, :, :, shape[3] - borders:shape[3]] = 0
    elif len(shape) == 2:
        images[0:borders, :] = 0
        images[:, 0:borders] = 0
        images[shape[0] - borders:shape[0], :] = 0
        images[:, shape[1] - borders:shape[1]] = 0
    else:
        print("Not implemented")
        exit()

    return images


from tqdm import tqdm
from skimage.transform import rotate
import torch.optim as optim


def train(model, train_loader, optimizer, epoch):
    model.train()
    criterion = kp_loss()

    running_loss = 0.

    for batch_idx, (batch_image, batch_label) in enumerate(tqdm(train_loader)):
        optimizer.zero_grad()
        # prever detector/descritor
        batch_image = batch_image.to(device)
        _kp1, _orie1 = model(batch_image)
        points = torch.randn(batch_image.shape[0], 2, 2)  # BxNx2 [x,y]
        imgs_trans, feature_kp_trans, features_ori_trans, coords_trans, mask_trans = random_augmentation(batch_image,
                                                                                                         _kp1, _orie1,
                                                                                                         points)
        _kp2, _orie2 = model(imgs_trans)

        # print(batch_image[0].min(), batch_image[0].max(), batch_image[0].mean())
        # print(imgs_trans[0].min(), imgs_trans[0].max(), imgs_trans[0].mean())
        loss = criterion(feature_kp_trans, _kp2)
        # loss = criterion(feature_kp_trans, _kp2,mask_trans.to(device))

        loss.backward()
        item = loss.item()
        running_loss += item
        # print(item,running_loss)
        optimizer.step()

    print('Train Epoch: {} \tLoss: {:.15f}'.format(
        epoch, running_loss))

    torch.save(model.state_dict(), 'best_model.pt')
    # torch.save({'epoch': epoch,
    #         'model_state_dict': model.state_dict(),
    #         'optimizer_state_dict': optimizer.state_dict(),
    #         'loss': running_loss},
    #   './model_best.pth')


def test(model, test_loader):

    return None


"""# Definir dataloader"""


transform = transforms.Compose(
    [transforms.ToTensor(),
     transforms.Resize(size=(196, 196)),
     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
     ])

trainset = torchvision.datasets.Flowers102(root='./data', split='train',
                                           download=True, transform=transform)
testset = torchvision.datasets.Flowers102(root='./data', split='test',
                                          download=True, transform=transform)

trainloader = torch.utils.data.DataLoader(trainset, batch_size=args.batch_size,
                                          shuffle=True, num_workers=2)

testloader = torch.utils.data.DataLoader(testset, batch_size=args.batch_size,
                                         shuffle=False, num_workers=2)


# from kornia.losses import ssim_loss
# class kp_loss:
#   def __init__(self) -> None:
#      self.basic = ssim_loss
#      self.margin = 1
#      self.margin_std = 0.001

#   def __call__(self, im1,im2,mask):
#      _std = im1.std()+self.margin_std
#      return (self.basic(im1*mask,im2*mask,3,reduction='sum')+self.margin)/(torch.sum(mask)*_std)

class kp_loss:
    def __init__(self) -> None:
        self.basic = torch.nn.MSELoss()

    def __call__(self, im1, im2):
        _std = im1.std() + 0.01
        return self.basic(im1, im2) / _std


# model=train(model,args)
torch.manual_seed(0)
args.epochs = 1

optimizer = optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.00001)
model = KeyEqGroup(args).to(device)
try:
    map_location = torch.device(device)

    # checkpoint = torch.load('./model_best.pth', map_location=map_location)
    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    # epoch = checkpoint['epoch']
    # loss = checkpoint['loss']
    # model.load(checkpoint['model_state_dict'])

    print(map_location)
    model.load_state_dict(torch.load('./best_model.pt', map_location=map_location))
    print("Já foi treinado")
except:
    print("Não foi treinado ainda")
    # print(loss,epoch,optimizer)
    for epoch in range(args.epochs):
        train(model.to(device), trainloader, optimizer, epoch)
        # test_accuracy = test(model, test_loader)
        # print(test_accuracy)

img1: torch.Tensor = imread('./data/arturito.jpg')
img2: torch.Tensor = imread('./data/simba.png')

resize = K.augmentation.Resize((196, 196))
img1, img2 = resize(img1), resize(img2)
print(img1.mean())
img_vis = torch.cat([img1, img2], dim=-1)
imshow(img_vis)

# img batch
img_batch = torch.cat([img1, img2], dim=0)
img_batch = img_batch.to(device)
print(img_batch.shape)

# calcular pontos feature
_kp1, _orie1 = model(img_batch)
print(_kp1.shape, _orie1.shape)
img_feat_kp = torch.cat([_kp1[0], _kp1[1]], dim=-1)
imshow(img_feat_kp)

points = torch.randn(img_batch.shape[0], 2, 2)  # BxNx2 [x,y]
imgs_trans, feat_kp_trans, feat_ori_trans, coords_trans, mask_trans = random_augmentation(img_batch, _kp1, _orie1,
                                                                                          points)
mask_trans = mask_trans.to(device)
print(" IMAGE: {}\n Feat KP.: {}\n Feat Ori.: {}\n KP: {}\n Masks: {}".format(
    imgs_trans.shape,
    feat_kp_trans.shape,
    feat_ori_trans.shape,
    coords_trans.shape,

    mask_trans.shape)
)
imgs_trans = imgs_trans.to(device)
imshow(feat_kp_trans[0])

_kp2, _orie2 = model(imgs_trans)
print(_kp2.shape, _orie2.shape)

img_feat_kp2 = torch.cat([feat_kp_trans[0], _kp2[0]], dim=-1)
imshow(img_feat_kp2)
kp_selec = KeyPointsSelection()

img_feat_kp2 = torch.cat([feat_kp_trans[0] * mask_trans[0], _kp2[0] * mask_trans[0]], dim=-1)
imshow(img_feat_kp2)
diff = (feat_kp_trans[0] * mask_trans[0]) - (_kp2[0] * mask_trans[0])
points = kp_selec(feat_kp_trans[0][0].detach().cpu(), 15, 100)
points = kp_selec(_kp2[0][0].detach().cpu(), 15, 100)
imshow(diff)

kp_selec = KeyPointsSelection()


def prever(img, model):
    model.eval()
    _k, _o = model(img.to(device))

    feat_k1 = _k[0][0].detach().cpu()
    plt.imshow(feat_k1)
    plt.show()
    print(feat_k1.min(), feat_k1.max(), feat_k1.mean())

    plt.imshow(_o[0][1].detach().cpu())
    plt.show()

    print(_o.shape)
    _b, _na, _c, _r = _o.shape  # bacth,num degree,col,row
    ori_arg_max = torch.argmax(_o, dim=1)
    bin_size = 360 / _na
    ori_arg_max = ori_arg_max * bin_size  # direcao do gradiente
    # para cada pixel
    print("orientacao")
    plt.imshow(ori_arg_max[0].detach().cpu())
    plt.show()

    img_temp = _k[0][0]
    new_img = remove_borders(img_temp, 7)
    points = kp_selec(new_img.detach().cpu(), 15, 60)


prever(img_batch, model)

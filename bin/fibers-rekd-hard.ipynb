{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data/datasets/fibers/doc_000.jpg', './data/datasets/fibers/doc_001.jpg', './data/datasets/fibers/doc_002.jpg', './data/datasets/fibers/doc_003.jpg', './data/datasets/fibers/doc_004.jpg', './data/datasets/fibers/doc_005.jpg', './data/datasets/fibers/doc_006.jpg', './data/datasets/fibers/doc_007.jpg', './data/datasets/fibers/doc_008.jpg', './data/datasets/fibers/doc_010.jpg', './data/datasets/fibers/doc_011.jpg', './data/datasets/fibers/doc_012.jpg', './data/datasets/fibers/doc_0128.jpg', './data/datasets/fibers/doc_0129.jpg', './data/datasets/fibers/doc_013.jpg', './data/datasets/fibers/doc_0130.jpg', './data/datasets/fibers/doc_0131.jpg', './data/datasets/fibers/doc_0132.jpg', './data/datasets/fibers/doc_0133.jpg', './data/datasets/fibers/doc_0134.jpg', './data/datasets/fibers/doc_0135.jpg', './data/datasets/fibers/doc_0136.jpg', './data/datasets/fibers/doc_0137.jpg', './data/datasets/fibers/doc_0138.jpg', './data/datasets/fibers/doc_0139.jpg', './data/datasets/fibers/doc_014.jpg', './data/datasets/fibers/doc_0140.jpg', './data/datasets/fibers/doc_0141.jpg', './data/datasets/fibers/doc_0142.jpg', './data/datasets/fibers/doc_0143.jpg', './data/datasets/fibers/doc_0144.jpg', './data/datasets/fibers/doc_0145.jpg', './data/datasets/fibers/doc_0146.jpg', './data/datasets/fibers/doc_0147.jpg', './data/datasets/fibers/doc_0148.jpg', './data/datasets/fibers/doc_0149.jpg', './data/datasets/fibers/doc_015.jpg', './data/datasets/fibers/doc_0150.jpg', './data/datasets/fibers/doc_0151.jpg', './data/datasets/fibers/doc_0152.jpg', './data/datasets/fibers/doc_0153.jpg', './data/datasets/fibers/doc_0154.jpg', './data/datasets/fibers/doc_0155.jpg', './data/datasets/fibers/doc_0156.jpg', './data/datasets/fibers/doc_0157.jpg', './data/datasets/fibers/doc_0158.jpg', './data/datasets/fibers/doc_0159.jpg', './data/datasets/fibers/doc_016.jpg', './data/datasets/fibers/doc_0160.jpg', './data/datasets/fibers/doc_0161.jpg', './data/datasets/fibers/doc_0162.jpg', './data/datasets/fibers/doc_0163.jpg', './data/datasets/fibers/doc_0164.jpg', './data/datasets/fibers/doc_0165.jpg', './data/datasets/fibers/doc_0166.jpg', './data/datasets/fibers/doc_0167.jpg', './data/datasets/fibers/doc_0168.jpg', './data/datasets/fibers/doc_0169.jpg', './data/datasets/fibers/doc_017.jpg', './data/datasets/fibers/doc_0170.jpg', './data/datasets/fibers/doc_0171.jpg', './data/datasets/fibers/doc_0172.jpg', './data/datasets/fibers/doc_0173.jpg', './data/datasets/fibers/doc_0174.jpg', './data/datasets/fibers/doc_0175.jpg', './data/datasets/fibers/doc_0176.jpg', './data/datasets/fibers/doc_0177.jpg', './data/datasets/fibers/doc_0178.jpg', './data/datasets/fibers/doc_0179.jpg', './data/datasets/fibers/doc_018.jpg', './data/datasets/fibers/doc_0180.jpg', './data/datasets/fibers/doc_0181.jpg', './data/datasets/fibers/doc_0182.jpg', './data/datasets/fibers/doc_0183.jpg', './data/datasets/fibers/doc_0184.jpg', './data/datasets/fibers/doc_0185.jpg', './data/datasets/fibers/doc_0186.jpg', './data/datasets/fibers/doc_0187.jpg', './data/datasets/fibers/doc_0188.jpg', './data/datasets/fibers/doc_0189.jpg', './data/datasets/fibers/doc_019.jpg', './data/datasets/fibers/doc_0190.jpg', './data/datasets/fibers/doc_0191.jpg', './data/datasets/fibers/doc_0192.jpg', './data/datasets/fibers/doc_0193.jpg', './data/datasets/fibers/doc_0194.jpg', './data/datasets/fibers/doc_0195.jpg', './data/datasets/fibers/doc_0196.jpg', './data/datasets/fibers/doc_0198.jpg', './data/datasets/fibers/doc_0199.jpg', './data/datasets/fibers/doc_020.jpg', './data/datasets/fibers/doc_0200.jpg', './data/datasets/fibers/doc_0201.jpg', './data/datasets/fibers/doc_0202.jpg', './data/datasets/fibers/doc_0203.jpg', './data/datasets/fibers/doc_0204.jpg', './data/datasets/fibers/doc_0205.jpg', './data/datasets/fibers/doc_0206.jpg', './data/datasets/fibers/doc_0207.jpg', './data/datasets/fibers/doc_0208.jpg', './data/datasets/fibers/doc_0209.jpg', './data/datasets/fibers/doc_021.jpg', './data/datasets/fibers/doc_0210.jpg', './data/datasets/fibers/doc_0211.jpg', './data/datasets/fibers/doc_0212.jpg', './data/datasets/fibers/doc_0213.jpg', './data/datasets/fibers/doc_0214.jpg', './data/datasets/fibers/doc_0215.jpg', './data/datasets/fibers/doc_0216.jpg', './data/datasets/fibers/doc_0217.jpg', './data/datasets/fibers/doc_0218.jpg', './data/datasets/fibers/doc_0219.jpg', './data/datasets/fibers/doc_022.jpg', './data/datasets/fibers/doc_0220.jpg', './data/datasets/fibers/doc_0221.jpg', './data/datasets/fibers/doc_0222.jpg', './data/datasets/fibers/doc_0223.jpg', './data/datasets/fibers/doc_0224.jpg', './data/datasets/fibers/doc_0225.jpg', './data/datasets/fibers/doc_0226.jpg', './data/datasets/fibers/doc_0227.jpg', './data/datasets/fibers/doc_0228.jpg', './data/datasets/fibers/doc_0229.jpg', './data/datasets/fibers/doc_023.jpg', './data/datasets/fibers/doc_0230.jpg', './data/datasets/fibers/doc_0231.jpg', './data/datasets/fibers/doc_0232.jpg', './data/datasets/fibers/doc_0233.jpg', './data/datasets/fibers/doc_0234.jpg', './data/datasets/fibers/doc_0235.jpg', './data/datasets/fibers/doc_0236.jpg', './data/datasets/fibers/doc_0237.jpg', './data/datasets/fibers/doc_0239.jpg', './data/datasets/fibers/doc_024.jpg', './data/datasets/fibers/doc_0240.jpg', './data/datasets/fibers/doc_0241.jpg', './data/datasets/fibers/doc_0243.jpg', './data/datasets/fibers/doc_0244.jpg', './data/datasets/fibers/doc_0245.jpg', './data/datasets/fibers/doc_0246.jpg', './data/datasets/fibers/doc_0247.jpg', './data/datasets/fibers/doc_0248.jpg', './data/datasets/fibers/doc_0249.jpg', './data/datasets/fibers/doc_025.jpg', './data/datasets/fibers/doc_0250.jpg', './data/datasets/fibers/doc_0251.jpg', './data/datasets/fibers/doc_0252.jpg', './data/datasets/fibers/doc_0253.jpg', './data/datasets/fibers/doc_0254.jpg', './data/datasets/fibers/doc_0255.jpg', './data/datasets/fibers/doc_0256.jpg', './data/datasets/fibers/doc_0257.jpg', './data/datasets/fibers/doc_0258.jpg', './data/datasets/fibers/doc_0259.jpg', './data/datasets/fibers/doc_026.jpg', './data/datasets/fibers/doc_0260.jpg', './data/datasets/fibers/doc_0261.jpg', './data/datasets/fibers/doc_0262.jpg', './data/datasets/fibers/doc_0263.jpg', './data/datasets/fibers/doc_0264.jpg', './data/datasets/fibers/doc_0265.jpg', './data/datasets/fibers/doc_0266.jpg', './data/datasets/fibers/doc_0267.jpg', './data/datasets/fibers/doc_0268.jpg', './data/datasets/fibers/doc_0269.jpg', './data/datasets/fibers/doc_027.jpg', './data/datasets/fibers/doc_0270.jpg', './data/datasets/fibers/doc_0271.jpg', './data/datasets/fibers/doc_0272.jpg', './data/datasets/fibers/doc_0273.jpg', './data/datasets/fibers/doc_0274.jpg', './data/datasets/fibers/doc_0275.jpg', './data/datasets/fibers/doc_0276.jpg', './data/datasets/fibers/doc_0278.jpg', './data/datasets/fibers/doc_0279.jpg', './data/datasets/fibers/doc_028.jpg', './data/datasets/fibers/doc_0280.jpg', './data/datasets/fibers/doc_0281.jpg', './data/datasets/fibers/doc_0282.jpg', './data/datasets/fibers/doc_0283.jpg', './data/datasets/fibers/doc_0284.jpg', './data/datasets/fibers/doc_0285.jpg', './data/datasets/fibers/doc_0286.jpg', './data/datasets/fibers/doc_0287.jpg', './data/datasets/fibers/doc_0288.jpg', './data/datasets/fibers/doc_0289.jpg', './data/datasets/fibers/doc_029.jpg', './data/datasets/fibers/doc_0291.jpg', './data/datasets/fibers/doc_0292.jpg', './data/datasets/fibers/doc_0293.jpg', './data/datasets/fibers/doc_0294.jpg', './data/datasets/fibers/doc_0296.jpg', './data/datasets/fibers/doc_0297.jpg', './data/datasets/fibers/doc_0298.jpg', './data/datasets/fibers/doc_0299.jpg', './data/datasets/fibers/doc_030.jpg', './data/datasets/fibers/doc_0300.jpg', './data/datasets/fibers/doc_0301.jpg', './data/datasets/fibers/doc_0302.jpg', './data/datasets/fibers/doc_0303.jpg', './data/datasets/fibers/doc_0304.jpg', './data/datasets/fibers/doc_0305.jpg', './data/datasets/fibers/doc_0306.jpg', './data/datasets/fibers/doc_0307.jpg', './data/datasets/fibers/doc_0308.jpg', './data/datasets/fibers/doc_0309.jpg', './data/datasets/fibers/doc_031.jpg', './data/datasets/fibers/doc_0310.jpg', './data/datasets/fibers/doc_0311.jpg', './data/datasets/fibers/doc_0312.jpg', './data/datasets/fibers/doc_0313.jpg', './data/datasets/fibers/doc_0314.jpg', './data/datasets/fibers/doc_0315.jpg', './data/datasets/fibers/doc_0316.jpg', './data/datasets/fibers/doc_0317.jpg', './data/datasets/fibers/doc_0318.jpg', './data/datasets/fibers/doc_0319.jpg', './data/datasets/fibers/doc_032.jpg', './data/datasets/fibers/doc_0320.jpg']\n",
      "['./data/datasets/fibers/doc_0321.jpg', './data/datasets/fibers/doc_0322.jpg', './data/datasets/fibers/doc_0323.jpg', './data/datasets/fibers/doc_0324.jpg', './data/datasets/fibers/doc_0325.jpg', './data/datasets/fibers/doc_0326.jpg', './data/datasets/fibers/doc_0327.jpg', './data/datasets/fibers/doc_0328.jpg', './data/datasets/fibers/doc_0329.jpg', './data/datasets/fibers/doc_033.jpg', './data/datasets/fibers/doc_0330.jpg', './data/datasets/fibers/doc_0331.jpg', './data/datasets/fibers/doc_0332.jpg', './data/datasets/fibers/doc_0333.jpg', './data/datasets/fibers/doc_0334.jpg', './data/datasets/fibers/doc_0335.jpg', './data/datasets/fibers/doc_0336.jpg', './data/datasets/fibers/doc_0337.jpg', './data/datasets/fibers/doc_0338.jpg', './data/datasets/fibers/doc_0339.jpg', './data/datasets/fibers/doc_034.jpg', './data/datasets/fibers/doc_0340.jpg', './data/datasets/fibers/doc_0341.jpg', './data/datasets/fibers/doc_0342.jpg', './data/datasets/fibers/doc_0343.jpg', './data/datasets/fibers/doc_0344.jpg', './data/datasets/fibers/doc_0345.jpg', './data/datasets/fibers/doc_0346.jpg', './data/datasets/fibers/doc_0347.jpg', './data/datasets/fibers/doc_0348.jpg', './data/datasets/fibers/doc_0349.jpg', './data/datasets/fibers/doc_035.jpg', './data/datasets/fibers/doc_0350.jpg', './data/datasets/fibers/doc_0351.jpg', './data/datasets/fibers/doc_0352.jpg', './data/datasets/fibers/doc_0353.jpg', './data/datasets/fibers/doc_0355.jpg', './data/datasets/fibers/doc_0357.jpg', './data/datasets/fibers/doc_0358.jpg', './data/datasets/fibers/doc_0359.jpg', './data/datasets/fibers/doc_036.jpg', './data/datasets/fibers/doc_0360.jpg', './data/datasets/fibers/doc_0361.jpg', './data/datasets/fibers/doc_0362.jpg', './data/datasets/fibers/doc_0363.jpg', './data/datasets/fibers/doc_0364.jpg', './data/datasets/fibers/doc_0365.jpg', './data/datasets/fibers/doc_0366.jpg', './data/datasets/fibers/doc_0367.jpg', './data/datasets/fibers/doc_0368.jpg', './data/datasets/fibers/doc_0369.jpg', './data/datasets/fibers/doc_037.jpg', './data/datasets/fibers/doc_0370.jpg', './data/datasets/fibers/doc_0371.jpg', './data/datasets/fibers/doc_0372.jpg', './data/datasets/fibers/doc_0374.jpg', './data/datasets/fibers/doc_0375.jpg', './data/datasets/fibers/doc_0376.jpg', './data/datasets/fibers/doc_0377.jpg', './data/datasets/fibers/doc_0378.jpg', './data/datasets/fibers/doc_0379.jpg', './data/datasets/fibers/doc_038.jpg', './data/datasets/fibers/doc_0380.jpg', './data/datasets/fibers/doc_0381.jpg', './data/datasets/fibers/doc_0382.jpg', './data/datasets/fibers/doc_0383.jpg', './data/datasets/fibers/doc_0384.jpg', './data/datasets/fibers/doc_0385.jpg', './data/datasets/fibers/doc_0386.jpg', './data/datasets/fibers/doc_0387.jpg', './data/datasets/fibers/doc_0388.jpg', './data/datasets/fibers/doc_0389.jpg', './data/datasets/fibers/doc_039.jpg', './data/datasets/fibers/doc_0391.jpg', './data/datasets/fibers/doc_0392.jpg', './data/datasets/fibers/doc_0393.jpg', './data/datasets/fibers/doc_0394.jpg', './data/datasets/fibers/doc_0396.jpg', './data/datasets/fibers/doc_0397.jpg', './data/datasets/fibers/doc_0398.jpg', './data/datasets/fibers/doc_0399.jpg', './data/datasets/fibers/doc_040.jpg', './data/datasets/fibers/doc_0400.jpg', './data/datasets/fibers/doc_0401.jpg', './data/datasets/fibers/doc_0402.jpg', './data/datasets/fibers/doc_0403.jpg', './data/datasets/fibers/doc_0404.jpg', './data/datasets/fibers/doc_0405.jpg', './data/datasets/fibers/doc_0406.jpg', './data/datasets/fibers/doc_0407.jpg', './data/datasets/fibers/doc_0408.jpg', './data/datasets/fibers/doc_0409.jpg', './data/datasets/fibers/doc_041.jpg', './data/datasets/fibers/doc_0410.jpg', './data/datasets/fibers/doc_0411.jpg', './data/datasets/fibers/doc_0412.jpg', './data/datasets/fibers/doc_0413.jpg', './data/datasets/fibers/doc_0414.jpg', './data/datasets/fibers/doc_0415.jpg', './data/datasets/fibers/doc_0416.jpg', './data/datasets/fibers/doc_0417.jpg', './data/datasets/fibers/doc_0418.jpg', './data/datasets/fibers/doc_0419.jpg', './data/datasets/fibers/doc_042.jpg', './data/datasets/fibers/doc_0420.jpg', './data/datasets/fibers/doc_0421.jpg', './data/datasets/fibers/doc_0422.jpg', './data/datasets/fibers/doc_0423.jpg', './data/datasets/fibers/doc_0424.jpg', './data/datasets/fibers/doc_0425.jpg', './data/datasets/fibers/doc_0426.jpg', './data/datasets/fibers/doc_0427.jpg', './data/datasets/fibers/doc_0428.jpg', './data/datasets/fibers/doc_0429.jpg', './data/datasets/fibers/doc_043.jpg', './data/datasets/fibers/doc_0430.jpg', './data/datasets/fibers/doc_0431.jpg', './data/datasets/fibers/doc_0432.jpg', './data/datasets/fibers/doc_0433.jpg', './data/datasets/fibers/doc_0434.jpg', './data/datasets/fibers/doc_0435.jpg', './data/datasets/fibers/doc_0436.jpg', './data/datasets/fibers/doc_0437.jpg', './data/datasets/fibers/doc_0438.jpg', './data/datasets/fibers/doc_0439.jpg', './data/datasets/fibers/doc_044.jpg', './data/datasets/fibers/doc_0440.jpg', './data/datasets/fibers/doc_0441.jpg', './data/datasets/fibers/doc_0442.jpg', './data/datasets/fibers/doc_0443.jpg', './data/datasets/fibers/doc_0444.jpg', './data/datasets/fibers/doc_0445.jpg', './data/datasets/fibers/doc_0446.jpg', './data/datasets/fibers/doc_0447.jpg', './data/datasets/fibers/doc_0448.jpg', './data/datasets/fibers/doc_0449.jpg', './data/datasets/fibers/doc_045.jpg', './data/datasets/fibers/doc_0450.jpg', './data/datasets/fibers/doc_046.jpg', './data/datasets/fibers/doc_047.jpg', './data/datasets/fibers/doc_048.jpg', './data/datasets/fibers/doc_049.jpg', './data/datasets/fibers/doc_050.jpg', './data/datasets/fibers/doc_051.jpg', './data/datasets/fibers/doc_052.jpg', './data/datasets/fibers/doc_053.jpg', './data/datasets/fibers/doc_054.jpg', './data/datasets/fibers/doc_055.jpg', './data/datasets/fibers/doc_056.jpg', './data/datasets/fibers/doc_057.jpg', './data/datasets/fibers/doc_058.jpg', './data/datasets/fibers/doc_059.jpg', './data/datasets/fibers/doc_060.jpg', './data/datasets/fibers/doc_061.jpg', './data/datasets/fibers/doc_062.jpg', './data/datasets/fibers/doc_063.jpg', './data/datasets/fibers/doc_064.jpg', './data/datasets/fibers/doc_065.jpg', './data/datasets/fibers/doc_066.jpg', './data/datasets/fibers/doc_067.jpg', './data/datasets/fibers/doc_068.jpg', './data/datasets/fibers/doc_069.jpg', './data/datasets/fibers/doc_070.jpg', './data/datasets/fibers/doc_071.jpg', './data/datasets/fibers/doc_072.jpg', './data/datasets/fibers/doc_073.jpg', './data/datasets/fibers/doc_074.jpg', './data/datasets/fibers/doc_075.jpg', './data/datasets/fibers/doc_076.jpg', './data/datasets/fibers/doc_077.jpg', './data/datasets/fibers/doc_078.jpg', './data/datasets/fibers/doc_079.jpg', './data/datasets/fibers/doc_080.jpg', './data/datasets/fibers/doc_081.jpg', './data/datasets/fibers/doc_082.jpg', './data/datasets/fibers/doc_083.jpg', './data/datasets/fibers/doc_084.jpg', './data/datasets/fibers/doc_085.jpg', './data/datasets/fibers/doc_086.jpg', './data/datasets/fibers/doc_087.jpg', './data/datasets/fibers/doc_088.jpg', './data/datasets/fibers/doc_089.jpg', './data/datasets/fibers/doc_090.jpg', './data/datasets/fibers/doc_091.jpg', './data/datasets/fibers/doc_092.jpg', './data/datasets/fibers/doc_093.jpg', './data/datasets/fibers/doc_094.jpg', './data/datasets/fibers/doc_095.jpg', './data/datasets/fibers/doc_096.jpg', './data/datasets/fibers/doc_097.jpg', './data/datasets/fibers/doc_098.jpg', './data/datasets/fibers/doc_099.jpg', './data/datasets/fibers/doc_100.jpg', './data/datasets/fibers/doc_101.jpg', './data/datasets/fibers/doc_102.jpg', './data/datasets/fibers/doc_103.jpg', './data/datasets/fibers/doc_104.jpg', './data/datasets/fibers/doc_105.jpg', './data/datasets/fibers/doc_106.jpg', './data/datasets/fibers/doc_107.jpg', './data/datasets/fibers/doc_108.jpg', './data/datasets/fibers/doc_109.jpg', './data/datasets/fibers/doc_110.jpg', './data/datasets/fibers/doc_111.jpg', './data/datasets/fibers/doc_112.jpg', './data/datasets/fibers/doc_113.jpg', './data/datasets/fibers/doc_114.jpg', './data/datasets/fibers/doc_115.jpg', './data/datasets/fibers/doc_116.jpg', './data/datasets/fibers/doc_117.jpg', './data/datasets/fibers/doc_118.jpg', './data/datasets/fibers/doc_119.jpg', './data/datasets/fibers/doc_120.jpg', './data/datasets/fibers/doc_121.jpg', './data/datasets/fibers/doc_122.jpg', './data/datasets/fibers/doc_123.jpg', './data/datasets/fibers/doc_124.jpg', './data/datasets/fibers/doc_125.jpg', './data/datasets/fibers/doc_126.jpg', './data/datasets/fibers/doc_127.jpg']\n",
      "220\n",
      "Namespace(exported=False, num_channels=1, pyramid_levels=3, scale_pyramid=1.3, dim_first=3, dim_second=5, dim_third=8, group_size=36, epochs=70, border_size=12, box_size=21, nms_size=5, img_size=120, batch_size=16, path_data='./data', path_model='model.pt', is_loss_ssim=True, margin_loss=2.0, outlier_rejection=False, show_feature=False)\n",
      "Model loaded from ./data/models/sp_map_fo_30.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import best.singular_point as sp\n",
    "from teste_util import *\n",
    "import teste_util as TS\n",
    "from utils.my_dataset import FibersDataset\n",
    "\n",
    "# Fixar a semente do Torch para operações específicas\n",
    "fixed_seed()\n",
    "# Carregar o dataset\n",
    "trainloader,testloader =read_dataload_fibers(sp.args.img_size)\n",
    "\n",
    "# Carregar o modelo singular points\n",
    "# path_single = './data/models/sp_53.pth' # 977/1020\n",
    "path_single = './data/models/sp_map_fo_30.pth'#1012/1020\n",
    "# path_single = './data/models/sp_map_fo_45.pth'#996/1020\n",
    "sp.args.num_channels = 1\n",
    "model = sp.SingularPoints(args=sp.args).to(sp.device)\n",
    "load_model(model,path_single,sp.device)\n",
    "\n",
    "#gerar variacao de transformacoes pespectivas e fotometrica\n",
    "iterator=iter(testloader)\n",
    "img,labels = next(iterator)\n",
    "params_lists =AugmentationParamsGenerator(6,img.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "## for fix seed\n",
    "import random, torch, numpy \n",
    "def get_config(jupyter=False):\n",
    "    parser = argparse.ArgumentParser(description='Train REKD Architecture')\n",
    "\n",
    "    ## basic configuration\n",
    "    parser.add_argument('--data_dir', type=str, default='../ImageNet2012/ILSVRC2012_img_val', #default='path-to-ImageNet',\n",
    "                            help='The root path to the data from which the synthetic dataset will be created.')\n",
    "    parser.add_argument('--synth_dir', type=str, default='', \n",
    "                            help='The path to save the generated sythetic image pairs.')\n",
    "    parser.add_argument('--log_dir', type=str, default='trained_models/weights',\n",
    "                            help='The path to save the REKD weights.')\n",
    "    parser.add_argument('--load_dir', type=str, default='',\n",
    "                        help='Set saved model parameters if resume training is desired.')                            \n",
    "    parser.add_argument('--exp_name', type=str, default='REKD',\n",
    "                            help='The Rotaton-equivaraiant Keypoint Detection (REKD) experiment name')\n",
    "    ## network architecture\n",
    "    parser.add_argument('--factor_scaling_pyramid', type=float, default=1.2,\n",
    "                        help='The scale factor between the multi-scale pyramid levels in the architecture.')\n",
    "    parser.add_argument('--group_size', type=int, default=36,  \n",
    "                        help='The number of groups for the group convolution.')\n",
    "    parser.add_argument('--dim_first', type=int, default=2,\n",
    "                        help='The number of channels of the first layer')\n",
    "    parser.add_argument('--dim_second', type=int, default=2,\n",
    "                        help='The number of channels of the second layer')\n",
    "    parser.add_argument('--dim_third', type=int, default=2,\n",
    "                        help='The number of channels of the thrid layer')                       \n",
    "    ## network training\n",
    "    parser.add_argument('--batch_size', type=int, default=16,\n",
    "                        help='The batch size for training.')\n",
    "    parser.add_argument('--num_epochs', type=int, default=20, \n",
    "                        help='Number of epochs for training.')\n",
    "    ## Loss function  \n",
    "    parser.add_argument('--init_initial_learning_rate', type=float, default=1e-3,\n",
    "                        help='The init initial learning rate value.')\n",
    "    parser.add_argument('--MSIP_sizes', type=str, default=\"8,16,24,32,40\",\n",
    "                        help='MSIP sizes.')\n",
    "    parser.add_argument('--MSIP_factor_loss', type=str, default=\"256.0,64.0,16.0,4.0,1.0\",\n",
    "                        help='MSIP loss balancing parameters.')\n",
    "    parser.add_argument('--ori_loss_balance', type=float, default=100., \n",
    "                        help='')\n",
    "    ## Dataset generation\n",
    "    parser.add_argument('--patch_size', type=int, default=192,\n",
    "                        help='The patch size of the generated dataset.')\n",
    "    parser.add_argument('--max_angle', type=int, default=180,\n",
    "                        help='The max angle value for generating a synthetic view to train REKD.')\n",
    "    parser.add_argument('--min_scale', type=float, default=1.0,\n",
    "                        help='The min scale value for generating a synthetic view to train REKD.')\n",
    "    parser.add_argument('--max_scale', type=float, default=1.0,\n",
    "                        help='The max scale value for generating a synthetic view to train REKD.')\n",
    "    parser.add_argument('--max_shearing', type=float, default=0.0,\n",
    "                        help='The max shearing value for generating a synthetic view to train REKD.')\n",
    "    parser.add_argument('--num_training_data', type=int, default=9000,\n",
    "                        help='The number of the generated dataset.')\n",
    "    parser.add_argument('--is_debugging', type=bool, default=False,\n",
    "                        help='Set variable to True if you desire to train network on a smaller dataset.')\n",
    "    ## For eval/inference\n",
    "    parser.add_argument('--num_points', type=int, default=1500,\n",
    "                        help='the number of points at evaluation time.')\n",
    "    parser.add_argument('--pyramid_levels', type=int, default=5,\n",
    "                        help='downsampling pyramid levels.')\n",
    "    parser.add_argument('--upsampled_levels', type=int, default=2,\n",
    "                        help='upsampling image levels.')\n",
    "    parser.add_argument('--nms_size', type=int, default=15,\n",
    "                        help='The NMS size for computing the validation repeatability.')\n",
    "    parser.add_argument('--border_size', type=int, default=15,\n",
    "                        help='The number of pixels to remove from the borders to compute the repeatability.')\n",
    "    ## For HPatches evaluation\n",
    "    parser.add_argument('--hpatches_path', type=str, default='./datasets/hpatches-sequences-release',\n",
    "                        help='dataset ')\n",
    "    parser.add_argument('--eval_split', type=str, default='debug',\n",
    "                        help='debug, view, illum, full, debug_view, debug_illum ...')      \n",
    "    parser.add_argument('--descriptor', type=str, default=\"hardnet\",\n",
    "                        help='hardnet, sosnet, hynet')    \n",
    "\n",
    "    args = parser.parse_args() if not jupyter else parser.parse_args(args=[])   \n",
    "\n",
    "    fixed_seed()\n",
    "\n",
    "    if args.synth_dir == \"\":\n",
    "        args.synth_dir = 'datasets/synth_data'\n",
    "\n",
    "    args.MSIP_sizes = [int(i) for i in args.MSIP_sizes.split(\",\")]\n",
    "    args.MSIP_factor_loss =[float(i) for i in args.MSIP_factor_loss.split(\",\")]\n",
    "\n",
    "    return args\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model paramter : trained_models/release_group36_f2_s2_t2.log/best_model.pt is loaded.\n",
      "================ List of Learnable model parameters ================ \n",
      "model.block1.0.weights torch.Size([22])\n",
      "model.block1.1.batch_norm_[36].weight torch.Size([2])\n",
      "model.block1.1.batch_norm_[36].bias torch.Size([2])\n",
      "model.block2.0.weights torch.Size([1584])\n",
      "model.block2.1.batch_norm_[36].weight torch.Size([2])\n",
      "model.block2.1.batch_norm_[36].bias torch.Size([2])\n",
      "model.block3.0.weights torch.Size([1584])\n",
      "model.block3.1.batch_norm_[36].weight torch.Size([2])\n",
      "model.block3.1.batch_norm_[36].bias torch.Size([2])\n",
      "model.ori_learner.0.weights torch.Size([72])\n",
      "model.last_layer_learner.0.weight torch.Size([6])\n",
      "model.last_layer_learner.0.bias torch.Size([6])\n",
      "model.last_layer_learner.1.weight torch.Size([1, 6, 1, 1])\n",
      "model.last_layer_learner.1.bias torch.Size([1])\n",
      "The number of learnable parameters : 3293 \n",
      "==================================================================== \n",
      "================ List of Learnable model parameters ================ \n",
      "model.features.0.weight torch.Size([32, 1, 3, 3])\n",
      "model.features.3.weight torch.Size([32, 32, 3, 3])\n",
      "model.features.6.weight torch.Size([64, 32, 3, 3])\n",
      "model.features.9.weight torch.Size([64, 64, 3, 3])\n",
      "model.features.12.weight torch.Size([128, 64, 3, 3])\n",
      "model.features.15.weight torch.Size([128, 128, 3, 3])\n",
      "model.features.19.weight torch.Size([128, 128, 8, 8])\n",
      "The number of learnable parameters : 1334560 \n",
      "==================================================================== \n"
     ]
    }
   ],
   "source": [
    "from external.REKD import REKD, count_model_parameters\n",
    "from external.hardnet_pytorch import HardNet\n",
    "\n",
    "args = get_config(jupyter=True)\n",
    "args.load_dir =  'trained_models/release_group36_f2_s2_t2.log/best_model.pt'\n",
    "\n",
    "device = torch.device('cuda')\n",
    "\n",
    "\n",
    "from torch import nn\n",
    "class Detector(nn.Module):\n",
    "        def __init__(self) -> None:\n",
    "           super().__init__()\n",
    "           detec = REKD(args, device)\n",
    "           detec = detec.to(device) ## use GPU\n",
    "           detec.load_state_dict(torch.load(args.load_dir))  ## Load the PyTorch learnable model parameters.\n",
    "           detec.eval()\n",
    "           self.model = detec\n",
    "           \n",
    "        def forward(self, x):\n",
    "            features_key,features_ori= self.model(x)\n",
    "            return features_key\n",
    "        \n",
    "        \n",
    "class Descriptor(nn.Module):\n",
    "        def __init__(self) -> None:\n",
    "           super().__init__()           \n",
    "           hardnet = HardNet()\n",
    "           checkpoint = torch.load('trained_models/pretrained_nets/HardNet++.pth')\n",
    "           hardnet.load_state_dict(checkpoint['state_dict'])\n",
    "           hardnet.eval()\n",
    "           hardnet.to(device) \n",
    "           self.model = hardnet\n",
    "        def forward(self, x):\n",
    "           return self.model(x)\n",
    "\n",
    "detec = Detector()\n",
    "desc = Descriptor()\n",
    "print(\"Model paramter : {} is loaded.\".format( args.load_dir ))\n",
    "count_model_parameters(detec)\n",
    "count_model_parameters(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import kornia\n",
    "from kornia.feature.scale_space_detector import get_default_detector_config, MultiResolutionDetector\n",
    "from kornia.feature.keynet import KeyNetDetector\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "PS = 32 # patch size\n",
    "\n",
    "keynet_default_config = {#986\n",
    "    'num_filters': 8,\n",
    "    'num_levels': 3,\n",
    "    'kernel_size': 5,\n",
    "    'Detector_conf': {'nms_size': 15, 'pyramid_levels': 1, 'up_levels': 1, 'scale_factor_levels': 1.3, 's_mult': 12.0},\n",
    "}\n",
    "\n",
    "\n",
    "keynet_default_config = {#990/1020\n",
    "    'num_filters': 8,\n",
    "    'num_levels': 3,\n",
    "    'kernel_size': 5,\n",
    "    'Detector_conf': {'nms_size': 5, 'pyramid_levels': 2, 'up_levels': 1, 'scale_factor_levels': 1.3, 's_mult': 12.0},\n",
    "}\n",
    "\n",
    "keynet_default_config = {#958\n",
    "    'num_filters': 8,\n",
    "    'num_levels': 3,\n",
    "    'kernel_size': 5,\n",
    "    'Detector_conf': {'nms_size': 25, 'pyramid_levels': 1, 'up_levels': 1, 'scale_factor_levels': 1.3, 's_mult': 12.0},\n",
    "}\n",
    "\n",
    "\n",
    "# keynet_default_config = { 990/1020\n",
    "#     'num_filters': 8,\n",
    "#     'num_levels': 3,\n",
    "#     'kernel_size': 5,\n",
    "#     'Detector_conf': {'nms_size': 5, 'pyramid_levels': 3, 'up_levels': 2, 'scale_factor_levels': 1.3, 's_mult': 15.0},\n",
    "# }\n",
    "# keynet_default_config = {990/120\n",
    "#     'num_filters': 8,\n",
    "#     'num_levels': 3,\n",
    "#     'kernel_size': 5,\n",
    "#     'Detector_conf': {'nms_size': 15, 'pyramid_levels': 3, 'up_levels': 2, 'scale_factor_levels': 1.3, 's_mult': 15.0},\n",
    "# }\n",
    "\n",
    "# keynet_default_config = {1006/1020\n",
    "#     'num_filters': 8,\n",
    "#     'num_levels': 3,\n",
    "#     'kernel_size': 5,\n",
    "#     'Detector_conf': {'nms_size': 5, 'pyramid_levels': 3, 'up_levels': 2, 'scale_factor_levels': 1.3, 's_mult': 22.0},\n",
    "# }\n",
    "\n",
    "# keynet_default_config = {948/1020\n",
    "#     'num_filters': 8,\n",
    "#     'num_levels': 3,\n",
    "#     'kernel_size': 5,\n",
    "#     'Detector_conf': {'nms_size': 5, 'pyramid_levels': 3, 'up_levels': 2, 'scale_factor_levels': 1.3, 's_mult': 10.0},\n",
    "# }\n",
    "\n",
    "keynet_default_config = {#962/1020 fibers 34/220 0.15\n",
    "    'num_filters': 8,\n",
    "    'num_levels': 3,\n",
    "    'kernel_size': 5,\n",
    "    'Detector_conf': {'nms_size': 5, 'pyramid_levels': 3, 'up_levels': 2, 'scale_factor_levels': 1.3, 's_mult': 12.0},\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# timg_gray = img.to(device)#timg_gray.to(device)\n",
    "TS.PS=32\n",
    "sift = kornia.feature.SIFTDescriptor(PS, rootsift=True).to(sp.device)\n",
    "descriptor = desc#sift\n",
    "detector = CustomNetDetector(detec,PS=TS.PS,keynet_conf=keynet_default_config).to(sp.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_extract_features(image, detector, descriptor, PS):\n",
    "    with torch.no_grad():\n",
    "        lafs, resps = detector(image[None])\n",
    "        # print('lafs  ',lafs.shape)\n",
    "        patches = kornia.feature.extract_patches_from_pyramid(image[None], lafs, PS)\n",
    "        B, N, CH, H, W = patches.size()\n",
    "        # print('patches  ',patches.shape)\n",
    "        descs = descriptor(patches.view(B * N, CH, H, W)).view(B, N, -1)\n",
    "        # print('descs  ',descs.shape)\n",
    "        return lafs, descs\n",
    "    \n",
    "def detect_extract_feat_in_batch(batch_img, detector, descriptor, PS):\n",
    "    repo_lafs_desc = []\n",
    "    with torch.no_grad():\n",
    "        for image  in batch_img:\n",
    "            \n",
    "            lafs, resps = detector(image[None])\n",
    "            patches = kornia.feature.extract_patches_from_pyramid(image[None], lafs, PS)\n",
    "            B, N, CH, H, W = patches.size()\n",
    "            descs = descriptor(patches.view(B * N, CH, H, W)).view(B, N, -1)\n",
    "            repo_lafs_desc.append((lafs,descs))\n",
    "    return repo_lafs_desc\n",
    "    \n",
    "\n",
    "def matching_imagens(ref_img,batch_img, repo_lafs_desc):\n",
    "    best_match_info = None\n",
    "    best_match_count = 0\n",
    "    best_match_index = None\n",
    "    with torch.no_grad():\n",
    "        # Detectar e extrair características da imagem de referência\n",
    "        lafs_ref, descs_ref = detect_and_extract_features(ref_img, detector, descriptor, PS)\n",
    "        \n",
    "        for i, (lafs_i, descs_i) in enumerate(repo_lafs_desc):\n",
    "            # Detectar e extrair características da imagem atual do batch\n",
    "            # lafs_i, descs_i = detect_and_extract_features(img, detector, descriptor, PS)\n",
    "            # Comparar as características da imagem de referência com a imagem atual do batch\n",
    "            scores, matches = kornia.feature.match_snn(descs_ref[0], descs_i[0], 0.85) # correspondencia dos descritories a uma distância de 0.9\n",
    "\n",
    "            if matches.shape[0] >= 4:\n",
    "                # Cálculo da homografia\n",
    "                inliers_mask = compute_homography(lafs_ref, lafs_i, matches)\n",
    "                # print(lafs_ref[0][None].shape, lafs_ref[0].shape, matches.shape, inliers_mask.shape)\n",
    "\n",
    "                # Check if this match is better than the previous best match\n",
    "                if matches.shape[0] > best_match_count:\n",
    "                    best_match_info = (lafs_ref[0][None].cpu(), lafs_i[0][None].cpu(), matches.cpu(),\n",
    "                                       kornia.tensor_to_image(ref_img.cpu()), kornia.tensor_to_image(batch_img[i].cpu()),\n",
    "                                       inliers_mask)\n",
    "                    best_match_count = matches.shape[0]\n",
    "                    best_match_index = i\n",
    "\n",
    "        if best_match_info is not None and best_match_index==0:# TODO: Remove this condition best_match_index==0\n",
    "            # Plot the best match\n",
    "            from kornia_moons.viz import draw_LAF_matches\n",
    "\n",
    "            draw_LAF_matches(\n",
    "                *best_match_info,\n",
    "                draw_dict={\"inlier_color\": (0.2, 1, 0.2), \"tentative_color\": (1, 1, 0.2, 0.3), \"feature_color\": None, \"vertical\": False},\n",
    "            )\n",
    "        # else:\n",
    "        #     print(\"No matches found with enough inliers.\")\n",
    "    return best_match_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76d877d136084c8f835e5e1bc3f71c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imgs_batch.shape:  torch.Size([44, 1, 120, 120]) (' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ', ' rrr ')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m params_item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(params_lists)\n\u001b[1;32m     12\u001b[0m timg_gray_t \u001b[39m=\u001b[39m aug_list(imgs_batch,params\u001b[39m=\u001b[39mparams_item)\n\u001b[0;32m---> 13\u001b[0m repo_lafs_desc\u001b[39m=\u001b[39m detect_extract_feat_in_batch(timg_gray_t,detector,descriptor,TS\u001b[39m.\u001b[39;49mPS)\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m i,img_gray \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(imgs_batch):\u001b[39m# itera em cada batch\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     match_index \u001b[39m=\u001b[39m matching_imagens(img_gray,timg_gray_t,repo_lafs_desc)\n",
      "Cell \u001b[0;32mIn[5], line 17\u001b[0m, in \u001b[0;36mdetect_extract_feat_in_batch\u001b[0;34m(batch_img, detector, descriptor, PS)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[1;32m     15\u001b[0m     \u001b[39mfor\u001b[39;00m image  \u001b[39min\u001b[39;00m batch_img:\n\u001b[0;32m---> 17\u001b[0m         lafs, resps \u001b[39m=\u001b[39m detector(image[\u001b[39mNone\u001b[39;49;00m])\n\u001b[1;32m     18\u001b[0m         patches \u001b[39m=\u001b[39m kornia\u001b[39m.\u001b[39mfeature\u001b[39m.\u001b[39mextract_patches_from_pyramid(image[\u001b[39mNone\u001b[39;00m], lafs, PS)\n\u001b[1;32m     19\u001b[0m         B, N, CH, H, W \u001b[39m=\u001b[39m patches\u001b[39m.\u001b[39msize()\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/kornia/feature/scale_space_detector.py:408\u001b[0m, in \u001b[0;36mMultiResolutionDetector.forward\u001b[0;34m(self, img, mask)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Three stage local feature detection. First the location and scale of interest points are determined by\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[39mdetect function. Then affine shape and orientation.\u001b[39;00m\n\u001b[1;32m    396\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    405\u001b[0m \u001b[39m    responses: shape [1xNx1]. Response function values for corresponding lafs\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    407\u001b[0m KORNIA_CHECK_SHAPE(img, [\u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mC\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mH\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mW\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 408\u001b[0m responses, lafs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetect(img, mask)\n\u001b[1;32m    409\u001b[0m lafs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maff(lafs, img)\n\u001b[1;32m    410\u001b[0m lafs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mori(lafs, img)\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/kornia/feature/scale_space_detector.py:356\u001b[0m, in \u001b[0;36mMultiResolutionDetector.detect\u001b[0;34m(self, img, mask)\u001b[0m\n\u001b[1;32m    353\u001b[0m img_up \u001b[39m=\u001b[39m resize(img_up, (nh, nw), interpolation\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m, align_corners\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    355\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 356\u001b[0m cur_scores, cur_lafs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdetect_features_on_single_level(img_up, num_points_level, up_factor_kpts)\n\u001b[1;32m    357\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m    358\u001b[0m \u001b[39m# print(\"Time for computing upper levels: \", end_time - start_time)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/kornia/feature/scale_space_detector.py:305\u001b[0m, in \u001b[0;36mMultiResolutionDetector.detect_features_on_single_level\u001b[0;34m(self, level_img, num_kp, factor)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdetect_features_on_single_level\u001b[39m(\n\u001b[1;32m    303\u001b[0m     \u001b[39mself\u001b[39m, level_img: Tensor, num_kp: \u001b[39mint\u001b[39m, factor: Tuple[\u001b[39mfloat\u001b[39m, \u001b[39mfloat\u001b[39m]\n\u001b[1;32m    304\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[Tensor, Tensor]:\n\u001b[0;32m--> 305\u001b[0m     det_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnms(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mremove_borders(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(level_img)))\n\u001b[1;32m    306\u001b[0m     device \u001b[39m=\u001b[39m level_img\u001b[39m.\u001b[39mdevice\n\u001b[1;32m    307\u001b[0m     dtype \u001b[39m=\u001b[39m level_img\u001b[39m.\u001b[39mdtype\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m, in \u001b[0;36mDetector.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[0;32m---> 21\u001b[0m     features_key,features_ori\u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(x)\n\u001b[1;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m features_key\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Documentos/python/singular-points/external/REKD.py:75\u001b[0m, in \u001b[0;36mREKD.forward\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, input_data):\n\u001b[0;32m---> 75\u001b[0m     features_key, features_o \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_features(input_data)\n\u001b[1;32m     77\u001b[0m     \u001b[39mreturn\u001b[39;00m features_key, features_o\n",
      "File \u001b[0;32m~/Documentos/python/singular-points/external/REKD.py:91\u001b[0m, in \u001b[0;36mREKD.compute_features\u001b[0;34m(self, input_data)\u001b[0m\n\u001b[1;32m     89\u001b[0m     features_t, features_o \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forwarding_networks_divide_grid(input_data_resized)\n\u001b[1;32m     90\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 91\u001b[0m     features_t, features_o \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forwarding_networks(input_data_resized)\n\u001b[1;32m     93\u001b[0m features_t \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(features_t, size\u001b[39m=\u001b[39m(H,W),align_corners\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     94\u001b[0m features_o \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39minterpolate(features_o, size\u001b[39m=\u001b[39m(H,W),align_corners\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, mode\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbilinear\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/Documentos/python/singular-points/external/REKD.py:115\u001b[0m, in \u001b[0;36mREKD._forwarding_networks\u001b[0;34m(self, input_data_resized)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39m## Geometric tensor feed forwarding\u001b[39;00m\n\u001b[1;32m    114\u001b[0m features_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock1(features_t)\n\u001b[0;32m--> 115\u001b[0m features_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mblock2(features_t)\n\u001b[1;32m    116\u001b[0m features_t \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblock3(features_t)\n\u001b[1;32m    118\u001b[0m \u001b[39m## orientation pooling\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/e2cnn/nn/modules/sequential_module.py:80\u001b[0m, in \u001b[0;36mSequentialModule.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     78\u001b[0m x \u001b[39m=\u001b[39m \u001b[39minput\u001b[39m\n\u001b[1;32m     79\u001b[0m \u001b[39mfor\u001b[39;00m m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_modules\u001b[39m.\u001b[39mvalues():\n\u001b[0;32m---> 80\u001b[0m     x \u001b[39m=\u001b[39m m(x)\n\u001b[1;32m     82\u001b[0m \u001b[39massert\u001b[39;00m x\u001b[39m.\u001b[39mtype \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mout_type\n\u001b[1;32m     84\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.11/site-packages/e2cnn/nn/modules/batchnormalization/inner.py:140\u001b[0m, in \u001b[0;36mInnerBatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    135\u001b[0m batchnorm \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbatch_norm_[\u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    137\u001b[0m \u001b[39mif\u001b[39;00m contiguous:\n\u001b[1;32m    138\u001b[0m     \u001b[39m# if the fields were contiguous, we can use slicing\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     output[:, indices[\u001b[39m0\u001b[39m]:indices[\u001b[39m1\u001b[39m], :, :] \u001b[39m=\u001b[39m batchnorm(\n\u001b[0;32m--> 140\u001b[0m         \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mtensor[:, indices[\u001b[39m0\u001b[39;49m]:indices[\u001b[39m1\u001b[39;49m], :, :]\u001b[39m.\u001b[39;49mview(b, \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, s, h, w)\n\u001b[1;32m    141\u001b[0m     )\u001b[39m.\u001b[39mview(b, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, h, w)\n\u001b[1;32m    142\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    143\u001b[0m     \u001b[39m# otherwise we have to use indexing\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     output[:, indices, :, :] \u001b[39m=\u001b[39m batchnorm(\n\u001b[1;32m    145\u001b[0m         \u001b[39minput\u001b[39m\u001b[39m.\u001b[39mtensor[:, indices, :, :]\u001b[39m.\u001b[39mview(b, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, s, h, w)\n\u001b[1;32m    146\u001b[0m     )\u001b[39m.\u001b[39mview(b, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, h, w)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "params_lists.aug_list.data_keys =[\"input\"]\n",
    "aug_list = params_lists.aug_list\n",
    "\n",
    "acertos = 0\n",
    "total = 0\n",
    "from tqdm.notebook import tqdm\n",
    "pbar =  tqdm(testloader)\n",
    "for imgs_batch,labels_batch in pbar:# itera em todo dataset\n",
    "    imgs_batch = imgs_batch.to(sp.device)\n",
    "    print(\"imgs_batch.shape: \",imgs_batch.shape,labels_batch)\n",
    "    params_item = next(params_lists)\n",
    "    timg_gray_t = aug_list(imgs_batch,params=params_item)\n",
    "    repo_lafs_desc= detect_extract_feat_in_batch(timg_gray_t,detector,descriptor,TS.PS)\n",
    "        \n",
    "    for i,img_gray in enumerate(imgs_batch):# itera em cada batch\n",
    "\n",
    "        match_index = matching_imagens(img_gray,timg_gray_t,repo_lafs_desc)\n",
    "        # print(\"match_index: \",match_index,\" i: \",i)\n",
    "        total+=1\n",
    "        if match_index == i:\n",
    "            acertos += 1\n",
    "        pbar.set_description(f\"acertos/total: {acertos}/{total}  \")\n",
    "print(\"acertos: \",acertos)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
